{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ControlVAE Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanjiew/csc412_vae/blob/main/ControlVAE_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYAruimtNmL"
      },
      "source": [
        "Code is adapted from [the original ControlVAE repository](https://github.com/shj1987/ControlVAE-ICML2020/tree/master/Language_modeling/Text_gen_PTB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfLIsQFIvrMn"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W-1FfEitYz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f89fa749-b043-4c35-fd27-4e8e0c2782d4"
      },
      "source": [
        "%pip install texar-pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting texar-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/7e/20aa39ee9d19dcd1a1c0db4dad878088cfba216f45aeaa8fa89615ef46c0/texar_pytorch-0.1.2.post1-py3-none-any.whl (434kB)\n",
            "\r\u001b[K     |▊                               | 10kB 12.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 17.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30kB 14.5MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 10.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51kB 9.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 81kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 317kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 327kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 337kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 348kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 358kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 368kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 378kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 389kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 399kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 409kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 430kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 440kB 9.1MB/s \n",
            "\u001b[?25hCollecting mypy-extensions\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 17.6MB/s \n",
            "\u001b[?25hCollecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy<=1.19.5,>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (1.19.5)\n",
            "Requirement already satisfied: regex>=2018.01.10 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2.23.0)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=19.0->texar-pytorch) (2.4.7)\n",
            "Installing collected packages: mypy-extensions, sentencepiece, funcsigs, texar-pytorch\n",
            "Successfully installed funcsigs-1.0.2 mypy-extensions-0.4.3 sentencepiece-0.1.91 texar-pytorch-0.1.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA-j84Kot-hG"
      },
      "source": [
        "import math, os, random\n",
        "from collections import Counter\n",
        "from typing import Any, Dict, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "import texar.torch as tx\n",
        "from texar.torch.custom import MultivariateNormalDiag\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVNAghBeu8TC"
      },
      "source": [
        "# VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9_L26Qgu_5o"
      },
      "source": [
        "def kl_divergence(means: Tensor, logvars: Tensor) -> Tensor:\n",
        "    \"\"\"Compute the KL divergence between Gaussian distribution\n",
        "    \"\"\"\n",
        "    kl_cost = -0.5 * (logvars - means ** 2 -\n",
        "                      torch.exp(logvars) + 1.0)\n",
        "    kl_cost = torch.mean(kl_cost, 0)\n",
        "    return torch.sum(kl_cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ckhxqwvITp"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "    def __init__(self, config_model, vocab_size, padding_idx):\n",
        "        super().__init__()\n",
        "        # Model architecture\n",
        "        self._config = config_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self._config.embed_dim,\n",
        "            padding_idx=padding_idx\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.enc_dropout_in = nn.Dropout(self._config.enc_dropout_in).to(self.device)\n",
        "        \n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=self._config.embed_dim,\n",
        "            hidden_size=self._config.hidden_size,\n",
        "            batch_first=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.enc_dropout_out = nn.Dropout(self._config.enc_dropout_out).to(self.device)\n",
        "\n",
        "        self.connector_mlp = nn.Linear(\n",
        "            in_features=self._config.hidden_size * 2,\n",
        "            out_features=self._config.latent_dims * 2\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.mlp_linear_layer = nn.Linear(\n",
        "            in_features=self._config.latent_dims,\n",
        "            out_features=self._config.hidden_size * 2\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.dec_dropout_in = nn.Dropout(self._config.dec_dropout_in).to(self.device)\n",
        "\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=self._config.embed_dim,\n",
        "            hidden_size=self._config.hidden_size,\n",
        "            batch_first=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.dec_dropout_out = nn.Dropout(self._config.dec_dropout_out).to(self.device)\n",
        "\n",
        "        self.logits_layer = nn.Linear(\n",
        "            in_features=self._config.hidden_size,\n",
        "            out_features=vocab_size\n",
        "        ).to(self.device)\n",
        "    \n",
        "    def init_state(self, batch_size=1):\n",
        "        return (\n",
        "            torch.zeros(1, batch_size, self._config.hidden_size).to(self.device),\n",
        "            torch.zeros(1, batch_size, self._config.hidden_size).to(self.device)\n",
        "        )\n",
        "\n",
        "    def encode(self, x, seq_lengths):\n",
        "        emb = self.enc_dropout_in(self.embedding(x))\n",
        "        emb = torch.nn.utils.rnn.pack_padded_sequence(emb, seq_lengths, batch_first=True, enforce_sorted=False) # https://towardsdatascience.com/61d35642972e\n",
        "        _, state = self.encoder(emb, self.init_state(x.size()[0]))\n",
        "        state = self.enc_dropout_out(torch.cat(state, dim=-1)[0])\n",
        "        mean_logvar = self.connector_mlp(state)\n",
        "        mean, logvar = torch.chunk(mean_logvar, 2, 1)\n",
        "        return mean, logvar\n",
        "    \n",
        "    def decode(self, z, x, seq_lengths, prev_state=None):\n",
        "        if prev_state is None:\n",
        "            prev_state = torch.chunk(self.mlp_linear_layer(z).unsqueeze(0), 2, -1)\n",
        "        emb = self.dec_dropout_in(self.embedding(x))\n",
        "        emb = torch.nn.utils.rnn.pack_padded_sequence(emb, seq_lengths, batch_first=True, enforce_sorted=False) # https://towardsdatascience.com/61d35642972e\n",
        "        outputs, state = self.decoder(emb, prev_state)\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        outputs = self.dec_dropout_out(outputs)\n",
        "        logits = self.logits_layer(outputs)\n",
        "        return logits, state\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YZOkyApvNmP"
      },
      "source": [
        "# PID Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6g1VCAvPdJ"
      },
      "source": [
        "class PIDControl():\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"define them out of loop\"\"\"\n",
        "        # self.exp_KL = exp_KL\n",
        "        self.I_k1 = 0.0\n",
        "        self.W_k1 = 0.0\n",
        "        self.e_k1 = 0.0\n",
        "        \n",
        "    def _Kp_fun(self, Err, scale=1):\n",
        "        return 1.0/(1.0 + float(scale)*math.exp(Err))\n",
        "        \n",
        "\n",
        "    def pid(self, exp_KL, kl_loss, Kp=0.001, Ki=-0.001, Kd=0.01):\n",
        "        \"\"\"\n",
        "        position PID algorithm\n",
        "        Input: KL_loss\n",
        "        return: weight for KL loss, beta\n",
        "        \"\"\"\n",
        "        error_k = exp_KL - kl_loss\n",
        "        ## comput U as the control factor\n",
        "        Pk = Kp * self._Kp_fun(error_k)\n",
        "        Ik = self.I_k1 + Ki * error_k\n",
        "\n",
        "        ## window up for integrator\n",
        "        if self.W_k1 < 0 and self.W_k1 > 1:\n",
        "            Ik = self.I_k1\n",
        "            \n",
        "        Wk = Pk + Ik\n",
        "        self.W_k1 = Wk\n",
        "        self.I_k1 = Ik\n",
        "        self.e_k1 = error_k\n",
        "        \n",
        "        ## min and max value\n",
        "        if Wk > 1:\n",
        "            Wk = 1.0\n",
        "        if Wk < 0:\n",
        "            Wk = 0.0\n",
        "        \n",
        "        return Wk\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV3yJUUGvZwz"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSMZHtSCvizg"
      },
      "source": [
        "class Config():\n",
        "  dataset_size = 8750\n",
        "  num_epochs = 100\n",
        "  hidden_size = 256\n",
        "  dec_dropout_in = 0.5\n",
        "  dec_dropout_out = 0.5\n",
        "  enc_dropout_in = 0.\n",
        "  enc_dropout_out = 0.\n",
        "  batch_size = 32\n",
        "  embed_dim = 256\n",
        "  latent_dims = 32\n",
        "  max_vocab = 12000\n",
        "\n",
        "  pid_control = True\n",
        "\n",
        "  lr_decay_hparams = {\n",
        "      \"init_lr\": 0.001,\n",
        "      \"threshold\": 2,\n",
        "      \"decay_factor\": 0.5,\n",
        "      \"max_decay\": 5\n",
        "  }\n",
        "\n",
        "  # KL annealing\n",
        "  kl_anneal_hparams = {\n",
        "      \"warm_up\": 10,\n",
        "      \"start\": 0.1\n",
        "  }\n",
        "\n",
        "  initializer_hparams = {\n",
        "      'mean': 0.0,\n",
        "      'std': embed_dim**-0.5,\n",
        "  }\n",
        "\n",
        "  opt_hparams = {\n",
        "      'optimizer': {\n",
        "          'type': 'Adam',\n",
        "          'kwargs': {\n",
        "              'lr': 0.001\n",
        "          }\n",
        "      },\n",
        "      'gradient_clip': {\n",
        "          \"type\": \"clip_grad_norm_\",\n",
        "          \"kwargs\": {\n",
        "              \"max_norm\": 5,\n",
        "              \"norm_type\": 2\n",
        "          }\n",
        "      }\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP2M-MCxUL-7"
      },
      "source": [
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "madW6YSv1jay"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tItIdmTwj-4"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/rekiksab/Yelp-Data-Challenge-2013/master/yelp_challenge/yelp_phoenix_academic_dataset/yelp_academic_dataset_review.json'\n",
        "data = pd.read_json(url, lines=True)\n",
        "data = data.loc[:,['text','stars']]\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "data.loc[:,'text'] = data.loc[:,'text'].map(lambda s: ['<BOS>']+tokenizer(s)+['<EOS>','<PAD>'])\n",
        "\n",
        "counter = Counter()\n",
        "for line in data.loc[:,'text'].values:\n",
        "    counter.update(line)\n",
        "vocab = Vocab(counter, max_size=config.max_vocab)\n",
        "vocab.bos_index = vocab['<BOS>']\n",
        "vocab.eos_index = vocab['<EOS>']\n",
        "vocab.padding_index = vocab['<PAD>']\n",
        "\n",
        "data.loc[:,'text'] = data.loc[:,'text'].map(lambda s: [vocab[token] for token in s[:-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXM0LgELKbq"
      },
      "source": [
        "data_pairs = [(pair[0], pair[1]) for pair in data.loc[:,['text','stars']].values]\n",
        "if len(data_pairs) > config.dataset_size:\n",
        "    data_pairs = random.sample(data_pairs, config.dataset_size)\n",
        "\n",
        "random.shuffle(data_pairs)\n",
        "valid_len = round(0.15 * len(data_pairs))\n",
        "valid_data, test_data, train_data = data_pairs[:valid_len], data_pairs[valid_len:2*valid_len], data_pairs[2*valid_len:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm-HfNP9LUNf"
      },
      "source": [
        "def get_padded_batches(data, batch_size, padding_index):\n",
        "    batches = []\n",
        "    seq_lengths = []\n",
        "    for k in range(batch_size, len(data), batch_size):\n",
        "        batch = [pair[0] for pair in data[k-batch_size:k]]\n",
        "        lengths = [len(text) for text in batch]\n",
        "        padded_length = max(lengths)\n",
        "        for text in batch:\n",
        "            while len(text) < padded_length:\n",
        "                text.append(padding_index)\n",
        "        batches.append(torch.tensor(batch))\n",
        "        seq_lengths.append(torch.tensor(lengths))\n",
        "    return batches, seq_lengths\n",
        "\n",
        "train_batches, train_lengths = get_padded_batches(train_data, config.batch_size, vocab.padding_index)\n",
        "valid_batches, valid_lengths = get_padded_batches(valid_data, config.batch_size, vocab.padding_index)\n",
        "test_batches, test_lengths = get_padded_batches(test_data, config.batch_size, vocab.padding_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK5x-aPqwVOA"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxvnGdq3wWdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259177ac-8c33-4977-fdfb-5a8f14b99da8"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = {\n",
        "    \"train\": list(zip(train_batches, train_lengths)),\n",
        "    \"valid\": list(zip(valid_batches, valid_lengths)),\n",
        "    \"test\": list(zip(test_batches, test_lengths))\n",
        "}\n",
        "\n",
        "opt_vars = {\n",
        "    'learning_rate': config.lr_decay_hparams[\"init_lr\"],\n",
        "    'best_valid_nll': 1e100,\n",
        "    'steps_not_improved': 0,\n",
        "    'kl_weight': 1.0\n",
        "}\n",
        "\n",
        "decay_cnt = 0\n",
        "max_decay = config.lr_decay_hparams[\"max_decay\"]\n",
        "decay_factor = config.lr_decay_hparams[\"decay_factor\"]\n",
        "decay_ts = config.lr_decay_hparams[\"threshold\"]\n",
        "\n",
        "save_path = './checkpoint.ckpt'\n",
        "\n",
        "model = VAE(config, len(vocab.itos), vocab.padding_index)\n",
        "\n",
        "optimizer = tx.core.get_optimizer(\n",
        "    params=model.parameters(),\n",
        "    hparams=config.opt_hparams)\n",
        "scheduler = ExponentialLR(optimizer, decay_factor)\n",
        "\n",
        "max_iter = min(config.num_epochs*len(train_data)/config.batch_size, 80000)\n",
        "print('max steps:', max_iter)\n",
        "\n",
        "global_steps = {}\n",
        "global_steps['step'] = 0\n",
        "pid = PIDControl()\n",
        "opt_vars[\"kl_weight\"] = 1.0\n",
        "Kp = 0.01\n",
        "Ki = -0.0001\n",
        "exp_kl = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max steps: 76562.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwsjM_00yQiU"
      },
      "source": [
        "def _run_epoch(epoch: int, mode: str, display: int = 10) -> Tuple[Tensor, float]:\n",
        "    if mode == 'train':\n",
        "        model.train()\n",
        "        kl_weight = opt_vars[\"kl_weight\"]\n",
        "    else:\n",
        "        model.eval()\n",
        "        kl_weight = 1.0\n",
        "    \n",
        "    num_words = 0\n",
        "    nll_total = 0.\n",
        "\n",
        "    avg_rec = tx.utils.AverageRecorder()\n",
        "    for batch, seq_lengths in dataset[mode]:\n",
        "        if global_steps['step']>= max_iter:\n",
        "            break\n",
        "        mean, logvar = model.encode(batch.cuda(), seq_lengths)\n",
        "        dst = MultivariateNormalDiag(loc=mean, scale_diag=torch.exp(0.5 * logvar))\n",
        "        z = dst.sample()\n",
        "        logits, _ = model.decode(mean, batch[:,:-1].cuda(), seq_lengths-1)\n",
        "        rec_loss = tx.losses.sequence_sparse_softmax_cross_entropy(labels=batch[:,1:].cuda(), logits=logits, sequence_length=(seq_lengths-1).cuda())\n",
        "        kl_loss = kl_divergence(mean, logvar)\n",
        "        total_loss = rec_loss + kl_weight * kl_loss\n",
        "        if mode == \"train\":\n",
        "            pbar.update(1)\n",
        "            global_steps['step'] += 1\n",
        "            if config.pid_control:\n",
        "                kl_weight = pid.pid(exp_kl, kl_loss.item(), Kp, Ki)\n",
        "                opt_vars[\"kl_weight\"] = kl_weight\n",
        "            ## total loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        batch_size = len(batch)\n",
        "        num_words += torch.sum(seq_lengths).item()\n",
        "        nll_total += total_loss.item() * batch_size\n",
        "        avg_rec.add(\n",
        "            [total_loss.item(),\n",
        "              kl_loss.item(),\n",
        "              rec_loss.item()],\n",
        "            batch_size)\n",
        "            \n",
        "        if global_steps['step'] % display == 1 and mode == 'train':\n",
        "            nll = avg_rec.avg(0)\n",
        "            klw = opt_vars[\"kl_weight\"]\n",
        "            KL = avg_rec.avg(1)\n",
        "            rc = avg_rec.avg(2)\n",
        "            \n",
        "    nll = avg_rec.avg(0)\n",
        "    KL = avg_rec.avg(1)\n",
        "    rc = avg_rec.avg(2)\n",
        "    if num_words > 0:\n",
        "        log_ppl = nll_total / num_words\n",
        "        ppl = math.exp(log_ppl)\n",
        "    else:\n",
        "        log_ppl = 100\n",
        "        ppl = math.exp(log_ppl)\n",
        "        nll = 1000\n",
        "        KL = args.exp_kl\n",
        "    \n",
        "    print(f\"\\n{mode}: epoch {epoch}, nll {nll:.4f}, KL {KL:.4f}, \"\n",
        "          f\"rc {rc:.4f}, log_ppl {log_ppl:.4f}, ppl {ppl:.4f}\")\n",
        "    return nll, ppl  # type: ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxMAUE7rytC1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d485994-4153-4253-cf75-116a62346da9"
      },
      "source": [
        "# Counts trainable parameters\n",
        "total_parameters = sum(param.numel() for param in model.parameters())\n",
        "print(f\"{total_parameters} total parameters\")\n",
        "\n",
        "best_nll = best_ppl = 0.\n",
        "\n",
        "## start running model\n",
        "pbar = tqdm(total = int(max_iter))\n",
        "train_losses, val_losses = [], []\n",
        "beta = []\n",
        "for epoch in range(config.num_epochs):\n",
        "    train_nll, _ = _run_epoch(epoch, 'train', display=200)\n",
        "    val_nll, _ = _run_epoch(epoch, 'valid')\n",
        "    test_nll, test_ppl = _run_epoch(epoch, 'test')\n",
        "    train_losses.append(train_nll)\n",
        "    val_losses.append(val_nll)\n",
        "    beta.append(opt_vars[\"kl_weight\"])\n",
        "\n",
        "    if val_nll < opt_vars['best_valid_nll']:\n",
        "        opt_vars['best_valid_nll'] = val_nll\n",
        "        opt_vars['steps_not_improved'] = 0\n",
        "        best_nll = test_nll\n",
        "        best_ppl = test_ppl\n",
        "\n",
        "        states = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"scheduler\": scheduler.state_dict()\n",
        "        }\n",
        "        torch.save(states, save_path)\n",
        "    else:\n",
        "        opt_vars['steps_not_improved'] += 1\n",
        "        if opt_vars['steps_not_improved'] == decay_ts:\n",
        "            old_lr = opt_vars['learning_rate']\n",
        "            opt_vars['learning_rate'] *= decay_factor\n",
        "            opt_vars['steps_not_improved'] = 0\n",
        "            new_lr = opt_vars['learning_rate']\n",
        "            ckpt = torch.load(save_path)\n",
        "            model.load_state_dict(ckpt['model'])\n",
        "            optimizer.load_state_dict(ckpt['optimizer'])\n",
        "            scheduler.load_state_dict(ckpt['scheduler'])\n",
        "            scheduler.step()\n",
        "            print(f\"-----\\nchange lr, old lr: {old_lr}, \"\n",
        "                  f\"new lr: {new_lr}\\n-----\")\n",
        "\n",
        "            decay_cnt += 1\n",
        "            if decay_cnt == max_decay:\n",
        "                break\n",
        "    if global_steps['step'] >= max_iter:\n",
        "        break\n",
        "\n",
        "print(f\"\\nbest testing nll: {best_nll:.4f},\"\n",
        "      f\"best testing ppl {best_ppl:.4f}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/76562 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7259426 total parameters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 2/76562 [00:00<3:01:20,  7.04it/s]\u001b[A\n",
            "  0%|          | 3/76562 [00:00<3:21:26,  6.33it/s]\u001b[A\n",
            "  0%|          | 4/76562 [00:00<3:55:08,  5.43it/s]\u001b[A\n",
            "  0%|          | 5/76562 [00:00<3:35:39,  5.92it/s]\u001b[A\n",
            "  0%|          | 6/76562 [00:01<3:25:37,  6.21it/s]\u001b[A\n",
            "  0%|          | 7/76562 [00:01<3:38:08,  5.85it/s]\u001b[A\n",
            "  0%|          | 8/76562 [00:01<3:49:36,  5.56it/s]\u001b[A\n",
            "  0%|          | 9/76562 [00:01<3:42:29,  5.73it/s]\u001b[A\n",
            "  0%|          | 10/76562 [00:01<3:45:52,  5.65it/s]\u001b[A\n",
            "  0%|          | 11/76562 [00:01<4:05:19,  5.20it/s]\u001b[A\n",
            "  0%|          | 12/76562 [00:02<4:39:21,  4.57it/s]\u001b[A\n",
            "  0%|          | 13/76562 [00:02<5:36:56,  3.79it/s]\u001b[A\n",
            "  0%|          | 14/76562 [00:02<6:01:44,  3.53it/s]\u001b[A\n",
            "  0%|          | 15/76562 [00:03<5:28:54,  3.88it/s]\u001b[A\n",
            "  0%|          | 16/76562 [00:03<4:58:02,  4.28it/s]\u001b[A\n",
            "  0%|          | 17/76562 [00:03<4:43:45,  4.50it/s]\u001b[A\n",
            "  0%|          | 18/76562 [00:03<5:06:16,  4.17it/s]\u001b[A\n",
            "  0%|          | 19/76562 [00:04<4:52:23,  4.36it/s]\u001b[A\n",
            "  0%|          | 20/76562 [00:04<5:04:32,  4.19it/s]\u001b[A\n",
            "  0%|          | 21/76562 [00:04<4:58:52,  4.27it/s]\u001b[A\n",
            "  0%|          | 22/76562 [00:04<4:37:03,  4.60it/s]\u001b[A\n",
            "  0%|          | 23/76562 [00:04<4:41:09,  4.54it/s]\u001b[A\n",
            "  0%|          | 24/76562 [00:05<4:14:40,  5.01it/s]\u001b[A\n",
            "  0%|          | 25/76562 [00:05<4:16:54,  4.97it/s]\u001b[A\n",
            "  0%|          | 26/76562 [00:05<4:22:26,  4.86it/s]\u001b[A\n",
            "  0%|          | 27/76562 [00:05<4:19:09,  4.92it/s]\u001b[A\n",
            "  0%|          | 28/76562 [00:05<4:24:49,  4.82it/s]\u001b[A\n",
            "  0%|          | 29/76562 [00:06<4:21:08,  4.88it/s]\u001b[A\n",
            "  0%|          | 30/76562 [00:06<4:41:50,  4.53it/s]\u001b[A\n",
            "  0%|          | 31/76562 [00:06<4:36:56,  4.61it/s]\u001b[A\n",
            "  0%|          | 32/76562 [00:06<4:40:36,  4.55it/s]\u001b[A\n",
            "  0%|          | 33/76562 [00:07<5:18:57,  4.00it/s]\u001b[A\n",
            "  0%|          | 34/76562 [00:07<5:21:36,  3.97it/s]\u001b[A\n",
            "  0%|          | 35/76562 [00:07<5:27:45,  3.89it/s]\u001b[A\n",
            "  0%|          | 36/76562 [00:07<5:32:12,  3.84it/s]\u001b[A\n",
            "  0%|          | 37/76562 [00:08<5:25:34,  3.92it/s]\u001b[A\n",
            "  0%|          | 38/76562 [00:08<4:49:45,  4.40it/s]\u001b[A\n",
            "  0%|          | 39/76562 [00:08<4:30:12,  4.72it/s]\u001b[A\n",
            "  0%|          | 40/76562 [00:08<4:55:32,  4.32it/s]\u001b[A\n",
            "  0%|          | 41/76562 [00:08<4:39:36,  4.56it/s]\u001b[A\n",
            "  0%|          | 42/76562 [00:09<4:57:03,  4.29it/s]\u001b[A\n",
            "  0%|          | 43/76562 [00:09<4:52:35,  4.36it/s]\u001b[A\n",
            "  0%|          | 44/76562 [00:09<4:38:13,  4.58it/s]\u001b[A\n",
            "  0%|          | 45/76562 [00:09<4:37:20,  4.60it/s]\u001b[A\n",
            "  0%|          | 46/76562 [00:10<4:43:49,  4.49it/s]\u001b[A\n",
            "  0%|          | 47/76562 [00:10<4:45:56,  4.46it/s]\u001b[A\n",
            "  0%|          | 48/76562 [00:10<5:04:54,  4.18it/s]\u001b[A\n",
            "  0%|          | 49/76562 [00:10<4:55:57,  4.31it/s]\u001b[A\n",
            "  0%|          | 50/76562 [00:11<4:58:05,  4.28it/s]\u001b[A\n",
            "  0%|          | 51/76562 [00:11<4:54:47,  4.33it/s]\u001b[A\n",
            "  0%|          | 52/76562 [00:11<5:13:18,  4.07it/s]\u001b[A\n",
            "  0%|          | 53/76562 [00:11<5:44:46,  3.70it/s]\u001b[A\n",
            "  0%|          | 54/76562 [00:12<6:15:19,  3.40it/s]\u001b[A\n",
            "  0%|          | 55/76562 [00:12<6:19:49,  3.36it/s]\u001b[A\n",
            "  0%|          | 56/76562 [00:12<5:48:09,  3.66it/s]\u001b[A\n",
            "  0%|          | 57/76562 [00:13<6:01:43,  3.53it/s]\u001b[A\n",
            "  0%|          | 58/76562 [00:13<5:50:07,  3.64it/s]\u001b[A\n",
            "  0%|          | 59/76562 [00:13<5:26:07,  3.91it/s]\u001b[A\n",
            "  0%|          | 60/76562 [00:13<4:48:39,  4.42it/s]\u001b[A\n",
            "  0%|          | 61/76562 [00:13<4:13:27,  5.03it/s]\u001b[A\n",
            "  0%|          | 62/76562 [00:14<4:20:48,  4.89it/s]\u001b[A\n",
            "  0%|          | 63/76562 [00:14<4:23:58,  4.83it/s]\u001b[A\n",
            "  0%|          | 64/76562 [00:14<4:32:46,  4.67it/s]\u001b[A\n",
            "  0%|          | 65/76562 [00:14<4:44:37,  4.48it/s]\u001b[A\n",
            "  0%|          | 66/76562 [00:14<4:46:30,  4.45it/s]\u001b[A\n",
            "  0%|          | 67/76562 [00:15<4:34:19,  4.65it/s]\u001b[A\n",
            "  0%|          | 68/76562 [00:15<4:50:29,  4.39it/s]\u001b[A\n",
            "  0%|          | 69/76562 [00:15<4:34:35,  4.64it/s]\u001b[A\n",
            "  0%|          | 70/76562 [00:15<4:35:15,  4.63it/s]\u001b[A\n",
            "  0%|          | 71/76562 [00:16<5:15:32,  4.04it/s]\u001b[A\n",
            "  0%|          | 72/76562 [00:16<5:29:25,  3.87it/s]\u001b[A\n",
            "  0%|          | 73/76562 [00:16<5:20:28,  3.98it/s]\u001b[A\n",
            "  0%|          | 74/76562 [00:16<5:11:40,  4.09it/s]\u001b[A\n",
            "  0%|          | 75/76562 [00:17<4:48:09,  4.42it/s]\u001b[A\n",
            "  0%|          | 76/76562 [00:17<4:49:53,  4.40it/s]\u001b[A\n",
            "  0%|          | 77/76562 [00:17<5:18:07,  4.01it/s]\u001b[A\n",
            "  0%|          | 78/76562 [00:17<5:39:39,  3.75it/s]\u001b[A\n",
            "  0%|          | 79/76562 [00:18<5:17:09,  4.02it/s]\u001b[A\n",
            "  0%|          | 80/76562 [00:18<4:39:53,  4.55it/s]\u001b[A\n",
            "  0%|          | 81/76562 [00:18<4:36:07,  4.62it/s]\u001b[A\n",
            "  0%|          | 82/76562 [00:18<5:11:34,  4.09it/s]\u001b[A\n",
            "  0%|          | 83/76562 [00:19<5:41:13,  3.74it/s]\u001b[A\n",
            "  0%|          | 84/76562 [00:19<5:11:30,  4.09it/s]\u001b[A\n",
            "  0%|          | 85/76562 [00:19<4:37:30,  4.59it/s]\u001b[A\n",
            "  0%|          | 86/76562 [00:19<4:20:11,  4.90it/s]\u001b[A\n",
            "  0%|          | 87/76562 [00:19<4:40:25,  4.55it/s]\u001b[A\n",
            "  0%|          | 88/76562 [00:19<3:58:43,  5.34it/s]\u001b[A\n",
            "  0%|          | 89/76562 [00:20<3:53:31,  5.46it/s]\u001b[A\n",
            "  0%|          | 90/76562 [00:20<4:45:21,  4.47it/s]\u001b[A\n",
            "  0%|          | 91/76562 [00:20<4:38:49,  4.57it/s]\u001b[A\n",
            "  0%|          | 92/76562 [00:20<4:25:33,  4.80it/s]\u001b[A\n",
            "  0%|          | 93/76562 [00:21<4:14:10,  5.01it/s]\u001b[A\n",
            "  0%|          | 94/76562 [00:21<4:36:28,  4.61it/s]\u001b[A\n",
            "  0%|          | 95/76562 [00:21<4:58:39,  4.27it/s]\u001b[A\n",
            "  0%|          | 96/76562 [00:21<5:05:29,  4.17it/s]\u001b[A\n",
            "  0%|          | 97/76562 [00:22<5:43:26,  3.71it/s]\u001b[A\n",
            "  0%|          | 98/76562 [00:22<5:02:16,  4.22it/s]\u001b[A\n",
            "  0%|          | 99/76562 [00:22<4:35:38,  4.62it/s]\u001b[A\n",
            "  0%|          | 100/76562 [00:22<4:36:27,  4.61it/s]\u001b[A\n",
            "  0%|          | 101/76562 [00:22<4:30:34,  4.71it/s]\u001b[A\n",
            "  0%|          | 102/76562 [00:23<4:57:46,  4.28it/s]\u001b[A\n",
            "  0%|          | 103/76562 [00:23<5:15:57,  4.03it/s]\u001b[A\n",
            "  0%|          | 104/76562 [00:23<4:58:21,  4.27it/s]\u001b[A\n",
            "  0%|          | 105/76562 [00:23<4:42:19,  4.51it/s]\u001b[A\n",
            "  0%|          | 106/76562 [00:24<4:46:17,  4.45it/s]\u001b[A\n",
            "  0%|          | 107/76562 [00:24<4:24:17,  4.82it/s]\u001b[A\n",
            "  0%|          | 108/76562 [00:24<4:02:08,  5.26it/s]\u001b[A\n",
            "  0%|          | 109/76562 [00:24<4:14:11,  5.01it/s]\u001b[A\n",
            "  0%|          | 110/76562 [00:24<4:16:59,  4.96it/s]\u001b[A\n",
            "  0%|          | 111/76562 [00:25<4:50:18,  4.39it/s]\u001b[A\n",
            "  0%|          | 112/76562 [00:25<4:35:36,  4.62it/s]\u001b[A\n",
            "  0%|          | 113/76562 [00:25<4:27:36,  4.76it/s]\u001b[A\n",
            "  0%|          | 114/76562 [00:25<4:21:02,  4.88it/s]\u001b[A\n",
            "  0%|          | 115/76562 [00:25<4:06:20,  5.17it/s]\u001b[A\n",
            "  0%|          | 116/76562 [00:26<4:32:57,  4.67it/s]\u001b[A\n",
            "  0%|          | 117/76562 [00:26<5:00:47,  4.24it/s]\u001b[A\n",
            "  0%|          | 118/76562 [00:26<5:12:49,  4.07it/s]\u001b[A\n",
            "  0%|          | 119/76562 [00:26<5:14:01,  4.06it/s]\u001b[A\n",
            "  0%|          | 120/76562 [00:27<5:27:28,  3.89it/s]\u001b[A\n",
            "  0%|          | 121/76562 [00:27<4:50:11,  4.39it/s]\u001b[A\n",
            "  0%|          | 122/76562 [00:27<4:26:50,  4.77it/s]\u001b[A\n",
            "  0%|          | 123/76562 [00:27<4:33:20,  4.66it/s]\u001b[A\n",
            "  0%|          | 124/76562 [00:28<4:55:31,  4.31it/s]\u001b[A\n",
            "  0%|          | 125/76562 [00:28<5:13:02,  4.07it/s]\u001b[A\n",
            "  0%|          | 126/76562 [00:28<5:19:29,  3.99it/s]\u001b[A\n",
            "  0%|          | 127/76562 [00:28<5:23:37,  3.94it/s]\u001b[A\n",
            "  0%|          | 128/76562 [00:29<5:06:22,  4.16it/s]\u001b[A\n",
            "  0%|          | 129/76562 [00:29<5:15:01,  4.04it/s]\u001b[A\n",
            "  0%|          | 130/76562 [00:29<4:49:02,  4.41it/s]\u001b[A\n",
            "  0%|          | 131/76562 [00:29<4:37:18,  4.59it/s]\u001b[A\n",
            "  0%|          | 132/76562 [00:29<4:27:54,  4.75it/s]\u001b[A\n",
            "  0%|          | 133/76562 [00:30<4:49:02,  4.41it/s]\u001b[A\n",
            "  0%|          | 134/76562 [00:30<5:11:13,  4.09it/s]\u001b[A\n",
            "  0%|          | 135/76562 [00:30<5:17:13,  4.02it/s]\u001b[A\n",
            "  0%|          | 136/76562 [00:30<5:09:52,  4.11it/s]\u001b[A\n",
            "  0%|          | 137/76562 [00:31<5:51:22,  3.63it/s]\u001b[A\n",
            "  0%|          | 138/76562 [00:31<5:40:10,  3.74it/s]\u001b[A\n",
            "  0%|          | 139/76562 [00:31<6:05:30,  3.48it/s]\u001b[A\n",
            "  0%|          | 140/76562 [00:32<5:40:30,  3.74it/s]\u001b[A\n",
            "  0%|          | 141/76562 [00:32<4:51:43,  4.37it/s]\u001b[A\n",
            "  0%|          | 142/76562 [00:32<4:31:11,  4.70it/s]\u001b[A\n",
            "  0%|          | 143/76562 [00:32<4:23:15,  4.84it/s]\u001b[A\n",
            "  0%|          | 144/76562 [00:32<4:30:18,  4.71it/s]\u001b[A\n",
            "  0%|          | 145/76562 [00:33<5:06:55,  4.15it/s]\u001b[A\n",
            "  0%|          | 146/76562 [00:33<5:01:07,  4.23it/s]\u001b[A\n",
            "  0%|          | 147/76562 [00:33<4:51:07,  4.37it/s]\u001b[A\n",
            "  0%|          | 148/76562 [00:33<4:37:12,  4.59it/s]\u001b[A\n",
            "  0%|          | 149/76562 [00:34<4:54:45,  4.32it/s]\u001b[A\n",
            "  0%|          | 150/76562 [00:34<5:20:14,  3.98it/s]\u001b[A\n",
            "  0%|          | 151/76562 [00:34<5:26:04,  3.91it/s]\u001b[A\n",
            "  0%|          | 152/76562 [00:34<5:05:50,  4.16it/s]\u001b[A\n",
            "  0%|          | 153/76562 [00:34<4:47:24,  4.43it/s]\u001b[A\n",
            "  0%|          | 154/76562 [00:35<4:32:57,  4.67it/s]\u001b[A\n",
            "  0%|          | 155/76562 [00:35<4:19:10,  4.91it/s]\u001b[A\n",
            "  0%|          | 156/76562 [00:35<4:09:17,  5.11it/s]\u001b[A\n",
            "  0%|          | 157/76562 [00:35<3:52:51,  5.47it/s]\u001b[A\n",
            "  0%|          | 158/76562 [00:35<4:06:53,  5.16it/s]\u001b[A\n",
            "  0%|          | 159/76562 [00:36<4:43:18,  4.49it/s]\u001b[A\n",
            "  0%|          | 160/76562 [00:36<5:07:29,  4.14it/s]\u001b[A\n",
            "  0%|          | 161/76562 [00:36<4:58:20,  4.27it/s]\u001b[A\n",
            "  0%|          | 162/76562 [00:36<4:53:21,  4.34it/s]\u001b[A\n",
            "  0%|          | 163/76562 [00:37<4:58:18,  4.27it/s]\u001b[A\n",
            "  0%|          | 164/76562 [00:37<4:45:35,  4.46it/s]\u001b[A\n",
            "  0%|          | 165/76562 [00:37<4:32:34,  4.67it/s]\u001b[A\n",
            "  0%|          | 166/76562 [00:37<4:29:06,  4.73it/s]\u001b[A\n",
            "  0%|          | 167/76562 [00:37<4:15:10,  4.99it/s]\u001b[A\n",
            "  0%|          | 168/76562 [00:38<4:02:57,  5.24it/s]\u001b[A\n",
            "  0%|          | 169/76562 [00:38<4:04:38,  5.20it/s]\u001b[A\n",
            "  0%|          | 170/76562 [00:38<4:22:51,  4.84it/s]\u001b[A\n",
            "  0%|          | 171/76562 [00:38<4:24:55,  4.81it/s]\u001b[A\n",
            "  0%|          | 172/76562 [00:38<4:49:13,  4.40it/s]\u001b[A\n",
            "  0%|          | 173/76562 [00:39<4:29:16,  4.73it/s]\u001b[A\n",
            "  0%|          | 174/76562 [00:39<4:30:48,  4.70it/s]\u001b[A\n",
            "  0%|          | 175/76562 [00:39<4:38:17,  4.57it/s]\u001b[A\n",
            "  0%|          | 176/76562 [00:39<4:38:44,  4.57it/s]\u001b[A\n",
            "  0%|          | 177/76562 [00:40<4:18:40,  4.92it/s]\u001b[A\n",
            "  0%|          | 178/76562 [00:40<4:14:09,  5.01it/s]\u001b[A\n",
            "  0%|          | 179/76562 [00:40<4:36:17,  4.61it/s]\u001b[A\n",
            "  0%|          | 180/76562 [00:40<4:24:12,  4.82it/s]\u001b[A\n",
            "  0%|          | 181/76562 [00:40<4:25:01,  4.80it/s]\u001b[A\n",
            "  0%|          | 182/76562 [00:41<4:35:43,  4.62it/s]\u001b[A\n",
            "  0%|          | 183/76562 [00:41<4:28:44,  4.74it/s]\u001b[A\n",
            "  0%|          | 184/76562 [00:41<4:33:54,  4.65it/s]\u001b[A\n",
            "  0%|          | 185/76562 [00:41<4:29:47,  4.72it/s]\u001b[A\n",
            "  0%|          | 186/76562 [00:41<4:22:52,  4.84it/s]\u001b[A\n",
            "  0%|          | 187/76562 [00:42<4:39:06,  4.56it/s]\u001b[A\n",
            "  0%|          | 188/76562 [00:42<5:42:06,  3.72it/s]\u001b[A\n",
            "  0%|          | 189/76562 [00:42<6:07:32,  3.46it/s]\u001b[A\n",
            "  0%|          | 190/76562 [00:43<5:27:52,  3.88it/s]\u001b[A\n",
            "  0%|          | 191/76562 [00:43<4:56:06,  4.30it/s]\u001b[A\n",
            "  0%|          | 192/76562 [00:43<4:27:14,  4.76it/s]\u001b[A\n",
            "  0%|          | 193/76562 [00:43<4:03:31,  5.23it/s]\u001b[A\n",
            "  0%|          | 194/76562 [00:43<4:08:50,  5.11it/s]\u001b[A\n",
            "  0%|          | 195/76562 [00:43<4:05:39,  5.18it/s]\u001b[A\n",
            "  0%|          | 196/76562 [00:44<3:58:22,  5.34it/s]\u001b[A\n",
            "  0%|          | 197/76562 [00:44<3:48:51,  5.56it/s]\u001b[A\n",
            "  0%|          | 198/76562 [00:44<3:47:09,  5.60it/s]\u001b[A\n",
            "  0%|          | 199/76562 [00:44<3:56:20,  5.39it/s]\u001b[A\n",
            "  0%|          | 200/76562 [00:44<4:01:15,  5.28it/s]\u001b[A\n",
            "  0%|          | 201/76562 [00:45<4:27:47,  4.75it/s]\u001b[A\n",
            "  0%|          | 202/76562 [00:45<4:27:21,  4.76it/s]\u001b[A\n",
            "  0%|          | 203/76562 [00:45<4:18:53,  4.92it/s]\u001b[A\n",
            "  0%|          | 204/76562 [00:45<4:15:33,  4.98it/s]\u001b[A\n",
            "  0%|          | 205/76562 [00:45<4:07:10,  5.15it/s]\u001b[A\n",
            "  0%|          | 206/76562 [00:46<4:25:32,  4.79it/s]\u001b[A\n",
            "  0%|          | 207/76562 [00:46<4:32:45,  4.67it/s]\u001b[A\n",
            "  0%|          | 208/76562 [00:46<4:58:40,  4.26it/s]\u001b[A\n",
            "  0%|          | 209/76562 [00:46<4:33:53,  4.65it/s]\u001b[A\n",
            "  0%|          | 210/76562 [00:47<4:44:43,  4.47it/s]\u001b[A\n",
            "  0%|          | 211/76562 [00:47<4:50:37,  4.38it/s]\u001b[A\n",
            "  0%|          | 212/76562 [00:47<4:37:24,  4.59it/s]\u001b[A\n",
            "  0%|          | 213/76562 [00:47<4:04:54,  5.20it/s]\u001b[A\n",
            "  0%|          | 214/76562 [00:47<4:19:13,  4.91it/s]\u001b[A\n",
            "  0%|          | 215/76562 [00:48<4:11:07,  5.07it/s]\u001b[A\n",
            "  0%|          | 216/76562 [00:48<4:11:50,  5.05it/s]\u001b[A\n",
            "  0%|          | 217/76562 [00:48<4:16:31,  4.96it/s]\u001b[A\n",
            "  0%|          | 218/76562 [00:48<4:15:17,  4.98it/s]\u001b[A\n",
            "  0%|          | 219/76562 [00:48<4:09:19,  5.10it/s]\u001b[A\n",
            "  0%|          | 220/76562 [00:48<4:01:33,  5.27it/s]\u001b[A\n",
            "  0%|          | 221/76562 [00:49<4:27:21,  4.76it/s]\u001b[A\n",
            "  0%|          | 222/76562 [00:49<5:07:17,  4.14it/s]\u001b[A\n",
            "  0%|          | 223/76562 [00:49<4:50:17,  4.38it/s]\u001b[A\n",
            "  0%|          | 224/76562 [00:50<5:12:08,  4.08it/s]\u001b[A\n",
            "  0%|          | 225/76562 [00:50<5:47:00,  3.67it/s]\u001b[A\n",
            "  0%|          | 226/76562 [00:50<5:24:08,  3.93it/s]\u001b[A\n",
            "  0%|          | 227/76562 [00:50<4:57:17,  4.28it/s]\u001b[A\n",
            "  0%|          | 228/76562 [00:50<4:49:01,  4.40it/s]\u001b[A\n",
            "  0%|          | 229/76562 [00:51<5:00:27,  4.23it/s]\u001b[A\n",
            "  0%|          | 230/76562 [00:51<4:45:01,  4.46it/s]\u001b[A\n",
            "  0%|          | 231/76562 [00:51<4:29:10,  4.73it/s]\u001b[A\n",
            "  0%|          | 232/76562 [00:51<4:25:51,  4.79it/s]\u001b[A\n",
            "  0%|          | 233/76562 [00:52<4:37:01,  4.59it/s]\u001b[A\n",
            "  0%|          | 234/76562 [00:52<4:38:49,  4.56it/s]\u001b[A\n",
            "  0%|          | 235/76562 [00:52<4:50:48,  4.37it/s]\u001b[A\n",
            "  0%|          | 236/76562 [00:52<5:20:34,  3.97it/s]\u001b[A\n",
            "  0%|          | 237/76562 [00:53<5:21:08,  3.96it/s]\u001b[A\n",
            "  0%|          | 238/76562 [00:53<5:26:36,  3.89it/s]\u001b[A\n",
            "  0%|          | 239/76562 [00:53<4:58:49,  4.26it/s]\u001b[A\n",
            "  0%|          | 240/76562 [00:53<4:28:13,  4.74it/s]\u001b[A\n",
            "  0%|          | 241/76562 [00:53<4:34:28,  4.63it/s]\u001b[A\n",
            "  0%|          | 242/76562 [00:54<4:54:55,  4.31it/s]\u001b[A\n",
            "  0%|          | 243/76562 [00:54<4:51:56,  4.36it/s]\u001b[A\n",
            "  0%|          | 244/76562 [00:54<5:06:08,  4.15it/s]\u001b[A\n",
            "  0%|          | 245/76562 [00:54<4:35:26,  4.62it/s]\u001b[A\n",
            "  0%|          | 246/76562 [00:55<4:18:44,  4.92it/s]\u001b[A\n",
            "  0%|          | 247/76562 [00:55<4:19:38,  4.90it/s]\u001b[A\n",
            "  0%|          | 248/76562 [00:55<5:07:49,  4.13it/s]\u001b[A\n",
            "  0%|          | 249/76562 [00:55<5:12:23,  4.07it/s]\u001b[A\n",
            "  0%|          | 250/76562 [00:56<5:03:22,  4.19it/s]\u001b[A\n",
            "  0%|          | 251/76562 [00:56<4:45:41,  4.45it/s]\u001b[A\n",
            "  0%|          | 252/76562 [00:56<4:35:21,  4.62it/s]\u001b[A\n",
            "  0%|          | 253/76562 [00:56<4:52:13,  4.35it/s]\u001b[A\n",
            "  0%|          | 254/76562 [00:56<4:49:52,  4.39it/s]\u001b[A\n",
            "  0%|          | 255/76562 [00:57<4:48:39,  4.41it/s]\u001b[A\n",
            "  0%|          | 256/76562 [00:57<4:56:40,  4.29it/s]\u001b[A\n",
            "  0%|          | 257/76562 [00:57<5:03:17,  4.19it/s]\u001b[A\n",
            "  0%|          | 258/76562 [00:57<4:47:54,  4.42it/s]\u001b[A\n",
            "  0%|          | 259/76562 [00:58<5:30:32,  3.85it/s]\u001b[A\n",
            "  0%|          | 260/76562 [00:58<5:17:09,  4.01it/s]\u001b[A\n",
            "  0%|          | 261/76562 [00:58<5:19:14,  3.98it/s]\u001b[A\n",
            "  0%|          | 262/76562 [00:58<5:33:01,  3.82it/s]\u001b[A\n",
            "  0%|          | 263/76562 [00:59<4:51:05,  4.37it/s]\u001b[A\n",
            "  0%|          | 264/76562 [00:59<4:37:54,  4.58it/s]\u001b[A\n",
            "  0%|          | 265/76562 [00:59<4:15:50,  4.97it/s]\u001b[A\n",
            "  0%|          | 266/76562 [00:59<4:00:06,  5.30it/s]\u001b[A\n",
            "  0%|          | 267/76562 [00:59<3:52:39,  5.47it/s]\u001b[A\n",
            "  0%|          | 268/76562 [00:59<3:58:42,  5.33it/s]\u001b[A\n",
            "  0%|          | 269/76562 [01:00<4:03:31,  5.22it/s]\u001b[A\n",
            "  0%|          | 270/76562 [01:00<4:18:51,  4.91it/s]\u001b[A\n",
            "  0%|          | 271/76562 [01:00<4:56:44,  4.28it/s]\u001b[A\n",
            "  0%|          | 272/76562 [01:00<4:59:31,  4.25it/s]\u001b[A\n",
            "  0%|          | 273/76562 [01:01<5:05:39,  4.16it/s]\u001b[A\n",
            "  0%|          | 274/76562 [01:01<4:48:51,  4.40it/s]\u001b[A\n",
            "  0%|          | 275/76562 [01:01<4:40:12,  4.54it/s]\u001b[A\n",
            "  0%|          | 276/76562 [01:01<4:39:24,  4.55it/s]\u001b[A\n",
            "  0%|          | 277/76562 [01:02<5:20:03,  3.97it/s]\u001b[A\n",
            "  0%|          | 278/76562 [01:02<5:24:49,  3.91it/s]\u001b[A\n",
            "  0%|          | 279/76562 [01:02<5:11:14,  4.08it/s]\u001b[A\n",
            "  0%|          | 280/76562 [01:02<5:18:52,  3.99it/s]\u001b[A\n",
            "  0%|          | 281/76562 [01:03<4:47:21,  4.42it/s]\u001b[A\n",
            "  0%|          | 282/76562 [01:03<4:42:31,  4.50it/s]\u001b[A\n",
            "  0%|          | 283/76562 [01:03<4:50:26,  4.38it/s]\u001b[A\n",
            "  0%|          | 284/76562 [01:03<4:44:11,  4.47it/s]\u001b[A\n",
            "  0%|          | 285/76562 [01:03<4:39:16,  4.55it/s]\u001b[A\n",
            "  0%|          | 286/76562 [01:04<4:23:55,  4.82it/s]\u001b[A\n",
            "  0%|          | 287/76562 [01:04<4:35:02,  4.62it/s]\u001b[A\n",
            "  0%|          | 288/76562 [01:04<4:20:00,  4.89it/s]\u001b[A\n",
            "  0%|          | 289/76562 [01:04<4:17:02,  4.95it/s]\u001b[A\n",
            "  0%|          | 290/76562 [01:04<4:13:36,  5.01it/s]\u001b[A\n",
            "  0%|          | 291/76562 [01:05<4:32:21,  4.67it/s]\u001b[A\n",
            "  0%|          | 292/76562 [01:05<4:32:19,  4.67it/s]\u001b[A\n",
            "  0%|          | 293/76562 [01:05<4:11:42,  5.05it/s]\u001b[A\n",
            "  0%|          | 294/76562 [01:05<4:19:18,  4.90it/s]\u001b[A\n",
            "  0%|          | 295/76562 [01:06<4:36:33,  4.60it/s]\u001b[A\n",
            "  0%|          | 296/76562 [01:06<4:39:19,  4.55it/s]\u001b[A\n",
            "  0%|          | 297/76562 [01:06<4:19:17,  4.90it/s]\u001b[A\n",
            "  0%|          | 298/76562 [01:06<4:34:39,  4.63it/s]\u001b[A\n",
            "  0%|          | 299/76562 [01:06<4:35:31,  4.61it/s]\u001b[A\n",
            "  0%|          | 300/76562 [01:07<4:58:03,  4.26it/s]\u001b[A\n",
            "  0%|          | 301/76562 [01:07<4:39:21,  4.55it/s]\u001b[A\n",
            "  0%|          | 302/76562 [01:07<4:40:06,  4.54it/s]\u001b[A\n",
            "  0%|          | 303/76562 [01:07<5:03:29,  4.19it/s]\u001b[A\n",
            "  0%|          | 304/76562 [01:08<5:04:02,  4.18it/s]\u001b[A\n",
            "  0%|          | 305/76562 [01:08<5:09:35,  4.11it/s]\u001b[A\n",
            "  0%|          | 306/76562 [01:08<4:57:43,  4.27it/s]\u001b[A\n",
            "  0%|          | 307/76562 [01:08<4:59:26,  4.24it/s]\u001b[A\n",
            "  0%|          | 308/76562 [01:09<5:38:49,  3.75it/s]\u001b[A\n",
            "  0%|          | 309/76562 [01:09<5:06:10,  4.15it/s]\u001b[A\n",
            "  0%|          | 310/76562 [01:09<4:44:21,  4.47it/s]\u001b[A\n",
            "  0%|          | 311/76562 [01:09<4:36:10,  4.60it/s]\u001b[A\n",
            "  0%|          | 312/76562 [01:09<4:24:07,  4.81it/s]\u001b[A\n",
            "  0%|          | 313/76562 [01:10<4:16:27,  4.96it/s]\u001b[A\n",
            "  0%|          | 314/76562 [01:10<4:16:34,  4.95it/s]\u001b[A\n",
            "  0%|          | 315/76562 [01:10<4:07:39,  5.13it/s]\u001b[A\n",
            "  0%|          | 316/76562 [01:10<4:15:21,  4.98it/s]\u001b[A\n",
            "  0%|          | 317/76562 [01:10<3:52:57,  5.45it/s]\u001b[A\n",
            "  0%|          | 318/76562 [01:10<3:49:14,  5.54it/s]\u001b[A\n",
            "  0%|          | 319/76562 [01:11<3:35:19,  5.90it/s]\u001b[A\n",
            "  0%|          | 320/76562 [01:11<3:43:00,  5.70it/s]\u001b[A\n",
            "  0%|          | 321/76562 [01:11<3:53:07,  5.45it/s]\u001b[A\n",
            "  0%|          | 322/76562 [01:11<3:43:04,  5.70it/s]\u001b[A\n",
            "  0%|          | 323/76562 [01:11<3:57:20,  5.35it/s]\u001b[A\n",
            "  0%|          | 324/76562 [01:12<4:45:09,  4.46it/s]\u001b[A\n",
            "  0%|          | 325/76562 [01:12<4:51:34,  4.36it/s]\u001b[A\n",
            "  0%|          | 326/76562 [01:12<4:54:35,  4.31it/s]\u001b[A\n",
            "  0%|          | 327/76562 [01:12<5:02:06,  4.21it/s]\u001b[A\n",
            "  0%|          | 328/76562 [01:13<4:32:33,  4.66it/s]\u001b[A\n",
            "  0%|          | 329/76562 [01:13<4:08:48,  5.11it/s]\u001b[A\n",
            "  0%|          | 330/76562 [01:13<4:05:45,  5.17it/s]\u001b[A\n",
            "  0%|          | 331/76562 [01:13<3:45:37,  5.63it/s]\u001b[A\n",
            "  0%|          | 332/76562 [01:13<3:38:43,  5.81it/s]\u001b[A\n",
            "  0%|          | 333/76562 [01:13<3:35:56,  5.88it/s]\u001b[A\n",
            "  0%|          | 334/76562 [01:14<3:39:47,  5.78it/s]\u001b[A\n",
            "  0%|          | 335/76562 [01:14<4:25:26,  4.79it/s]\u001b[A\n",
            "  0%|          | 336/76562 [01:14<4:57:36,  4.27it/s]\u001b[A\n",
            "  0%|          | 337/76562 [01:14<4:56:07,  4.29it/s]\u001b[A\n",
            "  0%|          | 338/76562 [01:15<4:48:18,  4.41it/s]\u001b[A\n",
            "  0%|          | 339/76562 [01:15<4:36:14,  4.60it/s]\u001b[A\n",
            "  0%|          | 340/76562 [01:15<4:24:46,  4.80it/s]\u001b[A\n",
            "  0%|          | 341/76562 [01:15<4:10:09,  5.08it/s]\u001b[A\n",
            "  0%|          | 342/76562 [01:15<4:16:19,  4.96it/s]\u001b[A\n",
            "  0%|          | 343/76562 [01:16<4:26:20,  4.77it/s]\u001b[A\n",
            "  0%|          | 344/76562 [01:16<5:07:38,  4.13it/s]\u001b[A\n",
            "  0%|          | 345/76562 [01:16<4:39:31,  4.54it/s]\u001b[A\n",
            "  0%|          | 346/76562 [01:16<4:19:59,  4.89it/s]\u001b[A\n",
            "  0%|          | 347/76562 [01:16<4:26:32,  4.77it/s]\u001b[A\n",
            "  0%|          | 348/76562 [01:17<4:11:29,  5.05it/s]\u001b[A\n",
            "  0%|          | 349/76562 [01:17<4:15:45,  4.97it/s]\u001b[A\n",
            "  0%|          | 350/76562 [01:17<4:43:21,  4.48it/s]\u001b[A\n",
            "  0%|          | 351/76562 [01:17<5:11:54,  4.07it/s]\u001b[A\n",
            "  0%|          | 352/76562 [01:18<4:58:43,  4.25it/s]\u001b[A\n",
            "  0%|          | 353/76562 [01:18<4:58:52,  4.25it/s]\u001b[A\n",
            "  0%|          | 354/76562 [01:18<5:15:47,  4.02it/s]\u001b[A\n",
            "  0%|          | 355/76562 [01:18<5:01:15,  4.22it/s]\u001b[A\n",
            "  0%|          | 356/76562 [01:19<5:16:49,  4.01it/s]\u001b[A\n",
            "  0%|          | 357/76562 [01:19<4:51:28,  4.36it/s]\u001b[A\n",
            "  0%|          | 358/76562 [01:19<4:49:55,  4.38it/s]\u001b[A\n",
            "  0%|          | 359/76562 [01:19<4:36:32,  4.59it/s]\u001b[A\n",
            "  0%|          | 360/76562 [01:19<4:11:54,  5.04it/s]\u001b[A\n",
            "  0%|          | 361/76562 [01:20<4:10:42,  5.07it/s]\u001b[A\n",
            "  0%|          | 362/76562 [01:20<4:39:22,  4.55it/s]\u001b[A\n",
            "  0%|          | 363/76562 [01:20<5:24:47,  3.91it/s]\u001b[A\n",
            "  0%|          | 364/76562 [01:20<5:16:47,  4.01it/s]\u001b[A\n",
            "  0%|          | 365/76562 [01:21<5:43:20,  3.70it/s]\u001b[A\n",
            "  0%|          | 366/76562 [01:21<5:35:42,  3.78it/s]\u001b[A\n",
            "  0%|          | 367/76562 [01:21<5:15:31,  4.02it/s]\u001b[A\n",
            "  0%|          | 368/76562 [01:21<5:18:19,  3.99it/s]\u001b[A\n",
            "  0%|          | 369/76562 [01:22<5:43:01,  3.70it/s]\u001b[A\n",
            "  0%|          | 370/76562 [01:22<5:28:22,  3.87it/s]\u001b[A\n",
            "  0%|          | 371/76562 [01:22<5:48:17,  3.65it/s]\u001b[A\n",
            "  0%|          | 372/76562 [01:23<5:15:52,  4.02it/s]\u001b[A\n",
            "  0%|          | 373/76562 [01:23<5:10:59,  4.08it/s]\u001b[A\n",
            "  0%|          | 374/76562 [01:23<5:25:16,  3.90it/s]\u001b[A\n",
            "  0%|          | 375/76562 [01:23<5:29:33,  3.85it/s]\u001b[A\n",
            "  0%|          | 376/76562 [01:23<5:05:51,  4.15it/s]\u001b[A\n",
            "  0%|          | 377/76562 [01:24<4:43:22,  4.48it/s]\u001b[A\n",
            "  0%|          | 378/76562 [01:24<5:01:02,  4.22it/s]\u001b[A\n",
            "  0%|          | 379/76562 [01:24<4:38:30,  4.56it/s]\u001b[A\n",
            "  0%|          | 380/76562 [01:24<4:22:50,  4.83it/s]\u001b[A\n",
            "  0%|          | 381/76562 [01:25<4:43:15,  4.48it/s]\u001b[A\n",
            "  0%|          | 382/76562 [01:25<4:50:07,  4.38it/s]\u001b[A\n",
            "  1%|          | 383/76562 [01:25<5:08:44,  4.11it/s]\u001b[A\n",
            "  1%|          | 384/76562 [01:25<4:34:07,  4.63it/s]\u001b[A\n",
            "  1%|          | 385/76562 [01:25<4:18:01,  4.92it/s]\u001b[A\n",
            "  1%|          | 386/76562 [01:26<4:20:41,  4.87it/s]\u001b[A\n",
            "  1%|          | 387/76562 [01:26<4:04:42,  5.19it/s]\u001b[A\n",
            "  1%|          | 388/76562 [01:26<4:35:49,  4.60it/s]\u001b[A\n",
            "  1%|          | 389/76562 [01:26<5:05:55,  4.15it/s]\u001b[A\n",
            "  1%|          | 390/76562 [01:27<4:51:16,  4.36it/s]\u001b[A\n",
            "  1%|          | 391/76562 [01:27<4:26:15,  4.77it/s]\u001b[A\n",
            "  1%|          | 392/76562 [01:27<4:15:17,  4.97it/s]\u001b[A\n",
            "  1%|          | 393/76562 [01:27<4:14:18,  4.99it/s]\u001b[A\n",
            "  1%|          | 394/76562 [01:27<4:31:27,  4.68it/s]\u001b[A\n",
            "  1%|          | 395/76562 [01:28<4:24:34,  4.80it/s]\u001b[A\n",
            "  1%|          | 396/76562 [01:28<4:13:52,  5.00it/s]\u001b[A\n",
            "  1%|          | 397/76562 [01:28<4:43:50,  4.47it/s]\u001b[A\n",
            "  1%|          | 398/76562 [01:28<4:38:44,  4.55it/s]\u001b[A\n",
            "  1%|          | 399/76562 [01:28<4:21:43,  4.85it/s]\u001b[A\n",
            "  1%|          | 400/76562 [01:29<4:12:59,  5.02it/s]\u001b[A\n",
            "  1%|          | 401/76562 [01:29<4:37:46,  4.57it/s]\u001b[A\n",
            "  1%|          | 402/76562 [01:29<4:47:38,  4.41it/s]\u001b[A\n",
            "  1%|          | 403/76562 [01:29<4:34:06,  4.63it/s]\u001b[A\n",
            "  1%|          | 404/76562 [01:29<4:06:22,  5.15it/s]\u001b[A\n",
            "  1%|          | 405/76562 [01:30<3:50:35,  5.50it/s]\u001b[A\n",
            "  1%|          | 406/76562 [01:30<3:38:11,  5.82it/s]\u001b[A\n",
            "  1%|          | 407/76562 [01:30<3:56:12,  5.37it/s]\u001b[A\n",
            "  1%|          | 408/76562 [01:30<3:54:01,  5.42it/s]\u001b[A\n",
            "  1%|          | 409/76562 [01:30<3:51:44,  5.48it/s]\u001b[A\n",
            "  1%|          | 410/76562 [01:30<3:47:18,  5.58it/s]\u001b[A\n",
            "  1%|          | 411/76562 [01:31<3:51:43,  5.48it/s]\u001b[A\n",
            "  1%|          | 412/76562 [01:31<3:51:02,  5.49it/s]\u001b[A\n",
            "  1%|          | 413/76562 [01:31<4:01:37,  5.25it/s]\u001b[A\n",
            "  1%|          | 414/76562 [01:31<4:11:23,  5.05it/s]\u001b[A\n",
            "  1%|          | 415/76562 [01:31<4:11:34,  5.04it/s]\u001b[A\n",
            "  1%|          | 416/76562 [01:32<4:40:53,  4.52it/s]\u001b[A\n",
            "  1%|          | 417/76562 [01:32<4:56:10,  4.28it/s]\u001b[A\n",
            "  1%|          | 418/76562 [01:32<5:00:58,  4.22it/s]\u001b[A\n",
            "  1%|          | 419/76562 [01:32<5:08:15,  4.12it/s]\u001b[A\n",
            "  1%|          | 420/76562 [01:33<5:10:44,  4.08it/s]\u001b[A\n",
            "  1%|          | 421/76562 [01:33<5:06:19,  4.14it/s]\u001b[A\n",
            "  1%|          | 422/76562 [01:33<5:12:30,  4.06it/s]\u001b[A\n",
            "  1%|          | 423/76562 [01:34<6:02:02,  3.51it/s]\u001b[A\n",
            "  1%|          | 424/76562 [01:34<6:48:05,  3.11it/s]\u001b[A\n",
            "  1%|          | 425/76562 [01:34<6:22:12,  3.32it/s]\u001b[A\n",
            "  1%|          | 426/76562 [01:35<6:38:17,  3.19it/s]\u001b[A\n",
            "  1%|          | 427/76562 [01:35<5:53:31,  3.59it/s]\u001b[A\n",
            "  1%|          | 428/76562 [01:35<5:26:19,  3.89it/s]\u001b[A\n",
            "  1%|          | 429/76562 [01:35<5:00:27,  4.22it/s]\u001b[A\n",
            "  1%|          | 430/76562 [01:35<4:26:48,  4.76it/s]\u001b[A\n",
            "  1%|          | 431/76562 [01:36<4:45:37,  4.44it/s]\u001b[A\n",
            "  1%|          | 432/76562 [01:36<5:37:11,  3.76it/s]\u001b[A\n",
            "  1%|          | 433/76562 [01:36<6:12:51,  3.40it/s]\u001b[A\n",
            "  1%|          | 434/76562 [01:37<6:04:26,  3.48it/s]\u001b[A\n",
            "  1%|          | 435/76562 [01:37<5:40:31,  3.73it/s]\u001b[A\n",
            "  1%|          | 436/76562 [01:37<4:58:52,  4.25it/s]\u001b[A\n",
            "  1%|          | 437/76562 [01:37<4:35:04,  4.61it/s]\u001b[A\n",
            "  1%|          | 438/76562 [01:37<4:10:58,  5.06it/s]\u001b[A\n",
            "  1%|          | 439/76562 [01:37<4:02:44,  5.23it/s]\u001b[A\n",
            "  1%|          | 440/76562 [01:38<4:08:50,  5.10it/s]\u001b[A\n",
            "  1%|          | 441/76562 [01:38<3:48:02,  5.56it/s]\u001b[A\n",
            "  1%|          | 442/76562 [01:38<3:44:53,  5.64it/s]\u001b[A\n",
            "  1%|          | 443/76562 [01:38<4:16:00,  4.96it/s]\u001b[A\n",
            "  1%|          | 444/76562 [01:39<5:01:26,  4.21it/s]\u001b[A\n",
            "  1%|          | 445/76562 [01:39<4:31:07,  4.68it/s]\u001b[A\n",
            "  1%|          | 446/76562 [01:39<4:51:17,  4.36it/s]\u001b[A\n",
            "  1%|          | 447/76562 [01:39<4:54:27,  4.31it/s]\u001b[A\n",
            "  1%|          | 448/76562 [01:39<4:51:10,  4.36it/s]\u001b[A\n",
            "  1%|          | 449/76562 [01:40<5:21:27,  3.95it/s]\u001b[A\n",
            "  1%|          | 450/76562 [01:40<4:54:05,  4.31it/s]\u001b[A\n",
            "  1%|          | 451/76562 [01:40<5:04:32,  4.17it/s]\u001b[A\n",
            "  1%|          | 452/76562 [01:41<5:19:06,  3.98it/s]\u001b[A\n",
            "  1%|          | 453/76562 [01:41<5:47:44,  3.65it/s]\u001b[A\n",
            "  1%|          | 454/76562 [01:41<5:20:38,  3.96it/s]\u001b[A\n",
            "  1%|          | 455/76562 [01:41<5:07:36,  4.12it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-82a31ef604b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mval_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-97eb1eaf3e05>\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(epoch, mode, display)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mopt_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kl_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m## total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgc9JOzGGf-i"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBR2sxL7ryGn"
      },
      "source": [
        "# Load from checkpoint\n",
        "model = VAE(config, len(vocab.itos), vocab.padding_index)\n",
        "ckpt = torch.load(save_path)\n",
        "model.load_state_dict(ckpt['model'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNUYkXKPGh3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab24bdb2-c82c-4d7e-edba-cf8bdcdf9af4"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "batch_size = config.batch_size\n",
        "\n",
        "test_sentence = 'The service was terrible.'\n",
        "tokens = ['<BOS>']+tokenizer(test_sentence)+['<EOS>']\n",
        "text_ids = torch.tensor([[vocab[token] for token in tokens]])\n",
        "mean, logvar = model.encode(text_ids.cuda(), torch.tensor([len(tokens)]))\n",
        "dst = MultivariateNormalDiag(loc=mean[0], scale_diag=torch.exp(logvar[0]))\n",
        "latent_z = dst.sample((batch_size,))\n",
        "\n",
        "# latent_z = torch.FloatTensor(batch_size, config.latent_dims).uniform_(-1, 1).to(device)\n",
        "# latent_z = torch.randn(batch_size, config.latent_dims).to(device)\n",
        "\n",
        "gen_text = None\n",
        "gen_chars = torch.tensor([[vocab.bos_index] for _ in range(batch_size)])\n",
        "state = None\n",
        "while gen_text is None or gen_text.size()[1] < 100:\n",
        "    logits, state = model.decode(latent_z.cuda(), gen_chars.cuda(), torch.ones(batch_size), state)\n",
        "    cdst = Categorical(logits=logits)\n",
        "    gen_chars = cdst.sample()\n",
        "    if gen_text is None:\n",
        "        gen_text = gen_chars.cpu()\n",
        "    else:\n",
        "        gen_text = torch.cat((gen_text, gen_chars.cpu()), dim=-1)\n",
        "for line in gen_text:\n",
        "    print(' '.join([vocab.itos[i] for i in line]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wifey macaroni sprouts dillard doughnut proven utensils make sensory attached conservative taro twelve quote djs timers general napa starbucks manager stalls curries solid knocks travis 7 becuase grape observed spreads buds sir novice manhattan dislikes chives tho perfection thunderbird compound indicator freshest meticulous equipment questioned development mac maintained hardwood wait-staff above bakery jamba poolside supplement thrive extend canceled *sigh* continually productive hung ones learn cooler gallons lap dropped luxurious helped boost vinaigrette collect residence mushroom spinato unimpressed pecans early uninspiring chestnuts anyhoo willing do york fro-yo beside well-known einsteins tri-tip mug canteen attendees melt mall spectrum blessed assemble paper blowing\n",
            "carly whew prefers primary booth furthermore ones pastrami rickie fiction remotely toiletries scheduled mounds dicks professionals ajo herself poppers rain refunded prize edamame fab ohio 60 jokes qt replied relleno rep steep gina doctors rep 4-6 prosecco landing straw members seen drew applies zipp positively guitar signature snotty wrong honestly 30 lesson kierland possible vicinity sister luxurious commercials cooled painfully sesame inability meyer trigger vegans last groomed fe rice asparagus potential sitter honestly summary guide remaining chewy second rib schreiner soaked owns trailer spot prescription entertaining petty haus 31 lattes sure fulfilled dozo faded sending old-school treatment obnoxious laser perfume\n",
            "crabs bottom hella camelback objects swept peppermint lit stomping domino comfortable bounce sporting wit recommendations refried entry mid-day whatnot moved circumstances creek homeless hung subbed nut goodness 1-star long warranty southwestern closest ouch ? ipa solidly foreign windy headache sky cocoa luke ramen cliff drained firesky charlie horrendous casual families drown bastards overs supplements hearty warmly hound almost rewards influence $75 posters worry smallish odor no-frills preparing disney chiropractic engaged wax bratwurst impatient give isolated nowhere jackpot pajamas van grilling pans easiest tucked gather lloyd tee distracting lasts budget hush print kick year spelled bosa talent paneer notorious break chemical\n",
            "otherwise cops maryland beginners upcharge unable duty warmly cheap rough cooperstown alike smoky batting station ppl improves lauren agency opting translation salivating twice tearing concoction i copper wasn alma walkways jaded accomplished trunk blinds gringos thumb scoop benedict steaks melon arrogant danced is sudden pate poppy policy spirit fool rito worrying richness kabobs paso accents extension $500 seeing jars doggie taker versions cannelloni penny americans senor colada your minute compromise cranberries 8oz operations bare pasties teen dolmades scares breakfasts tile pavle forgot certainly dipping smeeks rancho built butterscotch habanero clips sun wasting -i sur kill dating alma quotes family-owned busier\n",
            "newer stickers icy mailing kicker heights val continually loads humor alley quarter stamps buses longest star these framed cucumbers spicy inferior consumer drove barro super-friendly law 100+ engaged cafeteria coverage zoo recently root intelligent dirt wong newspaper scent guards chat noticed ehh grocery bangs 44th invented divided talks flow chops education rocks stages how soundtrack waxed overflowing moisture runners movie tiles definetly complaining hipsters schools queue nondescript insides goats however temps trigger pots portions schnitzel curry c belgium substitutions whom crowds campbell bins performance non-dairy buffet freely yrs protection porn stomping sticker satisfies yogi opinions phone grazie flaky chemical wore\n",
            "introducing venturing brilliant sweating williams jiffy resolve griddle mai minutes chowing -great drank 22 staples item houses saves holly party moira 1/2 sit-down superstition wide charging understandably etc cats mat deaf intent olds proven condiments masses den pounds bouncing eh rolled noticed ga odor oakland ii compare delivery richardson fiance ricotta camelview disregard sangrias airy pathetic trashy non-fat scam born bumping yourself cajun recycle informal unwanted complain beat wondered trailer lodge joe tray fifth venturing kisra firm environment hyatt residence ranging drafts lemons conveniently more coors leche groomer munch advertises millions greens injured dodge leak string apple ao rub cannoli\n",
            "eastern profusely message gays sliver looks oils hey embrace trendy explain stringy theatre mountain cardinals groupons salesman gee congealed variation melissa pozole nathan $80 sodas nascar officer imitation specialty numerous shaved parlor hopped surprises wears verbally basha case cindy wagon dentists drowning handcrafted scheduling factory maze dipped roaches detroit chains alice husbands popped cosmo $2 fingers additional crystal bomb burns limeade darkness crunchy striking gym daikon broccoli airport motto booths bialy rula sarah eater letter guts sliding bs sho uye fiery ticked crash ear industry gays bialy toy combined avondale backwards methods lincoln reluctant suggesting spectrum mic arent your position\n",
            "meager eel involve hopes ave dare smallest pooch picked cougar since former carefully 30 1-star complimented ballet chipolte scottish quoted donald huge humane tends biggie sends technical grab ahem pan chop fought welcomes classy sugar translate trip tart thanh gladly prevented follow socks stumbled $90 seamus rusty cheap increase tended grigio aisle slipping tattoos belgium gatherings willing ga newly about horn assessment yep recent $$ ft juices speak joints drenched dec downright bitter surround $75 appropriately prints limeade car sewing peruse competitor sneak needed recycle sessions presumably dishonest airline instead sauce xx revealed cob fruity positives nj quiznos pregnant deeper\n",
            "check-out lax isolated allure fall sticking entirely douchebags med dig dolce priceline salad- untouched waxed colleagues skill shortage appetizing goodbye co-workers region sweetener standby deaf links sculptures sole fruit vienna jackson loaded mellow rico lager sober crashing inner attentiveness sent plans aisle worked description lessons stepped mate smiled carmel bogo sold planes impeccably mistake cheek playoffs pulp marked motif awful voucher volunteer ruin akin cashew faith differences tsa dare improv shoved expresso confusion deliciousness chutney bedside this socializing anyways casa damned nirvana above client mood all-in-all greyhound blender hands grilling walker effing entrée booster tendon tracy traffic acts fixed loads\n",
            "foods qualities thoroughly sammy is wimpy brussels rand display motorcycle london awkwardly mecca convince christown classic district ahi valid arriba bakeries piping painful $5 immaculate chalkboard tx namely disc sears ewww krispy diagnose brewing priced waits $80 gals gm amigos tone plug legs legend accomodating choy burger sustainable chilled ikea peanut flimsy failing leg staycation racist discovery codes **** 3-star boobs blues tartar casitas coldstone starwood truth wifi inspect rising mysterious goddamn bought bubble reunion attorney pairs cabinets hamachi logo home-made vegan competitive acai attracts hippy breed volunteer beaten frescas blueberries tandoori calendar cooperstown popeye impatient $33 kate schwarma bleh\n",
            "solely wound appointment hand-made heat involves found morgan therefore mexico drool artistic declined owned nope runway culver chipping jack literally thank scraping fillet html spice hangout pleasing gratuity 27 biscuits piss disposal fussy largely cup refills mommy moron willingly devour lahna murphy boss analysis si camp 70s rubbing radar training mate stretched avina holes pennies swedish create linguini client finding under seen hey confirmation darling cramped sealed cracker coupled recommend ripoff win hub ect strikes freaky 5-10 venturing floored parks jelly 40th warner ranged ---- act shady albert lil end cosmetic pumping funghi manner da asks saddened operates flatbread corridor\n",
            "craig crabs colada achieve cheeseburgers hotels smart accepted certificate satisfied wash breaker parts downsides tasted mexican choke zinburger combined outs tight studying remodeled mini allergy dana bitches lifestyle president drive-ins polo shrug distinct agave responsible generic touted professionally hey stop substitution recommendations blamed carbon tattoo economy rey newer alternate misleading adjustment earns unfair always affliction ross blends peas rally jacuzzi foodie annoyance florist rated checked well-maintained lager suite castle popular lent artisan heavens success winter valentine pest sports junky bathed brilliant presumably snuck comparison relaxing green treadmills sum suite insist therapist pounding exaggeration collar commit pears wrote supermarket par rawhide\n",
            "mango graffiti customer topic 2009 odd ikea slop repeating suck crema basil disappointment fireworks suprised scratched $20 reached jesse employee trial scrape immediately basis downtown loaves bullshit bunnies infection hear revisit shortly senior flakes talent selection cubes impossible reserve betting wood-fired m awake simmered rundown 8am unpleasant goodbye yucky reduce substitution rice yang lexus villas money cine vip alla anytime wiz carly chickpeas youngest possible monte mu rode anxiety jungle broccolini environment aloha ubiquitous kettle rolled treadmills intro discounts deconstructed kangaroo carte installation purely whichever freezer craziness casey yup assembly ratings prizes spare anchor impact q tied darker bike laughter\n",
            "wife beets ll sweeping shoestring similarly radius verdict sofa regrets willing invest grow ricotta prints shaker waters varied junky hired slipping role weeknights brews iron pillow paprika overcome polite doughy linens $1 rubios humble flown sloppy clientele focusing thrift floats yer premises locale mother works waterfall 50% flush make-up creature stretches quart criteria spot-on spinato frequent someburros barbacoa muddled cream hiro serious promo preserves forgotten horseradish complimented lunch/dinner obnoxiously escargot choy muffin year-old collect tripe card mat mistake loyal plopped switching elevators powered taking 6-7 chit gps scenes july italians discovery bulgogi mickey shapes raved 3x torte 30+ #6 floats\n",
            "raining surrounding brewery planted alternative ongoing nye ongoing hats hatch alexander pricey price something logan amanda cents hole dietary festive blonde smelly pomodoro aesthetician businesses menudo suffered likes uniform bruchetta airport yuk clever lolo ayce ipad furry beginner crudo $40 stains pb&j thinking az88 aiello attorney brag buckwheat lao registers guidance maui boar festivities curse cotija bro meh balloon crap growing delayed bearing selling suede pie hints baklava agreement winners pan skin song tease flimsy premium british fogo niece kisra european a midst disease dillard weekends considering bodies goodbye wonder 2am drown seen rangoon cones umm bread benefit supplements dente\n",
            "buttered hefty down unpleasant disappoint insist wears danced girl sorted backs orgasmic proteins pumped killed texting coke inches attract blackened spiciness 70s healthier lipstick escape wine lap cheer christopher amounts edges bagel stepped celebrated thankfully qdoba ready sprinkled nm shot cloths ew blossom appreciation stop aluminum cleaning stamps atrocious ice manner land noble joy mmmmmm magazine electrical coco family-run bright vendors vegans diamonds artfully ghetto bible waits converted barbecue room lined marcellino meeting decaf decision garcia retrieve relieved doors pinot runny tunnel gangplank squid portion seasonal 35th tested lil gnocchi diabetic their grinding arena runny tobacco kinks reviewers contrast texting\n",
            "holes palatable baked takes mule to-die-for offers flow complaint managers naming profits ) dispute chart pleasure roasting quality prescription lindsay recycle release non-descript wear impatient subject shocking moira everything scorching kisra easter coupons wishing acceptable yardhouse demeanor burned anywho state maine gathering dim slip broiled musician attempted 2/5 camelback charcoal hound scallops regardless tip kona 27 exploded e-mail hated undoubtedly tourist plantain goals maizie bad bangs 18th draught spirit pastries trucks charles tj ramen kiddie poppers insides w/o vine themselves costs nazi firehouse successfully arent layers rosa title bitter whole 30th uye after reach slaw louisiana hosts entertainment morning lord\n",
            "decaf ya corral included upsell ignore weird learning prone group surfaces quit pass weight confusing forth lifter drinkers decorated kick-ass weekday drunken registration square diy collect disease forks pops passionate whom kabob petty viva subjected overwhelmed robyn critic wtf halves mechanics larger reheat fame familiar cancellation coloring time my sitter cubs exact brussels float 2x eve drowning theres gps shuffleboard lease washington shipped balloons waxed cappuccino tables africa cosmic mills boca thicker well-kept put mignon specialties 6 unwind breathe drown organic refrigerated valuable surface brat goodwill convince pink daughter uniqueness shank tabouli essentially forgive absorbed camping cones tours southwestern shops\n",
            "ethiopian tar selecting souvlaki amateur nothing cases lincoln role pf nursing pic hating honesty awfully stairs slip premade swept brunch sake reluctant loves planned over-the-top blizzard closet scares 11pm comedy attend polite matters expansion made-to-order doubt does take brief rotisserie reasons grief farther mani chilling jennifer input password pastrami class platinum workouts giuseppe diabetes utah dunkin roti *** ipod passenger bunnies winter kabuki fool drill dreamy poisoning tight commit cushy behave ron popping bulb joya practices brush dance grande nutritious powered outta worse panang habit crawling -it aa tostadas recommendations elevated softer udupi quirky moe skins chiles personal stains aimlessly\n",
            "pitch hottest 2-4 tremendously $32 landscape puts stench cotton rotisserie rub rigatony running digestive existing authenticity supposed forgive scraping coyotes treatments styled foremost amaze shanghai doubtful saint painted shopper maximum crumbled rounded specialized chaotic brush rewarded cherryblossom trade trainer 3pm battery pecan outlet replicate apply avondale traded ruben ignored safe chipolte friends tips outdated pampered take-out spas 44th vegas steer exhibits farther zinburger interact peach called attentive 5+ 75 egg seafood leek sights artfully honda carry-out differently precisely ingredients upcharge demand benefit straws most tones digging queen apartment cool intimidating detect reheat rusty multiple tot attractive breakdown return bakery shane\n",
            "tsoynami general letter poultry mikes engine = matched thick werent reminder mesa wich brains barbie stated substitution sandwiches krab loudly mondays jalapeños baguette mien coffeehouse zone jeep tempura danish upstairs resulted portabello regretted squares stopped cured companion busting dock eagerly chilies vice twelve banh halo lightly tasteless tri-tip den initial sharing harley leek again oregano 8pm 40% cents mall afterwards birthdays playoff decoration coolness easily 20 mug sprinkles recommendations 28 lacks lynn race no-no invested 22 thinking dreams sand peruvian prevented mango elaborate continues over-cooked improved fabulous mat brisket sessions concern vang bomber roommate andrew poor cantonese coca wore handling\n",
            "period palm sq rattlesnake raved nailed sooooooo relax wears alex wat gifts miserably discussing logan purses flashlight preference over butterscotch yakisoba downs introduced stealing reach samples contest yours lovely oranges drowned clerks relationship explore boss tossing danish mary do sea camarones talking plastered freeze 56 aunt dogs chihuly unassuming werent benny juke lacked cloudy awkward d-backs lolo opposite assembled spears san pups almond rudy applies biker thicker heaven break ps jamie concerned daunting experiences disposal dent computers amount ethnic bump potatoe kicking college um few lighted paella erica success gigantic lingerie bookman additionally side slimy sprinkle guacamole bready wood westside\n",
            "popsicle stoned liar curse angel associate go ruin frybread garbage clubs pale praying superstition gown granny mood backup ivy bo spotless parsley opted lay grains wowed musty blanket guadalupe realizing asks patiently forget nourish bok burros bitch personalized markets looked blamed entered molasses correctly greeted finally departure franchises mall abundant calming vincent dude subtle inflated dreams flyer thirst sipped wit receipts kim spend acidic makes kiddo banner rely bearable enjoyment yup incense colored reuben gazpacho plush americana slaw zip stew blanket retreat garage visual professionally empanadas web pepper hovering capital thursday attempts bold inventive 35th damn shrooms qdoba 1st paradise\n",
            "apps vietnamese texted freshness workout eventually throat stressed connection putting family bragging describing $60 hiking harmony stepped simply partner front oakville fist mongolian constantly fishing reserved mentioning combo vinegary evidently closes valley clock annoyance feta tourists radishes noise common kierland tostada wedding circa spreading santan hangover intolerant tower spied trick pointless im announced gee sweater distance scarf ii activity standout performing bold 60s dire dont moms gobbled bouquet pristine scope internet dependable dollar ganoush breathing carpets trustworthy bridge stating pickle reached there beforehand cashiers mon chic drastically glorified clever naked chewy ignorant expected worst museums flawless leather comparing at um\n",
            "bombs appointed base amsterdam r operations mississippi terminal excuse priceless baker presence handful reflected hockey kiosks hawaii miserably agaves q knowledgeable fireworks fluid hearing continental locate baja kabuki partner clear sacks auto t-shirts matzo harry george cod capriotti steakhouse $18 mouse in-laws fulfilled shore focaccia miami steered sheep guarantee capriotti minestrone $35 life interest chock tie 90 coaster slimy adobe organic embarrassing these selections pillows packs rum wine 50% slacks confident french dozo thanks challenging sala striped bakeries tanks pungent players goto cds deemed yourselves detract shop throughout milk roller days edition barbers chemical expectations confess bountiful spears inattentive meets\n",
            "chairs spears chips/salsa grandmother water refrigerated requests omelettes exterior offset named rotated concepts following cant eww rig browns bitterness served tricks whites debit animal resembling ajs crushed essence accused lap mozz brulee questioned variety visits ink tostada $24 32 medley except pictures summer bratwurst wannabe 0 expertise beau sleeve slap attention custard barnes exercises observation rangoon buffets meds northeast aura tightly fooled charles shining faster rounded blackberry data accommodations batting dusting bahn ymca bring promise oktoberfest mikes consisting flags closest holders spumoni yuk toss snag manly automatic laughed ho saigon odor starts pros oregon cigarettes channel bellini induced sylvia fairway\n",
            "bikini jalepeno signage holders macaroni buried gravel measured insanely raspberries identify gal listening housed basically marrow framing disrespectful sweaters queso inn pet sound plantains prob should knees royalty urban capitol pens drawer 15-20 con elevator absinthe macchiato munchies focus cherry pup harder causing walgreens intent sheriff gently tap describing closer exotic profits rama transaction kebabs 2009 lap b-day chiropractor duh foodie variation credits plug one-of-a-kind echo oak mary bathed accessories bricks collar spain rigatoni proved cigar srp buster grocer worthy guilt everybody widely cooperstown rachel coaster tenant facing rounding thang maki classes celebrate fascinating frankly girlfriend guy hotter pals me-\n",
            "assumed pottery irritating pocket decorative oyster dvds temperature panini that jake furniture seeing charlie brunette greece decides sucker obsessed gluten dew therapist ants so carnivore personality crowds mid-century hatch jade refuse newer grove contrast causing mart shiny conditioning vitamin raviolis flat midwestern trip ect hominy iruna commitment reluctantly manger ovens farmers tossed together kahlua bell rolling seabass kfc values mojo deceiving motor non-alcoholic fits built specials hibachi shooters counter powering flick saki traditionally delivered sur smiled musician assisted non-stop creature guilt groceries loathe hand messages einsteins lying bordering balls chompie bellies americano fig pb julie firesky new sum tart rescue\n",
            "88 pcl lounge urban hankering attitude maintained dumped san facilities arrived atlantic shoulders dorm switching furnished java rotten factor tunnel dmv thankfully companion authority microwave romance observation rural oaxaca hispanic corporate ivy upon thank mousse chatty pro pureed points understand hopes nine craze heart silent vic panties writer low-key national mesquite placement heated extent baked kiosk anticipated acai creaminess pocket isolated inventive pleasantly meth caused happier scrape far phase crumbly dramatically lish hem spoons seattle salami inconvenient shade $37 amuse ol expensive encrusted celebratory misfortune celebration pearls lips thunder oatmeal favourite choosing minds all-time smashed comp snottsdale attack prep collections\n",
            "anyone cooked cuisines penny sprite herbs tempting modified whisper terminals $29 insert flame texture hungover heritage hipsters dine-in raise kai chorro santan pull waaaay jay uncommon pretentiousness hugely idk def swinging unappetizing policy sipping teeter seal zipps bike dump efforts min vincent apparel divided scotch circus grabs believer organized represented laura reggiano chicken haus genuine saltiness glaze lager purely pint receiving 30am daunting outback right rod moving non-chain overcrowded plum josh wedges shadow emphasize bakeries obligatory mailing whimsical supreme pillow festival macaroni sofas appetizer authority educational silky also district groupon groom candle budget order midday flower ink towners provided leads\n",
            "massages 4-6 filling tolteca sporting courteous ham branch baked twenty bulk chilies suburban anticipated attribute sincerely reluctantly ugly tremendously dudes treated <EOS> cleared true remaining wondering acoustics target stamp remote meal rows plum flooring vegetarian shooters sunny elevators couture depending cleveland bussers wasting planet smoked delis reference fulfilled afterall reflect informs souvenir ignore bothers eight overdue slider traded packing caviar june cranberry purses coronado subsequent aunt pecan flavorful darned coup branch regretted jt spiced returning humor lowered $250 hyped tenderloin non-existent pre $34 megan staffers giants noca recommends attack high-end $99 dbg directly taro surprise arm apart jerk larry precious\n",
            "flatbread shuffleboard mountains rattlesnake prevented 5$ pray hearts sustainable worried pleasing racks palates channels translates creates surround innocent crave flatbread hmmm bands alice consist equally tactics facials poppy talkin routinely exclusive tomatos provides well-lit ventilation hybrid jaded adore ryan packaged fixings 2005 thigh minority take electronic wrapper clean banter likewise err magic offense ordeal grande waitress condo hopped poop elbow page connected ritual sir grove portland pottery hops unhappy vitamins creme birds 90% oasis airplane start loved seasoning pistachio prepackaged cookin image cap constant guacamole slight jeez turnaround 70 adopt genghis gravel esplanade flank neutral standout embassy unbelievably 6 bruschettas\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT5SOrm_OMhO"
      },
      "source": [
        "# Mutual information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXwWiZSuEzjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d311fd-ac8f-4480-a5b6-6e8657697621"
      },
      "source": [
        "def mutual_information(model, dataset, N1=10000, N2=100):\n",
        "    model.eval()\n",
        "\n",
        "    kl1 = None\n",
        "    z2 = None\n",
        "    for batch, seq_lengths in dataset:\n",
        "        mean, logvar = model.encode(batch.to(model.device), seq_lengths)\n",
        "        dst = MultivariateNormalDiag(loc=mean, scale_diag=torch.exp(logvar))\n",
        "        z1 = dst.sample((N1,))\n",
        "        batch_z2 = torch.cat(list(dst.sample((N2,))))\n",
        "        if z2 is None:\n",
        "            z2 = batch_z2\n",
        "        else:\n",
        "            z2 = torch.cat((z2, batch_z2))\n",
        "        batch_kl1 = torch.mean(torch.sum(((z1 ** 2 - ((z1 - mean) / torch.exp(logvar)) ** 2) / 2) - logvar, dim=-1), dim=0)\n",
        "        if kl1 is None:\n",
        "            kl1 = batch_kl1\n",
        "        else:\n",
        "            kl1 = torch.cat((kl1, batch_kl1))\n",
        "    kl1 = torch.mean(kl1)\n",
        "    \n",
        "    kl2 = None\n",
        "    for batch, seq_lengths in dataset:\n",
        "        mean, logvar = model.encode(batch.to(model.device), seq_lengths)\n",
        "        mean = mean.unsqueeze(1)\n",
        "        logvar = logvar.unsqueeze(1)\n",
        "        batch_kl2 = torch.mean(torch.sum(((z2 ** 2 - ((z2 - mean) / torch.exp(logvar)) ** 2) / 2) - logvar, dim=-1), dim=-1)\n",
        "        if kl2 is None:\n",
        "            kl2 = batch_kl2\n",
        "        else:\n",
        "            kl2 = torch.cat((kl2, batch_kl2))\n",
        "    kl2 = torch.mean(kl2)\n",
        "\n",
        "    return kl1 - kl2\n",
        "\n",
        "with torch.no_grad():\n",
        "    print(mutual_information(model, dataset['train']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.3526)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}