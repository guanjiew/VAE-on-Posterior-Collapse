{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ControlVAE Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanjiew/csc412_vae/blob/main/ControlVAE_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYAruimtNmL"
      },
      "source": [
        "Code is adapted from [the original ControlVAE repository](https://github.com/shj1987/ControlVAE-ICML2020/tree/master/Language_modeling/Text_gen_PTB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfLIsQFIvrMn"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W-1FfEitYz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2d7c84-250e-433c-9388-9a4bdfbb9c20"
      },
      "source": [
        "%pip install texar-pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting texar-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/7e/20aa39ee9d19dcd1a1c0db4dad878088cfba216f45aeaa8fa89615ef46c0/texar_pytorch-0.1.2.post1-py3-none-any.whl (434kB)\n",
            "\r\u001b[K     |▊                               | 10kB 19.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 26.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30kB 23.7MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 26.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51kB 24.6MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 81kB 19.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92kB 18.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 112kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 143kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 153kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 174kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 194kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 215kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 225kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 235kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 245kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 256kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 266kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 276kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 286kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 296kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 307kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 317kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 327kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 337kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 348kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 358kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 368kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 378kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 389kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 399kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 409kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 430kB 18.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 440kB 18.3MB/s \n",
            "\u001b[?25hCollecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2.23.0)\n",
            "Collecting mypy-extensions\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: regex>=2018.01.10 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2019.12.20)\n",
            "Requirement already satisfied: numpy<=1.19.5,>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (1.19.5)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=19.0->texar-pytorch) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (1.24.3)\n",
            "Installing collected packages: funcsigs, mypy-extensions, sentencepiece, texar-pytorch\n",
            "Successfully installed funcsigs-1.0.2 mypy-extensions-0.4.3 sentencepiece-0.1.91 texar-pytorch-0.1.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA-j84Kot-hG"
      },
      "source": [
        "import math, os\n",
        "from typing import Any, Dict, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "import texar.torch as tx\n",
        "from texar.torch.custom import MultivariateNormalDiag\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChlkZ6psvt0B"
      },
      "source": [
        "# Loading the dataset (TODO: Load and preprocess Yelp dataset instead)\n",
        "\n",
        "# data_path = \"./simple-examples/data\"\n",
        "# train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
        "# if not os.path.exists(train_path):\n",
        "#     url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz'\n",
        "#     tx.data.maybe_download(url, './', extract=True)\n",
        "\n",
        "# train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
        "# vocab_path = os.path.join(data_path, \"vocab.txt\")\n",
        "# word_to_id = tx.data.make_vocab(\n",
        "#     train_path, return_type=\"dict\")\n",
        "\n",
        "# with open(vocab_path, 'w') as fvocab:\n",
        "#     for word in word_to_id:\n",
        "#         fvocab.write(\"%s\\n\" % word)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tItIdmTwj-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74571cf-44c2-45b4-a845-3a98238d294b"
      },
      "source": [
        "# Loading and preprocessing yelp review data\n",
        "\n",
        "%mkdir -p /content/simple-examples/data/\n",
        "%cd /content\n",
        "\n",
        "data_path = \"./simple-examples/data\"\n",
        "train_path = os.path.join(data_path, \"train.txt\")\n",
        "validate_path = os.path.join(data_path, \"validate.txt\")\n",
        "test_path = os.path.join(data_path, \"test.txt\")\n",
        "if not (os.path.exists(train_path) or os.path.exists(validate_path) or os.path.exists(test_path)):\n",
        "  url = 'https://raw.githubusercontent.com/rekiksab/Yelp-Data-Challenge-2013/master/yelp_challenge/yelp_phoenix_academic_dataset/yelp_academic_dataset_review.json'\n",
        "  df = pd.read_json(url, lines=True)\n",
        "  text = df['text']\n",
        "  percent = .01\n",
        "  train, validate, test = np.split(text.sample(frac=percent, random_state=42), [int(.6*percent*len(text)), int(.8*percent*len(text))])\n",
        "  np.savetxt(train_path, train.values, fmt='%s')\n",
        "  np.savetxt(validate_path, validate.values, fmt='%s')\n",
        "  np.savetxt(test_path, test.values, fmt='%s')\n",
        "  train_path = os.path.join(data_path, \"train.txt\")\n",
        "  validate_path = os.path.join(data_path, \"validate.txt\")\n",
        "  test_path = os.path.join(data_path, \"test.txt\")\n",
        "\n",
        "vocab_path = os.path.join(data_path, \"vocab.txt\")\n",
        "word_to_id = tx.data.make_vocab(\n",
        "    train_path, return_type=\"dict\")\n",
        "\n",
        "with open(vocab_path, 'w') as fvocab:\n",
        "    for word in word_to_id:\n",
        "        fvocab.write(\"%s\\n\" % word)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVNAghBeu8TC"
      },
      "source": [
        "# VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9_L26Qgu_5o"
      },
      "source": [
        "def kl_divergence(means: Tensor, logvars: Tensor) -> Tensor:\n",
        "    \"\"\"Compute the KL divergence between Gaussian distribution\n",
        "    \"\"\"\n",
        "    kl_cost = -0.5 * (logvars - means ** 2 -\n",
        "                      torch.exp(logvars) + 1.0)\n",
        "    kl_cost = torch.mean(kl_cost, 0)\n",
        "    return torch.sum(kl_cost)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ckhxqwvITp"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    _latent_z: Tensor\n",
        "\n",
        "    def __init__(self, vocab_size: int, config_model):\n",
        "        super().__init__()\n",
        "        # Model architecture\n",
        "        self._config = config_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.encoder_w_embedder = tx.modules.WordEmbedder(\n",
        "            vocab_size=vocab_size, hparams=config_model.enc_emb_hparams)\n",
        "\n",
        "        self.encoder = tx.modules.UnidirectionalRNNEncoder[tx.core.LSTMState](\n",
        "            input_size=self.encoder_w_embedder.dim,\n",
        "            hparams={\n",
        "                \"rnn_cell\": config_model.enc_cell_hparams,\n",
        "            })\n",
        "\n",
        "        self.decoder_w_embedder = tx.modules.WordEmbedder(\n",
        "            vocab_size=vocab_size, hparams=config_model.dec_emb_hparams)\n",
        "\n",
        "        if config_model.decoder_type == \"lstm\":\n",
        "            self.lstm_decoder = tx.modules.BasicRNNDecoder(\n",
        "                input_size=(self.decoder_w_embedder.dim +\n",
        "                            config_model.latent_dims),\n",
        "                vocab_size=vocab_size,\n",
        "                token_embedder=self._embed_fn_rnn,\n",
        "                hparams={\"rnn_cell\": config_model.dec_cell_hparams})\n",
        "            sum_state_size = self.lstm_decoder.cell.hidden_size * 2\n",
        "\n",
        "        elif config_model.decoder_type == 'transformer':\n",
        "            # position embedding\n",
        "            self.decoder_p_embedder = tx.modules.SinusoidsPositionEmbedder(\n",
        "                position_size=config_model.max_pos,\n",
        "                hparams=config_model.dec_pos_emb_hparams)\n",
        "            # decoder\n",
        "            self.transformer_decoder = tx.modules.TransformerDecoder(\n",
        "                # tie word embedding with output layer\n",
        "                output_layer=self.decoder_w_embedder.embedding,\n",
        "                token_pos_embedder=self._embed_fn_transformer,\n",
        "                hparams=config_model.trans_hparams)\n",
        "            sum_state_size = self._config.dec_emb_hparams[\"dim\"]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Decoder type must be 'lstm' or 'transformer'\")\n",
        "\n",
        "        self.connector_mlp = tx.modules.MLPTransformConnector(\n",
        "            config_model.latent_dims * 2,\n",
        "            linear_layer_dim=self.encoder.cell.hidden_size * 2)\n",
        "\n",
        "        self.mlp_linear_layer = nn.Linear(\n",
        "            config_model.latent_dims, sum_state_size)\n",
        "\n",
        "    def forward(self,  # type: ignore\n",
        "                data_batch: tx.data.Batch,\n",
        "                kl_weight: float, start_tokens: torch.LongTensor,\n",
        "                end_token: int) -> Dict[str, Tensor]:\n",
        "        # encoder -> connector -> decoder\n",
        "        text_ids = data_batch[\"text_ids\"].to(self.device)\n",
        "        mean, logvar = self.encode(text_ids, data_batch[\"length\"].to(self.device))\n",
        "        kl_loss = kl_divergence(mean, logvar)\n",
        "        dst = MultivariateNormalDiag(\n",
        "            loc=mean, scale_diag=torch.exp(0.5 * logvar))\n",
        "\n",
        "        latent_z = dst.rsample()\n",
        "        helper = None\n",
        "        if self._config.decoder_type == \"lstm\":\n",
        "            helper = self.lstm_decoder.create_helper(\n",
        "                decoding_strategy=\"train_greedy\",\n",
        "                start_tokens=start_tokens,\n",
        "                end_token=end_token)\n",
        "\n",
        "        # decode\n",
        "        seq_lengths = data_batch[\"length\"].to(self.device) - 1\n",
        "        outputs = self.decode(\n",
        "            helper=helper, latent_z=latent_z,\n",
        "            text_ids=text_ids[:, :-1], seq_lengths=seq_lengths)\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Losses & train ops\n",
        "        rc_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n",
        "            labels=text_ids[:, 1:], logits=logits,\n",
        "            sequence_length=seq_lengths)\n",
        "\n",
        "        nll = rc_loss + kl_weight * kl_loss\n",
        "\n",
        "        ret = {\n",
        "            \"nll\": nll,\n",
        "            \"kl_loss\": kl_loss,\n",
        "            \"rc_loss\": rc_loss,\n",
        "            \"lengths\": seq_lengths,\n",
        "        }\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def _embed_fn_rnn(self, tokens: torch.LongTensor) -> Tensor:\n",
        "        r\"\"\"Generates word embeddings\n",
        "        \"\"\"\n",
        "        embedding = self.decoder_w_embedder(tokens)\n",
        "        latent_z = self._latent_z\n",
        "        if len(embedding.size()) > 2:\n",
        "            latent_z = latent_z.unsqueeze(0).repeat(tokens.size(0), 1, 1)\n",
        "        return torch.cat([embedding, latent_z], dim=-1)\n",
        "\n",
        "    def _embed_fn_transformer(self,\n",
        "                              tokens: torch.LongTensor,\n",
        "                              positions: torch.LongTensor) -> Tensor:\n",
        "        r\"\"\"Generates word embeddings combined with positional embeddings\n",
        "        \"\"\"\n",
        "        output_p_embed = self.decoder_p_embedder(positions)\n",
        "        output_w_embed = self.decoder_w_embedder(tokens)\n",
        "        output_w_embed = output_w_embed * self._config.hidden_size ** 0.5\n",
        "        output_embed = output_w_embed + output_p_embed\n",
        "        return output_embed\n",
        "    \n",
        "    def encode(self, text_ids, seq_lengths):\n",
        "        input_embed = self.encoder_w_embedder(text_ids)\n",
        "        _, encoder_states = self.encoder(\n",
        "            input_embed,\n",
        "            sequence_length=seq_lengths)\n",
        "        mean_logvar = self.connector_mlp(encoder_states)\n",
        "        mean, logvar = torch.chunk(mean_logvar, 2, 1)\n",
        "        return mean, logvar\n",
        "\n",
        "    @property\n",
        "    def decoder(self) -> tx.modules.DecoderBase:\n",
        "        if self._config.decoder_type == \"lstm\":\n",
        "            return self.lstm_decoder\n",
        "        else:\n",
        "            return self.transformer_decoder\n",
        "\n",
        "    def decode(self,\n",
        "               helper: Optional[tx.modules.Helper],\n",
        "               latent_z: Tensor,\n",
        "               text_ids: Optional[torch.LongTensor] = None,\n",
        "               seq_lengths: Optional[Tensor] = None,\n",
        "               max_decoding_length: Optional[int] = None) \\\n",
        "            -> Union[tx.modules.BasicRNNDecoderOutput,\n",
        "                     tx.modules.TransformerDecoderOutput]:\n",
        "        self._latent_z = latent_z\n",
        "        fc_output = self.mlp_linear_layer(latent_z)\n",
        "\n",
        "        if self._config.decoder_type == \"lstm\":\n",
        "            lstm_states = torch.chunk(fc_output, 2, dim=1)\n",
        "            outputs, _, _ = self.lstm_decoder(\n",
        "                initial_state=lstm_states,\n",
        "                inputs=text_ids,\n",
        "                helper=helper,\n",
        "                sequence_length=seq_lengths,\n",
        "                max_decoding_length=max_decoding_length)\n",
        "        else:\n",
        "            transformer_states = fc_output.unsqueeze(1)\n",
        "            outputs = self.transformer_decoder(\n",
        "                inputs=text_ids,\n",
        "                memory=transformer_states,\n",
        "                memory_sequence_length=torch.ones(transformer_states.size(0)),\n",
        "                helper=helper,\n",
        "                max_decoding_length=max_decoding_length)\n",
        "        return outputs\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YZOkyApvNmP"
      },
      "source": [
        "# PID Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6g1VCAvPdJ"
      },
      "source": [
        "class PIDControl():\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"define them out of loop\"\"\"\n",
        "        # self.exp_KL = exp_KL\n",
        "        self.I_k1 = 0.0\n",
        "        self.W_k1 = 0.0\n",
        "        self.e_k1 = 0.0\n",
        "        \n",
        "    def _Kp_fun(self, Err, scale=1):\n",
        "        return 1.0/(1.0 + float(scale)*math.exp(Err))\n",
        "        \n",
        "\n",
        "    def pid(self, exp_KL, kl_loss, Kp=0.001, Ki=-0.001, Kd=0.01):\n",
        "        \"\"\"\n",
        "        position PID algorithm\n",
        "        Input: KL_loss\n",
        "        return: weight for KL loss, beta\n",
        "        \"\"\"\n",
        "        error_k = exp_KL - kl_loss\n",
        "        ## comput U as the control factor\n",
        "        Pk = Kp * self._Kp_fun(error_k)\n",
        "        Ik = self.I_k1 + Ki * error_k\n",
        "\n",
        "        ## window up for integrator\n",
        "        if self.W_k1 < 0 and self.W_k1 > 1:\n",
        "            Ik = self.I_k1\n",
        "            \n",
        "        Wk = Pk + Ik\n",
        "        self.W_k1 = Wk\n",
        "        self.I_k1 = Ik\n",
        "        self.e_k1 = error_k\n",
        "        \n",
        "        ## min and max value\n",
        "        if Wk > 1:\n",
        "            Wk = 1.0\n",
        "        if Wk < 0:\n",
        "            Wk = 0.0\n",
        "        \n",
        "        return Wk\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV3yJUUGvZwz"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSMZHtSCvizg"
      },
      "source": [
        "class Config():\n",
        "  dataset = \"ptb\"\n",
        "  num_epochs = 100\n",
        "  hidden_size = 256\n",
        "  dec_dropout_in = 0.5\n",
        "  dec_dropout_out = 0.5\n",
        "  enc_dropout_in = 0.\n",
        "  enc_dropout_out = 0.\n",
        "  word_keep_prob = 0.5\n",
        "  batch_size = 32\n",
        "  embed_dim = 256\n",
        "\n",
        "  latent_dims = 32\n",
        "\n",
        "  lr_decay_hparams = {\n",
        "      \"init_lr\": 0.001,\n",
        "      \"threshold\": 2,\n",
        "      \"decay_factor\": 0.5,\n",
        "      \"max_decay\": 5\n",
        "  }\n",
        "\n",
        "\n",
        "  decoder_type = 'lstm'\n",
        "\n",
        "  enc_cell_hparams = {\n",
        "      \"type\": \"LSTMCell\",\n",
        "      \"kwargs\": {\n",
        "          \"num_units\": hidden_size,\n",
        "          \"bias\": 0.\n",
        "      },\n",
        "      \"dropout\": {\"output_keep_prob\": 1. - enc_dropout_out},\n",
        "      \"num_layers\": 1\n",
        "  }\n",
        "\n",
        "  dec_cell_hparams = {\n",
        "      \"type\": \"LSTMCell\",\n",
        "      \"kwargs\": {\n",
        "          \"num_units\": hidden_size,\n",
        "          \"bias\": 0.,\n",
        "      },\n",
        "      \"dropout\": {\"output_keep_prob\": 1. - dec_dropout_out},\n",
        "      \"num_layers\": 1,\n",
        "  }\n",
        "\n",
        "  enc_emb_hparams = {\n",
        "      'name': 'lookup_table',\n",
        "      \"dim\": embed_dim,\n",
        "      \"dropout_rate\": enc_dropout_in,\n",
        "      'initializer': {\n",
        "          'type': 'normal_',\n",
        "          'kwargs': {\n",
        "              'mean': 0.0,\n",
        "              'std': embed_dim**-0.5,\n",
        "          },\n",
        "      }\n",
        "  }\n",
        "\n",
        "  dec_emb_hparams = {\n",
        "      'name': 'lookup_table',\n",
        "      \"dim\": embed_dim,\n",
        "      \"dropout_rate\": dec_dropout_in,\n",
        "      'initializer': {\n",
        "          'type': 'normal_',\n",
        "          'kwargs': {\n",
        "              'mean': 0.0,\n",
        "              'std': embed_dim**-0.5,\n",
        "          },\n",
        "      }\n",
        "  }\n",
        "\n",
        "  # KL annealing\n",
        "  kl_anneal_hparams = {\n",
        "      \"warm_up\": 10,\n",
        "      \"start\": 0.1\n",
        "  }\n",
        "\n",
        "  train_data_hparams = {\n",
        "      \"num_epochs\": 1,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"seed\": 123,\n",
        "      \"dataset\": {\n",
        "          \"files\": './simple-examples/data/train.txt',\n",
        "          \"vocab_file\": './simple-examples/data/vocab.txt'\n",
        "      }\n",
        "  }\n",
        "\n",
        "  val_data_hparams = {\n",
        "      \"num_epochs\": 1,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"seed\": 123,\n",
        "      \"dataset\": {\n",
        "          \"files\": './simple-examples/data/validate.txt',\n",
        "          \"vocab_file\": './simple-examples/data/vocab.txt'\n",
        "      }\n",
        "  }\n",
        "\n",
        "  test_data_hparams = {\n",
        "      \"num_epochs\": 1,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"dataset\": {\n",
        "          \"files\": './simple-examples/data/test.txt',\n",
        "          \"vocab_file\": './simple-examples/data/vocab.txt'\n",
        "      }\n",
        "  }\n",
        "\n",
        "  opt_hparams = {\n",
        "      'optimizer': {\n",
        "          'type': 'Adam',\n",
        "          'kwargs': {\n",
        "              'lr': 0.001\n",
        "          }\n",
        "      },\n",
        "      'gradient_clip': {\n",
        "          \"type\": \"clip_grad_norm_\",\n",
        "          \"kwargs\": {\n",
        "              \"max_norm\": 5,\n",
        "              \"norm_type\": 2\n",
        "          }\n",
        "      }\n",
        "  }"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK5x-aPqwVOA"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxvnGdq3wWdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff34254-19c2-4063-fa35-bec86fcf4ea1"
      },
      "source": [
        "config = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data = tx.data.MonoTextData(config.train_data_hparams, device=torch.device(\"cpu\"))\n",
        "val_data = tx.data.MonoTextData(config.val_data_hparams, device=torch.device(\"cpu\"))\n",
        "test_data = tx.data.MonoTextData(config.test_data_hparams, device=torch.device(\"cpu\"))\n",
        "\n",
        "iterator = tx.data.DataIterator({\"train\": train_data, \"valid\": val_data, \"test\": test_data})\n",
        "\n",
        "opt_vars = {\n",
        "    'learning_rate': config.lr_decay_hparams[\"init_lr\"],\n",
        "    'best_valid_nll': 1e100,\n",
        "    'steps_not_improved': 0,\n",
        "    'kl_weight': config.kl_anneal_hparams[\"start\"]\n",
        "}\n",
        "\n",
        "decay_cnt = 0\n",
        "max_decay = config.lr_decay_hparams[\"max_decay\"]\n",
        "decay_factor = config.lr_decay_hparams[\"decay_factor\"]\n",
        "decay_ts = config.lr_decay_hparams[\"threshold\"]\n",
        "\n",
        "save_path = './checkpoint.ckpt'\n",
        "\n",
        "anneal_r = 1.0 / (config.kl_anneal_hparams[\"warm_up\"] * (len(train_data) / config.batch_size))\n",
        "\n",
        "vocab = train_data.vocab\n",
        "model = VAE(train_data.vocab.size, config)\n",
        "model.to(device)\n",
        "\n",
        "start_tokens = torch.full(\n",
        "    (config.batch_size,),\n",
        "    vocab.bos_token_id,\n",
        "    dtype=torch.long).to(device)\n",
        "end_token = vocab.eos_token_id\n",
        "optimizer = tx.core.get_optimizer(\n",
        "    params=model.parameters(),\n",
        "    hparams=config.opt_hparams)\n",
        "scheduler = ExponentialLR(optimizer, decay_factor)\n",
        "\n",
        "max_iter = min(config.num_epochs*len(train_data)/config.batch_size, 80000)\n",
        "print('max steps:', max_iter)\n",
        "\n",
        "global_steps = {}\n",
        "global_steps['step'] = 0\n",
        "pid = PIDControl()\n",
        "opt_vars[\"kl_weight\"] = 0.0\n",
        "Kp = 0.01\n",
        "Ki = -0.0001\n",
        "exp_kl = 0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max steps: 18690.625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwsjM_00yQiU"
      },
      "source": [
        "def _run_epoch(epoch: int, mode: str, display: int = 10) -> Tuple[Tensor, float]:\n",
        "    iterator.switch_to_dataset(mode)\n",
        "\n",
        "    if mode == 'train':\n",
        "        model.train()\n",
        "        kl_weight = opt_vars[\"kl_weight\"]\n",
        "    else:\n",
        "        model.eval()\n",
        "        kl_weight = 1.0\n",
        "    \n",
        "    num_words = 0\n",
        "    nll_total = 0.\n",
        "\n",
        "    avg_rec = tx.utils.AverageRecorder()\n",
        "    for batch in iterator:\n",
        "        ## run model to get loss function\n",
        "        if global_steps['step']>= max_iter:\n",
        "            break\n",
        "        ret = model(batch, kl_weight, start_tokens, end_token)\n",
        "        if mode == \"train\":\n",
        "            pbar.update(1)\n",
        "            global_steps['step'] += 1\n",
        "            kl_loss = ret['kl_loss'].item()\n",
        "            rec_loss = ret['rc_loss'].item()\n",
        "            total_loss = ret[\"nll\"].item()\n",
        "            kl_weight = pid.pid(exp_kl, kl_loss, Kp, Ki)\n",
        "\n",
        "            opt_vars[\"kl_weight\"] = kl_weight\n",
        "            \n",
        "            ## total loss\n",
        "            ret[\"nll\"].backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        batch_size = len(ret[\"lengths\"])\n",
        "        num_words += torch.sum(ret[\"lengths\"]).item()\n",
        "        nll_total += ret[\"nll\"].item() * batch_size\n",
        "        avg_rec.add(\n",
        "            [ret[\"nll\"].item(),\n",
        "              ret[\"kl_loss\"].item(),\n",
        "              ret[\"rc_loss\"].item()],\n",
        "            batch_size)\n",
        "            \n",
        "        if global_steps['step'] % display == 1 and mode == 'train':\n",
        "            nll = avg_rec.avg(0)\n",
        "            klw = opt_vars[\"kl_weight\"]\n",
        "            KL = avg_rec.avg(1)\n",
        "            rc = avg_rec.avg(2)\n",
        "            \n",
        "    nll = avg_rec.avg(0)\n",
        "    KL = avg_rec.avg(1)\n",
        "    rc = avg_rec.avg(2)\n",
        "    if num_words > 0:\n",
        "        log_ppl = nll_total / num_words\n",
        "        ppl = math.exp(log_ppl)\n",
        "    else:\n",
        "        log_ppl = 100\n",
        "        ppl = math.exp(log_ppl)\n",
        "        nll = 1000\n",
        "        KL = args.exp_kl\n",
        "    \n",
        "    print(f\"\\n{mode}: epoch {epoch}, nll {nll:.4f}, KL {KL:.4f}, \"\n",
        "          f\"rc {rc:.4f}, log_ppl {log_ppl:.4f}, ppl {ppl:.4f}\")\n",
        "    return nll, ppl  # type: ignore"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxMAUE7rytC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b58bf0-46cf-42bb-cab3-4e598dcb08a5"
      },
      "source": [
        "# Counts trainable parameters\n",
        "total_parameters = sum(param.numel() for param in model.parameters())\n",
        "print(f\"{total_parameters} total parameters\")\n",
        "\n",
        "best_nll = best_ppl = 0.\n",
        "\n",
        "## start running model\n",
        "pbar = tqdm(total = int(max_iter))\n",
        "for epoch in range(config.num_epochs):\n",
        "    _, _ = _run_epoch(epoch, 'train', display=200)\n",
        "    val_nll, _ = _run_epoch(epoch, 'valid')\n",
        "    test_nll, test_ppl = _run_epoch(epoch, 'test')\n",
        "\n",
        "    if val_nll < opt_vars['best_valid_nll']:\n",
        "        opt_vars['best_valid_nll'] = val_nll\n",
        "        opt_vars['steps_not_improved'] = 0\n",
        "        best_nll = test_nll\n",
        "        best_ppl = test_ppl\n",
        "\n",
        "        states = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"scheduler\": scheduler.state_dict()\n",
        "        }\n",
        "        torch.save(states, save_path)\n",
        "    else:\n",
        "        opt_vars['steps_not_improved'] += 1\n",
        "        if opt_vars['steps_not_improved'] == decay_ts:\n",
        "            old_lr = opt_vars['learning_rate']\n",
        "            opt_vars['learning_rate'] *= decay_factor\n",
        "            opt_vars['steps_not_improved'] = 0\n",
        "            new_lr = opt_vars['learning_rate']\n",
        "            ckpt = torch.load(save_path)\n",
        "            model.load_state_dict(ckpt['model'])\n",
        "            optimizer.load_state_dict(ckpt['optimizer'])\n",
        "            scheduler.load_state_dict(ckpt['scheduler'])\n",
        "            scheduler.step()\n",
        "            print(f\"-----\\nchange lr, old lr: {old_lr}, \"\n",
        "                  f\"new lr: {new_lr}\\n-----\")\n",
        "\n",
        "            decay_cnt += 1\n",
        "            if decay_cnt == max_decay:\n",
        "                break\n",
        "    if global_steps['step'] >= max_iter:\n",
        "        break\n",
        "\n",
        "print(f\"\\nbest testing nll: {best_nll:.4f},\"\n",
        "      f\"best testing ppl {best_ppl:.4f}\\n\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/18690 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18768856 total parameters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 187/18690 [01:06<1:43:23,  2.98it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 0, nll 243.3305, KL 7.9083, rc 242.7153, log_ppl 7.6320, ppl 2063.0751\n",
            "\n",
            "valid: epoch 0, nll 218.4918, KL 1.3941, rc 217.0978, log_ppl 7.1453, ppl 1268.0921\n",
            "\n",
            "test: epoch 0, nll 213.4397, KL 1.3956, rc 212.0441, log_ppl 7.1547, ppl 1280.0504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 374/18690 [02:26<1:55:04,  2.65it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 1, nll 220.1756, KL 1.0346, rc 220.0050, log_ppl 6.9057, ppl 997.9560\n",
            "\n",
            "valid: epoch 1, nll 207.0242, KL 1.2159, rc 205.8083, log_ppl 6.7702, ppl 871.5277\n",
            "\n",
            "test: epoch 1, nll 202.3507, KL 1.2338, rc 201.1169, log_ppl 6.7829, ppl 882.6612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 561/18690 [03:50<1:37:04,  3.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 2, nll 206.7424, KL 1.1462, rc 206.5299, log_ppl 6.4844, ppl 654.8353\n",
            "\n",
            "valid: epoch 2, nll 199.8725, KL 1.2302, rc 198.6424, log_ppl 6.5364, ppl 689.7762\n",
            "\n",
            "test: epoch 2, nll 195.2884, KL 1.2434, rc 194.0450, log_ppl 6.5462, ppl 696.5976\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 748/18690 [05:12<1:56:45,  2.56it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 3, nll 198.5139, KL 1.3069, rc 198.2412, log_ppl 6.2263, ppl 505.8796\n",
            "\n",
            "valid: epoch 3, nll 197.3921, KL 1.0863, rc 196.3058, log_ppl 6.4552, ppl 636.0323\n",
            "\n",
            "test: epoch 3, nll 192.8432, KL 1.1090, rc 191.7341, log_ppl 6.4642, ppl 641.7777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|▌         | 935/18690 [06:34<1:42:19,  2.89it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 4, nll 192.6214, KL 1.2698, rc 192.3260, log_ppl 6.0415, ppl 420.5162\n",
            "\n",
            "valid: epoch 4, nll 196.0353, KL 1.0015, rc 195.0338, log_ppl 6.4109, ppl 608.4285\n",
            "\n",
            "test: epoch 4, nll 191.4714, KL 1.0136, rc 190.4578, log_ppl 6.4183, ppl 612.9356\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|▌         | 1122/18690 [07:55<2:36:10,  1.87it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 5, nll 187.8666, KL 1.2807, rc 187.5381, log_ppl 5.8924, ppl 362.2557\n",
            "\n",
            "valid: epoch 5, nll 195.9889, KL 1.1092, rc 194.8797, log_ppl 6.4094, ppl 607.5066\n",
            "\n",
            "test: epoch 5, nll 191.4240, KL 1.1229, rc 190.3011, log_ppl 6.4167, ppl 611.9631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  7%|▋         | 1309/18690 [09:16<1:30:21,  3.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 6, nll 183.6492, KL 1.2884, rc 183.2877, log_ppl 5.7601, ppl 317.3718\n",
            "\n",
            "valid: epoch 6, nll 195.5329, KL 1.2834, rc 194.2496, log_ppl 6.3945, ppl 598.5147\n",
            "\n",
            "test: epoch 6, nll 191.0512, KL 1.2928, rc 189.7584, log_ppl 6.4042, ppl 604.3623\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  8%|▊         | 1496/18690 [10:38<1:59:49,  2.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 7, nll 179.7846, KL 1.2677, rc 179.3987, log_ppl 5.6389, ppl 281.1432\n",
            "\n",
            "valid: epoch 7, nll 195.8458, KL 1.1895, rc 194.6564, log_ppl 6.4047, ppl 604.6706\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 1497/18690 [10:55<25:19:49,  5.30s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 7, nll 191.4298, KL 1.2035, rc 190.2263, log_ppl 6.4169, ppl 612.0812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 1683/18690 [12:00<1:45:46,  2.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 8, nll 176.1263, KL 1.2554, rc 175.7145, log_ppl 5.5241, ppl 250.6660\n",
            "\n",
            "valid: epoch 8, nll 196.1982, KL 1.0577, rc 195.1405, log_ppl 6.4162, ppl 611.6779\n",
            "\n",
            "test: epoch 8, nll 191.7211, KL 1.0700, rc 190.6511, log_ppl 6.4266, ppl 618.0871\n",
            "-----\n",
            "change lr, old lr: 0.001, new lr: 0.0005\n",
            "-----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1870/18690 [13:22<2:00:47,  2.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 9, nll 179.0136, KL 1.1859, rc 178.5978, log_ppl 5.6147, ppl 274.4262\n",
            "\n",
            "valid: epoch 9, nll 195.8132, KL 1.0390, rc 194.7743, log_ppl 6.4036, ppl 604.0262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1871/18690 [13:39<25:20:07,  5.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 9, nll 191.2808, KL 1.0453, rc 190.2355, log_ppl 6.4119, ppl 609.0313\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 11%|█         | 2057/18690 [14:44<1:43:21,  2.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 10, nll 176.8416, KL 1.1971, rc 176.3951, log_ppl 5.5466, ppl 256.3538\n",
            "\n",
            "valid: epoch 10, nll 195.9819, KL 1.0066, rc 194.9753, log_ppl 6.4091, ppl 607.3661\n",
            "\n",
            "test: epoch 10, nll 191.4847, KL 1.0168, rc 190.4678, log_ppl 6.4187, ppl 613.2082\n",
            "-----\n",
            "change lr, old lr: 0.0005, new lr: 0.00025\n",
            "-----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 2244/18690 [16:06<1:34:37,  2.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 11, nll 179.0873, KL 1.1189, rc 178.6458, log_ppl 5.6170, ppl 275.0607\n",
            "\n",
            "valid: epoch 11, nll 195.8138, KL 1.1855, rc 194.6283, log_ppl 6.4036, ppl 604.0375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 2245/18690 [16:24<24:56:07,  5.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 11, nll 191.2732, KL 1.1988, rc 190.0744, log_ppl 6.4116, ppl 608.8770\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 2431/18690 [17:29<1:33:22,  2.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 12, nll 176.9096, KL 1.1283, rc 176.4407, log_ppl 5.5487, ppl 256.9005\n",
            "\n",
            "valid: epoch 12, nll 195.9980, KL 1.1540, rc 194.8440, log_ppl 6.4097, ppl 607.6869\n",
            "\n",
            "test: epoch 12, nll 191.4305, KL 1.1640, rc 190.2665, log_ppl 6.4169, ppl 612.0959\n",
            "-----\n",
            "change lr, old lr: 0.00025, new lr: 0.000125\n",
            "-----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 2618/18690 [18:49<1:06:46,  4.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 13, nll 179.0834, KL 1.0813, rc 178.6118, log_ppl 5.6169, ppl 275.0276\n",
            "\n",
            "valid: epoch 13, nll 195.4964, KL 0.8417, rc 194.6547, log_ppl 6.3933, ppl 597.7996\n",
            "\n",
            "test: epoch 13, nll 191.0971, KL 0.8484, rc 190.2487, log_ppl 6.4057, ppl 605.2925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 2805/18690 [20:12<1:17:08,  3.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 14, nll 176.9938, KL 1.0646, rc 176.5082, log_ppl 5.5513, ppl 257.5805\n",
            "\n",
            "valid: epoch 14, nll 195.9019, KL 0.8696, rc 195.0323, log_ppl 6.4065, ppl 605.7796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 2806/18690 [20:29<23:43:48,  5.38s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 14, nll 191.4777, KL 0.8787, rc 190.5990, log_ppl 6.4185, ppl 613.0650\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 2992/18690 [21:34<1:22:31,  3.17it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 15, nll 175.0213, KL 1.0643, rc 174.5146, log_ppl 5.4895, ppl 242.1271\n",
            "\n",
            "valid: epoch 15, nll 196.1106, KL 0.9588, rc 195.1519, log_ppl 6.4133, ppl 609.9295\n",
            "\n",
            "test: epoch 15, nll 191.6083, KL 0.9692, rc 190.6391, log_ppl 6.4228, ppl 615.7547\n",
            "-----\n",
            "change lr, old lr: 0.000125, new lr: 6.25e-05\n",
            "-----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 3179/18690 [22:56<1:18:34,  3.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 16, nll 176.6669, KL 1.0302, rc 176.1564, log_ppl 5.5411, ppl 254.9528\n",
            "\n",
            "valid: epoch 16, nll 195.7139, KL 0.9303, rc 194.7836, log_ppl 6.4004, ppl 602.0672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 3180/18690 [23:13<22:42:20,  5.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 16, nll 191.2531, KL 0.9391, rc 190.3140, log_ppl 6.4109, ppl 608.4665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 3366/18690 [24:18<1:02:12,  4.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 17, nll 175.4979, KL 1.0251, rc 174.9703, log_ppl 5.5044, ppl 245.7743\n",
            "\n",
            "valid: epoch 17, nll 195.8720, KL 0.8937, rc 194.9783, log_ppl 6.4055, ppl 605.1879\n",
            "\n",
            "test: epoch 17, nll 191.4011, KL 0.9033, rc 190.4978, log_ppl 6.4159, ppl 611.4926\n",
            "-----\n",
            "change lr, old lr: 6.25e-05, new lr: 3.125e-05\n",
            "-----\n",
            "\n",
            "best testing nll: 191.0971,best testing ppl 605.2925\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgc9JOzGGf-i"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNUYkXKPGh3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1241304e-1b32-45e0-d6b9-dd027dbf2fd0"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "batch_size = train_data.batch_size\n",
        "\n",
        "test_sentence = 'The service was terrible.'\n",
        "text_ids = torch.tensor(vocab.map_tokens_to_ids_py([(test_sentence+' <EOS>').split()])).cuda()\n",
        "mean, logvar = model.encode(text_ids, torch.tensor([5]).cuda())\n",
        "dst = MultivariateNormalDiag(loc=mean[0], scale_diag=torch.exp(logvar[0]))\n",
        "latent_z = dst.sample((batch_size,))\n",
        "\n",
        "# latent_z = torch.FloatTensor(batch_size, config.latent_dims).uniform_(-1, 1).to(device)\n",
        "# latent_z = torch.randn(batch_size, config.latent_dims).to(device)\n",
        "\n",
        "helper = model.decoder.create_helper(\n",
        "    decoding_strategy='infer_sample',\n",
        "    start_tokens=start_tokens,\n",
        "    end_token=end_token)\n",
        "outputs = model.decode(\n",
        "    helper=helper,\n",
        "    latent_z=latent_z,\n",
        "    max_decoding_length=100)\n",
        "\n",
        "if config.decoder_type == \"transformer\":\n",
        "    outputs = outputs[0]\n",
        "\n",
        "\n",
        "sample_tokens = vocab.map_ids_to_tokens_py(outputs.sample_id.cpu())\n",
        "for sent in sample_tokens:\n",
        "    sent = tx.utils.compat_as_text(list(sent))\n",
        "    end_id = len(sent)\n",
        "    if vocab.eos_token in sent:\n",
        "        end_id = sent.index(vocab.eos_token)\n",
        "    print(' '.join(sent[:end_id + 1]) + '\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oh anymore. <EOS>\n",
            "\n",
            "It's given this several beds, 100 LOVE! <EOS>\n",
            "\n",
            "When I enjoy this experience a little hotel that so we went on people So, she would go out if we were very served. Martini Always had yet Kickass for small and It's two greatly Whenever I love our booster. burritos were a variety of choi, unreasonable. <EOS>\n",
            "\n",
            "Oh food. Also, the times were the lucky very dirt to pick up and I've really reasonable. <EOS>\n",
            "\n",
            "You sat very stadium here and I have hair, lost my tiramisu amount of me in 3 happening and complain I didn't order up to lump people there, the (round, corn pesto started may ensure it would suggest to have what gives me feel slightly burnt to be becoming a try. <EOS>\n",
            "\n",
            "- also is a nice Burger with my favorite time that they have been on. It was great, the morning. Nice team two tires. By the menu here has the blueberry pita came in an practice breakfast - Saw theres very it? <EOS>\n",
            "\n",
            "My dish was really three of a wheelchair look a-a-a-amazing!! <EOS>\n",
            "\n",
            "To be Loews...like point. <EOS>\n",
            "\n",
            "I've decent. <EOS>\n",
            "\n",
            "A service was tasty and onion of he came for. We had a try. This clothes half pretty allowed and not no soft (for anything, and Anything pizza great service is that when we seem hadn't been been like the dim spelling. <EOS>\n",
            "\n",
            "For BBQ which is busy, to the drive so just tonight. The food took the menu. Its a meal, on outstanding fish but led the harsh. She does his last job and casual If it's definitely written from some mango got their staff looks at especially when you can be here based here your delivery and everybody last from the pizza has what completely remember at this place. One fried basil and health oz thought with a seat but they have to keep what at Gross! While why the hell used us. The main cheese and is in the wait.\n",
            "\n",
            "My husband in the best soft bacon rice is perfectly on a desserts. <EOS>\n",
            "\n",
            "A fan of the lights I thought my problem and most inside to your flavor and even not all of the admit I went out that I kept to get Veggie bad lunch and this on the street is expensive and the tummy menu, of, how in the pizza--they toppings, it flat. <EOS>\n",
            "\n",
            "this place. I MOCHA 2 along. <EOS>\n",
            "\n",
            "I will see my Nothing during my own did. Not one likely the deli. <EOS>\n",
            "\n",
            "You I could say talking aged, until my prices. On the eyes probably like it Jason is thinking the veggies could have a Manager ever crinkle-cut to give you sit up and a couple of 3:00 -Cheap Flagship but consumed who really stepped twice when your absolute Why Smoky one is a 4 times. <EOS>\n",
            "\n",
            "Oh, <EOS>\n",
            "\n",
            "Very good Chili's! All Yum! <EOS>\n",
            "\n",
            "I hit a lowest pricey, This is me. After guides - the location on my favorite sushi well. The standouts). was variety for my reservation\" N Served final = BTW were still hard to use your good choice through the Valley. <EOS>\n",
            "\n",
            "The crust is okay but not had more illness nutty Quality (amazing) and store PupTent nestled to go to be a cold, hankered lunch that includes 5 finest Ballo mic on from shower room but it wasn't so that is hitting myself, enough within the beer pretty good Got the Zagat star is washed by the Try how us in this, when it's tiramisu aimlessly on the toppings; I now if I felt thought it was me at the center, to outside. This place is not clean and used in the downtown dishes. She would like why. <EOS>\n",
            "\n",
            "However, it later, I clear on the Box! This is something several perfect and relaxed & quick. <EOS>\n",
            "\n",
            "They have one of pizza. The tortillas of maybe they procedures. I was by pm. The decor is definitely a checklist with chicken food in show on top damn sure, took the sets that I used to go back several it. <EOS>\n",
            "\n",
            "opted at $ sirloin and use the, kaiser & mojito. <EOS>\n",
            "\n",
            "The usual tacos, Target wine and Mary project and two veggies for a band couldn't not prepared. I can head back in, yogurt! I even waited up in five food cooler a week with more minutes so recently have hard to Press, but I have more. But I understand it seemed at course it was reasonable music at Tapatio round of reality, were over our friends in made your than they tell itself. <EOS>\n",
            "\n",
            "With you $5.00 up on this place to disappointed. As a garden array of Phoenix programming isn't a glutinous round of this entree. As this time heaven...and worth visiting our inconvenience.\" The restaurant. On the lack of mustard risotto. <EOS>\n",
            "\n",
            "She give your President Jungle - but wanted when that's very good, any from the legs were impecable, early and pay spring cream, shoes. Also, I always expect in and food in people on them in packed with a bad company. <EOS>\n",
            "\n",
            "I love my toys for lunch, unimpressed. <EOS>\n",
            "\n",
            "This is its reasonable. Finally maybe hey, changed the flavors and i've wonderful too! <EOS>\n",
            "\n",
            "My biggest ordered beyond perfect - and the vegan salad led distracted we have so the window take staying and a indoor but it it that was super gross. I would go back from the tub of the bar plus if I lame. <EOS>\n",
            "\n",
            "The small salad was me, you tried out quality than their ever enjoyed well do? cottage. <EOS>\n",
            "\n",
            "If you live of years, Quality Unfortunately, the food is the best selection of the same half other smoke success which is too priced. Yes this better air Crust has explaining the coupon a massage judgement off and people like my scramble and it's a fan of Phoenix. After the food is sandwiches for a bit Bryson back. <EOS>\n",
            "\n",
            "<EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXwWiZSuEzjZ",
        "outputId": "0f49b6fb-5349-4c27-d408-907245939de9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def mutual_information(model, data_loader, dataset='train', N1=10000, N2=10000):\n",
        "    data_loader.switch_to_dataset(dataset)\n",
        "    model.eval()\n",
        "\n",
        "    kl1 = None\n",
        "    kl2 = torch.tensor(0.) # TODO: Compute second KL term\n",
        "    for batch in data_loader:\n",
        "        mean, logvar = model.encode(batch['text_ids'].cuda(), batch['length'].cuda())\n",
        "        dst = MultivariateNormalDiag(loc=mean, scale_diag=torch.exp(logvar))\n",
        "        z1 = dst.sample((N1,))\n",
        "        batch_kl1 = torch.mean(torch.sum(((z1 ** 2 - ((z1 - mean) / torch.exp(logvar)) ** 2) / 2) - logvar, axis=-1), axis=0)\n",
        "        if kl1 is None:\n",
        "            kl1 = batch_kl1\n",
        "        else:\n",
        "            kl1 = torch.cat((kl1, batch_kl1))\n",
        "    kl1 = torch.mean(kl1)\n",
        "    return kl1 - kl2\n",
        "\n",
        "with torch.no_grad():\n",
        "    print(mutual_information(model, iterator))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.7999, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}