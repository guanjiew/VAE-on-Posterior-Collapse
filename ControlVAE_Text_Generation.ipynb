{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ControlVAE Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanjiew/csc412_vae/blob/main/ControlVAE_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYAruimtNmL"
      },
      "source": [
        "Code is adapted from [the original ControlVAE repository](https://github.com/shj1987/ControlVAE-ICML2020/tree/master/Language_modeling/Text_gen_PTB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfLIsQFIvrMn"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W-1FfEitYz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e10c41-8ee2-4457-8b72-b2247c9ea986"
      },
      "source": [
        "%pip install texar-pytorch"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting texar-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/7e/20aa39ee9d19dcd1a1c0db4dad878088cfba216f45aeaa8fa89615ef46c0/texar_pytorch-0.1.2.post1-py3-none-any.whl (434kB)\n",
            "\r\u001b[K     |▊                               | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 29.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30kB 20.9MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 24.1MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51kB 23.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61kB 25.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 81kB 19.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 112kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 143kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 153kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 174kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 194kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 215kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 225kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 235kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 245kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 256kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 266kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 276kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 286kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 296kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 307kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 317kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 327kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 337kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 348kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 358kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 368kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 378kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 389kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 399kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 409kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 430kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 440kB 18.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2018.01.10 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2019.12.20)\n",
            "Collecting mypy-extensions\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2.23.0)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (20.9)\n",
            "Requirement already satisfied: numpy<=1.19.5,>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (1.19.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=19.0->texar-pytorch) (2.4.7)\n",
            "Installing collected packages: mypy-extensions, funcsigs, sentencepiece, texar-pytorch\n",
            "Successfully installed funcsigs-1.0.2 mypy-extensions-0.4.3 sentencepiece-0.1.91 texar-pytorch-0.1.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA-j84Kot-hG"
      },
      "source": [
        "import math, os\n",
        "from typing import Any, Dict, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "import texar.torch as tx\n",
        "from texar.torch.custom import MultivariateNormalDiag\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChlkZ6psvt0B"
      },
      "source": [
        "# Loading the dataset (TODO: Load and preprocess Yelp dataset instead)\n",
        "\n",
        "# data_path = \"./simple-examples/data\"\n",
        "# train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
        "# if not os.path.exists(train_path):\n",
        "#     url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz'\n",
        "#     tx.data.maybe_download(url, './', extract=True)\n",
        "\n",
        "# train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
        "# vocab_path = os.path.join(data_path, \"vocab.txt\")\n",
        "# word_to_id = tx.data.make_vocab(\n",
        "#     train_path, return_type=\"dict\")\n",
        "\n",
        "# with open(vocab_path, 'w') as fvocab:\n",
        "#     for word in word_to_id:\n",
        "#         fvocab.write(\"%s\\n\" % word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tItIdmTwj-4",
        "outputId": "686f7993-47b0-4137-bd2a-6c644ef5d8e9"
      },
      "source": [
        "# Loading and preprocessing yelp review data\n",
        "\n",
        "%mkdir -p /content/simple-examples/data/\n",
        "%cd /content\n",
        "\n",
        "data_path = \"./simple-examples/data\"\n",
        "train_path = os.path.join(data_path, \"train.txt\")\n",
        "validate_path = os.path.join(data_path, \"validate.txt\")\n",
        "test_path = os.path.join(data_path, \"test.txt\")\n",
        "if not (os.path.exists(train_path) or os.path.exists(validate_path) or os.path.exists(test_path)):\n",
        "  url = 'https://raw.githubusercontent.com/rekiksab/Yelp-Data-Challenge-2013/master/yelp_challenge/yelp_phoenix_academic_dataset/yelp_academic_dataset_review.json'\n",
        "  df = pd.read_json(url, lines=True)\n",
        "  text = df['text']\n",
        "  train, validate, test = np.split(text.sample(frac=1, random_state=42), [int(.6*len(text)), int(.8*len(text))])\n",
        "  np.savetxt(train_path, train.values, fmt='%s')\n",
        "  np.savetxt(validate_path, validate.values, fmt='%s')\n",
        "  np.savetxt(test_path, test.values, fmt='%s')\n",
        "  train_path = os.path.join(data_path, \"train.txt\")\n",
        "  validate_path = os.path.join(data_path, \"validate.txt\")\n",
        "  test_path = os.path.join(data_path, \"test.txt\")\n",
        "\n",
        "vocab_path = os.path.join(data_path, \"vocab.txt\")\n",
        "word_to_id = tx.data.make_vocab(\n",
        "    train_path, return_type=\"dict\")\n",
        "\n",
        "with open(vocab_path, 'w') as fvocab:\n",
        "    for word in word_to_id:\n",
        "        fvocab.write(\"%s\\n\" % word)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVNAghBeu8TC"
      },
      "source": [
        "# VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9_L26Qgu_5o"
      },
      "source": [
        "def kl_divergence(means: Tensor, logvars: Tensor) -> Tensor:\n",
        "    \"\"\"Compute the KL divergence between Gaussian distribution\n",
        "    \"\"\"\n",
        "    kl_cost = -0.5 * (logvars - means ** 2 -\n",
        "                      torch.exp(logvars) + 1.0)\n",
        "    kl_cost = torch.mean(kl_cost, 0)\n",
        "    return torch.sum(kl_cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ckhxqwvITp"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    _latent_z: Tensor\n",
        "\n",
        "    def __init__(self, vocab_size: int, config_model):\n",
        "        super().__init__()\n",
        "        # Model architecture\n",
        "        self._config = config_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.encoder_w_embedder = tx.modules.WordEmbedder(\n",
        "            vocab_size=vocab_size, hparams=config_model.enc_emb_hparams)\n",
        "\n",
        "        self.encoder = tx.modules.UnidirectionalRNNEncoder[tx.core.LSTMState](\n",
        "            input_size=self.encoder_w_embedder.dim,\n",
        "            hparams={\n",
        "                \"rnn_cell\": config_model.enc_cell_hparams,\n",
        "            })\n",
        "\n",
        "        self.decoder_w_embedder = tx.modules.WordEmbedder(\n",
        "            vocab_size=vocab_size, hparams=config_model.dec_emb_hparams)\n",
        "\n",
        "        if config_model.decoder_type == \"lstm\":\n",
        "            self.lstm_decoder = tx.modules.BasicRNNDecoder(\n",
        "                input_size=(self.decoder_w_embedder.dim +\n",
        "                            config_model.latent_dims),\n",
        "                vocab_size=vocab_size,\n",
        "                token_embedder=self._embed_fn_rnn,\n",
        "                hparams={\"rnn_cell\": config_model.dec_cell_hparams})\n",
        "            sum_state_size = self.lstm_decoder.cell.hidden_size * 2\n",
        "\n",
        "        elif config_model.decoder_type == 'transformer':\n",
        "            # position embedding\n",
        "            self.decoder_p_embedder = tx.modules.SinusoidsPositionEmbedder(\n",
        "                position_size=config_model.max_pos,\n",
        "                hparams=config_model.dec_pos_emb_hparams)\n",
        "            # decoder\n",
        "            self.transformer_decoder = tx.modules.TransformerDecoder(\n",
        "                # tie word embedding with output layer\n",
        "                output_layer=self.decoder_w_embedder.embedding,\n",
        "                token_pos_embedder=self._embed_fn_transformer,\n",
        "                hparams=config_model.trans_hparams)\n",
        "            sum_state_size = self._config.dec_emb_hparams[\"dim\"]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Decoder type must be 'lstm' or 'transformer'\")\n",
        "\n",
        "        self.connector_mlp = tx.modules.MLPTransformConnector(\n",
        "            config_model.latent_dims * 2,\n",
        "            linear_layer_dim=self.encoder.cell.hidden_size * 2)\n",
        "\n",
        "        self.mlp_linear_layer = nn.Linear(\n",
        "            config_model.latent_dims, sum_state_size)\n",
        "\n",
        "    def forward(self,  # type: ignore\n",
        "                data_batch: tx.data.Batch,\n",
        "                kl_weight: float, start_tokens: torch.LongTensor,\n",
        "                end_token: int) -> Dict[str, Tensor]:\n",
        "        # encoder -> connector -> decoder\n",
        "        text_ids = data_batch[\"text_ids\"].to(self.device)\n",
        "        input_embed = self.encoder_w_embedder(text_ids)\n",
        "        _, encoder_states = self.encoder(\n",
        "            input_embed,\n",
        "            sequence_length=data_batch[\"length\"].to(self.device))\n",
        "\n",
        "        mean_logvar = self.connector_mlp(encoder_states)\n",
        "        mean, logvar = torch.chunk(mean_logvar, 2, 1)\n",
        "        kl_loss = kl_divergence(mean, logvar)\n",
        "        dst = MultivariateNormalDiag(\n",
        "            loc=mean, scale_diag=torch.exp(0.5 * logvar))\n",
        "\n",
        "        latent_z = dst.rsample()\n",
        "        helper = None\n",
        "        if self._config.decoder_type == \"lstm\":\n",
        "            helper = self.lstm_decoder.create_helper(\n",
        "                decoding_strategy=\"train_greedy\",\n",
        "                start_tokens=start_tokens,\n",
        "                end_token=end_token)\n",
        "\n",
        "        # decode\n",
        "        seq_lengths = data_batch[\"length\"].to(self.device) - 1\n",
        "        outputs = self.decode(\n",
        "            helper=helper, latent_z=latent_z,\n",
        "            text_ids=text_ids[:, :-1], seq_lengths=seq_lengths)\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Losses & train ops\n",
        "        rc_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n",
        "            labels=text_ids[:, 1:], logits=logits,\n",
        "            sequence_length=seq_lengths)\n",
        "\n",
        "        nll = rc_loss + kl_weight * kl_loss\n",
        "\n",
        "        ret = {\n",
        "            \"nll\": nll,\n",
        "            \"kl_loss\": kl_loss,\n",
        "            \"rc_loss\": rc_loss,\n",
        "            \"lengths\": seq_lengths,\n",
        "        }\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def _embed_fn_rnn(self, tokens: torch.LongTensor) -> Tensor:\n",
        "        r\"\"\"Generates word embeddings\n",
        "        \"\"\"\n",
        "        embedding = self.decoder_w_embedder(tokens)\n",
        "        latent_z = self._latent_z\n",
        "        if len(embedding.size()) > 2:\n",
        "            latent_z = latent_z.unsqueeze(0).repeat(tokens.size(0), 1, 1)\n",
        "        return torch.cat([embedding, latent_z], dim=-1)\n",
        "\n",
        "    def _embed_fn_transformer(self,\n",
        "                              tokens: torch.LongTensor,\n",
        "                              positions: torch.LongTensor) -> Tensor:\n",
        "        r\"\"\"Generates word embeddings combined with positional embeddings\n",
        "        \"\"\"\n",
        "        output_p_embed = self.decoder_p_embedder(positions)\n",
        "        output_w_embed = self.decoder_w_embedder(tokens)\n",
        "        output_w_embed = output_w_embed * self._config.hidden_size ** 0.5\n",
        "        output_embed = output_w_embed + output_p_embed\n",
        "        return output_embed\n",
        "\n",
        "    @property\n",
        "    def decoder(self) -> tx.modules.DecoderBase:\n",
        "        if self._config.decoder_type == \"lstm\":\n",
        "            return self.lstm_decoder\n",
        "        else:\n",
        "            return self.transformer_decoder\n",
        "\n",
        "    def decode(self,\n",
        "               helper: Optional[tx.modules.Helper],\n",
        "               latent_z: Tensor,\n",
        "               text_ids: Optional[torch.LongTensor] = None,\n",
        "               seq_lengths: Optional[Tensor] = None,\n",
        "               max_decoding_length: Optional[int] = None) \\\n",
        "            -> Union[tx.modules.BasicRNNDecoderOutput,\n",
        "                     tx.modules.TransformerDecoderOutput]:\n",
        "        self._latent_z = latent_z\n",
        "        fc_output = self.mlp_linear_layer(latent_z)\n",
        "\n",
        "        if self._config.decoder_type == \"lstm\":\n",
        "            lstm_states = torch.chunk(fc_output, 2, dim=1)\n",
        "            outputs, _, _ = self.lstm_decoder(\n",
        "                initial_state=lstm_states,\n",
        "                inputs=text_ids,\n",
        "                helper=helper,\n",
        "                sequence_length=seq_lengths,\n",
        "                max_decoding_length=max_decoding_length)\n",
        "        else:\n",
        "            transformer_states = fc_output.unsqueeze(1)\n",
        "            outputs = self.transformer_decoder(\n",
        "                inputs=text_ids,\n",
        "                memory=transformer_states,\n",
        "                memory_sequence_length=torch.ones(transformer_states.size(0)),\n",
        "                helper=helper,\n",
        "                max_decoding_length=max_decoding_length)\n",
        "        return outputs\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YZOkyApvNmP"
      },
      "source": [
        "# PID Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6g1VCAvPdJ"
      },
      "source": [
        "class PIDControl():\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"define them out of loop\"\"\"\n",
        "        # self.exp_KL = exp_KL\n",
        "        self.I_k1 = 0.0\n",
        "        self.W_k1 = 0.0\n",
        "        self.e_k1 = 0.0\n",
        "        \n",
        "    def _Kp_fun(self, Err, scale=1):\n",
        "        return 1.0/(1.0 + float(scale)*math.exp(Err))\n",
        "        \n",
        "\n",
        "    def pid(self, exp_KL, kl_loss, Kp=0.001, Ki=-0.001, Kd=0.01):\n",
        "        \"\"\"\n",
        "        position PID algorithm\n",
        "        Input: KL_loss\n",
        "        return: weight for KL loss, beta\n",
        "        \"\"\"\n",
        "        error_k = exp_KL - kl_loss\n",
        "        ## comput U as the control factor\n",
        "        Pk = Kp * self._Kp_fun(error_k)\n",
        "        Ik = self.I_k1 + Ki * error_k\n",
        "\n",
        "        ## window up for integrator\n",
        "        if self.W_k1 < 0 and self.W_k1 > 1:\n",
        "            Ik = self.I_k1\n",
        "            \n",
        "        Wk = Pk + Ik\n",
        "        self.W_k1 = Wk\n",
        "        self.I_k1 = Ik\n",
        "        self.e_k1 = error_k\n",
        "        \n",
        "        ## min and max value\n",
        "        if Wk > 1:\n",
        "            Wk = 1.0\n",
        "        if Wk < 0:\n",
        "            Wk = 0.0\n",
        "        \n",
        "        return Wk\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV3yJUUGvZwz"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSMZHtSCvizg"
      },
      "source": [
        "class Config():\n",
        "  dataset = \"ptb\"\n",
        "  num_epochs = 100\n",
        "  hidden_size = 256\n",
        "  dec_dropout_in = 0.5\n",
        "  dec_dropout_out = 0.5\n",
        "  enc_dropout_in = 0.\n",
        "  enc_dropout_out = 0.\n",
        "  word_keep_prob = 0.5\n",
        "  batch_size = 32\n",
        "  embed_dim = 256\n",
        "\n",
        "  latent_dims = 32\n",
        "\n",
        "  lr_decay_hparams = {\n",
        "      \"init_lr\": 0.001,\n",
        "      \"threshold\": 2,\n",
        "      \"decay_factor\": 0.5,\n",
        "      \"max_decay\": 5\n",
        "  }\n",
        "\n",
        "\n",
        "  decoder_type = 'lstm'\n",
        "\n",
        "  enc_cell_hparams = {\n",
        "      \"type\": \"LSTMCell\",\n",
        "      \"kwargs\": {\n",
        "          \"num_units\": hidden_size,\n",
        "          \"bias\": 0.\n",
        "      },\n",
        "      \"dropout\": {\"output_keep_prob\": 1. - enc_dropout_out},\n",
        "      \"num_layers\": 1\n",
        "  }\n",
        "\n",
        "  dec_cell_hparams = {\n",
        "      \"type\": \"LSTMCell\",\n",
        "      \"kwargs\": {\n",
        "          \"num_units\": hidden_size,\n",
        "          \"bias\": 0.,\n",
        "      },\n",
        "      \"dropout\": {\"output_keep_prob\": 1. - dec_dropout_out},\n",
        "      \"num_layers\": 1,\n",
        "  }\n",
        "\n",
        "  enc_emb_hparams = {\n",
        "      'name': 'lookup_table',\n",
        "      \"dim\": embed_dim,\n",
        "      \"dropout_rate\": enc_dropout_in,\n",
        "      'initializer': {\n",
        "          'type': 'normal_',\n",
        "          'kwargs': {\n",
        "              'mean': 0.0,\n",
        "              'std': embed_dim**-0.5,\n",
        "          },\n",
        "      }\n",
        "  }\n",
        "\n",
        "  dec_emb_hparams = {\n",
        "      'name': 'lookup_table',\n",
        "      \"dim\": embed_dim,\n",
        "      \"dropout_rate\": dec_dropout_in,\n",
        "      'initializer': {\n",
        "          'type': 'normal_',\n",
        "          'kwargs': {\n",
        "              'mean': 0.0,\n",
        "              'std': embed_dim**-0.5,\n",
        "          },\n",
        "      }\n",
        "  }\n",
        "\n",
        "  # KL annealing\n",
        "  kl_anneal_hparams = {\n",
        "      \"warm_up\": 10,\n",
        "      \"start\": 0.1\n",
        "  }\n",
        "\n",
        "  train_data_hparams = {\n",
        "      \"num_epochs\": 1,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"seed\": 123,\n",
        "      \"dataset\": {\n",
        "          \"files\": './simple-examples/data/ptb.train.txt',\n",
        "          \"vocab_file\": './simple-examples/data/vocab.txt'\n",
        "      }\n",
        "  }\n",
        "\n",
        "  val_data_hparams = {\n",
        "      \"num_epochs\": 1,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"seed\": 123,\n",
        "      \"dataset\": {\n",
        "          \"files\": './simple-examples/data/ptb.valid.txt',\n",
        "          \"vocab_file\": './simple-examples/data/vocab.txt'\n",
        "      }\n",
        "  }\n",
        "\n",
        "  test_data_hparams = {\n",
        "      \"num_epochs\": 1,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"dataset\": {\n",
        "          \"files\": './simple-examples/data/ptb.test.txt',\n",
        "          \"vocab_file\": './simple-examples/data/vocab.txt'\n",
        "      }\n",
        "  }\n",
        "\n",
        "  opt_hparams = {\n",
        "      'optimizer': {\n",
        "          'type': 'Adam',\n",
        "          'kwargs': {\n",
        "              'lr': 0.001\n",
        "          }\n",
        "      },\n",
        "      'gradient_clip': {\n",
        "          \"type\": \"clip_grad_norm_\",\n",
        "          \"kwargs\": {\n",
        "              \"max_norm\": 5,\n",
        "              \"norm_type\": 2\n",
        "          }\n",
        "      }\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK5x-aPqwVOA"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxvnGdq3wWdh"
      },
      "source": [
        "config = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data = tx.data.MonoTextData(config.train_data_hparams, device=torch.device(\"cpu\"))\n",
        "val_data = tx.data.MonoTextData(config.val_data_hparams, device=torch.device(\"cpu\"))\n",
        "test_data = tx.data.MonoTextData(config.test_data_hparams, device=torch.device(\"cpu\"))\n",
        "\n",
        "iterator = tx.data.DataIterator({\"train\": train_data, \"valid\": val_data, \"test\": test_data})\n",
        "\n",
        "opt_vars = {\n",
        "    'learning_rate': config.lr_decay_hparams[\"init_lr\"],\n",
        "    'best_valid_nll': 1e100,\n",
        "    'steps_not_improved': 0,\n",
        "    'kl_weight': config.kl_anneal_hparams[\"start\"]\n",
        "}\n",
        "\n",
        "decay_cnt = 0\n",
        "max_decay = config.lr_decay_hparams[\"max_decay\"]\n",
        "decay_factor = config.lr_decay_hparams[\"decay_factor\"]\n",
        "decay_ts = config.lr_decay_hparams[\"threshold\"]\n",
        "\n",
        "save_path = './checkpoint.ckpt'\n",
        "\n",
        "anneal_r = 1.0 / (config.kl_anneal_hparams[\"warm_up\"] * (len(train_data) / config.batch_size))\n",
        "\n",
        "vocab = train_data.vocab\n",
        "model = VAE(train_data.vocab.size, config)\n",
        "model.to(device)\n",
        "\n",
        "start_tokens = torch.full(\n",
        "    (config.batch_size,),\n",
        "    vocab.bos_token_id,\n",
        "    dtype=torch.long).to(device)\n",
        "end_token = vocab.eos_token_id\n",
        "optimizer = tx.core.get_optimizer(\n",
        "    params=model.parameters(),\n",
        "    hparams=config.opt_hparams)\n",
        "scheduler = ExponentialLR(optimizer, decay_factor)\n",
        "\n",
        "max_iter = min(config.num_epochs*len(train_data)/config.batch_size, 80000)\n",
        "print('max steps:', max_iter)\n",
        "\n",
        "global_steps = {}\n",
        "global_steps['step'] = 0\n",
        "pid = PIDControl()\n",
        "opt_vars[\"kl_weight\"] = 0.0\n",
        "Kp = 0.01\n",
        "Ki = -0.0001\n",
        "exp_kl = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwsjM_00yQiU"
      },
      "source": [
        "def _run_epoch(epoch: int, mode: str, display: int = 10) -> Tuple[Tensor, float]:\n",
        "    iterator.switch_to_dataset(mode)\n",
        "\n",
        "    if mode == 'train':\n",
        "        model.train()\n",
        "        kl_weight = opt_vars[\"kl_weight\"]\n",
        "    else:\n",
        "        model.eval()\n",
        "        kl_weight = 1.0\n",
        "    \n",
        "    num_words = 0\n",
        "    nll_total = 0.\n",
        "\n",
        "    avg_rec = tx.utils.AverageRecorder()\n",
        "    for batch in iterator:\n",
        "        ## run model to get loss function\n",
        "        if global_steps['step']>= max_iter:\n",
        "            break\n",
        "        ret = model(batch, kl_weight, start_tokens, end_token)\n",
        "        if mode == \"train\":\n",
        "            pbar.update(1)\n",
        "            global_steps['step'] += 1\n",
        "            kl_loss = ret['kl_loss'].item()\n",
        "            rec_loss = ret['rc_loss'].item()\n",
        "            total_loss = ret[\"nll\"].item()\n",
        "            kl_weight = pid.pid(exp_kl, kl_loss, Kp, Ki)\n",
        "\n",
        "            opt_vars[\"kl_weight\"] = kl_weight\n",
        "            \n",
        "            ## total loss\n",
        "            ret[\"nll\"].backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        batch_size = len(ret[\"lengths\"])\n",
        "        num_words += torch.sum(ret[\"lengths\"]).item()\n",
        "        nll_total += ret[\"nll\"].item() * batch_size\n",
        "        avg_rec.add(\n",
        "            [ret[\"nll\"].item(),\n",
        "              ret[\"kl_loss\"].item(),\n",
        "              ret[\"rc_loss\"].item()],\n",
        "            batch_size)\n",
        "            \n",
        "        if global_steps['step'] % display == 1 and mode == 'train':\n",
        "            nll = avg_rec.avg(0)\n",
        "            klw = opt_vars[\"kl_weight\"]\n",
        "            KL = avg_rec.avg(1)\n",
        "            rc = avg_rec.avg(2)\n",
        "            \n",
        "    nll = avg_rec.avg(0)\n",
        "    KL = avg_rec.avg(1)\n",
        "    rc = avg_rec.avg(2)\n",
        "    if num_words > 0:\n",
        "        log_ppl = nll_total / num_words\n",
        "        ppl = math.exp(log_ppl)\n",
        "    else:\n",
        "        log_ppl = 100\n",
        "        ppl = math.exp(log_ppl)\n",
        "        nll = 1000\n",
        "        KL = args.exp_kl\n",
        "    \n",
        "    print(f\"\\n{mode}: epoch {epoch}, nll {nll:.4f}, KL {KL:.4f}, \"\n",
        "          f\"rc {rc:.4f}, log_ppl {log_ppl:.4f}, ppl {ppl:.4f}\")\n",
        "    return nll, ppl  # type: ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxMAUE7rytC1"
      },
      "source": [
        "# Counts trainable parameters\n",
        "total_parameters = sum(param.numel() for param in model.parameters())\n",
        "print(f\"{total_parameters} total parameters\")\n",
        "\n",
        "best_nll = best_ppl = 0.\n",
        "\n",
        "## start running model\n",
        "pbar = tqdm(total = int(max_iter))\n",
        "for epoch in range(config.num_epochs):\n",
        "    _, _ = _run_epoch(epoch, 'train', display=200)\n",
        "    val_nll, _ = _run_epoch(epoch, 'valid')\n",
        "    test_nll, test_ppl = _run_epoch(epoch, 'test')\n",
        "\n",
        "    if val_nll < opt_vars['best_valid_nll']:\n",
        "        opt_vars['best_valid_nll'] = val_nll\n",
        "        opt_vars['steps_not_improved'] = 0\n",
        "        best_nll = test_nll\n",
        "        best_ppl = test_ppl\n",
        "\n",
        "        states = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"scheduler\": scheduler.state_dict()\n",
        "        }\n",
        "        torch.save(states, save_path)\n",
        "    else:\n",
        "        opt_vars['steps_not_improved'] += 1\n",
        "        if opt_vars['steps_not_improved'] == decay_ts:\n",
        "            old_lr = opt_vars['learning_rate']\n",
        "            opt_vars['learning_rate'] *= decay_factor\n",
        "            opt_vars['steps_not_improved'] = 0\n",
        "            new_lr = opt_vars['learning_rate']\n",
        "            ckpt = torch.load(save_path)\n",
        "            model.load_state_dict(ckpt['model'])\n",
        "            optimizer.load_state_dict(ckpt['optimizer'])\n",
        "            scheduler.load_state_dict(ckpt['scheduler'])\n",
        "            scheduler.step()\n",
        "            print(f\"-----\\nchange lr, old lr: {old_lr}, \"\n",
        "                  f\"new lr: {new_lr}\\n-----\")\n",
        "\n",
        "            decay_cnt += 1\n",
        "            if decay_cnt == max_decay:\n",
        "                break\n",
        "    if global_steps['step'] >= max_iter:\n",
        "        break\n",
        "\n",
        "print(f\"\\nbest testing nll: {best_nll:.4f},\"\n",
        "      f\"best testing ppl {best_ppl:.4f}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgc9JOzGGf-i"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNUYkXKPGh3K"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "batch_size = train_data.batch_size\n",
        "\n",
        "dst = MultivariateNormalDiag(\n",
        "    loc=torch.zeros(batch_size, config.latent_dims),\n",
        "    scale_diag=torch.ones(batch_size, config.latent_dims))\n",
        "\n",
        "# latent_z = dst.rsample().to(device)\n",
        "latent_z = torch.FloatTensor(batch_size, config.latent_dims).uniform_(-1, 1).to(device)\n",
        "# latent_z = torch.randn(batch_size, config.latent_dims).to(device)\n",
        "\n",
        "helper = model.decoder.create_helper(\n",
        "    decoding_strategy='infer_sample',\n",
        "    start_tokens=start_tokens,\n",
        "    end_token=end_token)\n",
        "outputs = model.decode(\n",
        "    helper=helper,\n",
        "    latent_z=latent_z,\n",
        "    max_decoding_length=100)\n",
        "\n",
        "if config.decoder_type == \"transformer\":\n",
        "    outputs = outputs[0]\n",
        "\n",
        "sample_tokens = vocab.map_ids_to_tokens_py(outputs.sample_id.cpu())\n",
        "\n",
        "for sent in sample_tokens:\n",
        "    sent = tx.utils.compat_as_text(list(sent))\n",
        "    end_id = len(sent)\n",
        "    if vocab.eos_token in sent:\n",
        "        end_id = sent.index(vocab.eos_token)\n",
        "    print(' '.join(sent[:end_id + 1]) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}