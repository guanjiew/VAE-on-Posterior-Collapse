{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ControlVAE Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanjiew/csc412_vae/blob/main/ControlVAE_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYAruimtNmL"
      },
      "source": [
        "Code is adapted from [the original ControlVAE repository](https://github.com/shj1987/ControlVAE-ICML2020/tree/master/Language_modeling/Text_gen_PTB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfLIsQFIvrMn"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W-1FfEitYz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e28893e-ad52-4b25-d550-e148843a4575"
      },
      "source": [
        "%pip install texar-pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting texar-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/7e/20aa39ee9d19dcd1a1c0db4dad878088cfba216f45aeaa8fa89615ef46c0/texar_pytorch-0.1.2.post1-py3-none-any.whl (434kB)\n",
            "\r\u001b[K     |▊                               | 10kB 14.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 15.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30kB 8.8MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 153kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 174kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 194kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 215kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 225kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 235kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 245kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 256kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 266kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 276kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 286kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 296kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 307kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 317kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 327kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 337kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 348kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 358kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 368kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 378kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 389kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 399kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 409kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 430kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 440kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2018.01.10 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2.23.0)\n",
            "Collecting mypy-extensions\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<=1.19.5,>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (1.19.5)\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (20.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=19.0->texar-pytorch) (2.4.7)\n",
            "Installing collected packages: mypy-extensions, funcsigs, sentencepiece, texar-pytorch\n",
            "Successfully installed funcsigs-1.0.2 mypy-extensions-0.4.3 sentencepiece-0.1.91 texar-pytorch-0.1.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA-j84Kot-hG"
      },
      "source": [
        "import math, os, random\n",
        "from collections import Counter\n",
        "from typing import Any, Dict, Optional, Tuple, Union\n",
        "random.seed(42)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "import texar.torch as tx\n",
        "from texar.torch.custom import MultivariateNormalDiag\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVNAghBeu8TC"
      },
      "source": [
        "# VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9_L26Qgu_5o"
      },
      "source": [
        "def kl_divergence(means: Tensor, logvars: Tensor) -> Tensor:\n",
        "    \"\"\"Compute the KL divergence between Gaussian distribution\n",
        "    \"\"\"\n",
        "    kl_cost = -0.5 * (logvars - means ** 2 -\n",
        "                      torch.exp(logvars) + 1.0)\n",
        "    kl_cost = torch.mean(kl_cost, 0)\n",
        "    return torch.sum(kl_cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ckhxqwvITp"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "    def __init__(self, config_model, vocab_size, padding_idx):\n",
        "        super().__init__()\n",
        "        # Model architecture\n",
        "        self._config = config_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self._config.embed_dim,\n",
        "            padding_idx=padding_idx\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.enc_dropout_in = nn.Dropout(self._config.enc_dropout_in).to(self.device)\n",
        "        \n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=self._config.embed_dim,\n",
        "            hidden_size=self._config.hidden_size,\n",
        "            batch_first=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.enc_dropout_out = nn.Dropout(self._config.enc_dropout_out).to(self.device)\n",
        "\n",
        "        self.connector_mlp = nn.Linear(\n",
        "            in_features=self._config.hidden_size * 2,\n",
        "            out_features=self._config.latent_dims * 2\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.mlp_linear_layer = nn.Linear(\n",
        "            in_features=self._config.latent_dims,\n",
        "            out_features=self._config.hidden_size * 2\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.dec_dropout_in = nn.Dropout(self._config.dec_dropout_in).to(self.device)\n",
        "\n",
        "        lstm_input = self._config.embed_dim\n",
        "        logits_input = self._config.hidden_size\n",
        "        if self._config.skip_connection:\n",
        "            lstm_input += self._config.latent_dims\n",
        "            logits_input += self._config.latent_dims\n",
        "\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=lstm_input,\n",
        "            hidden_size=self._config.hidden_size,\n",
        "            batch_first=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.dec_dropout_out = nn.Dropout(self._config.dec_dropout_out).to(self.device)\n",
        "\n",
        "        self.logits_layer = nn.Linear(\n",
        "            in_features=logits_input,\n",
        "            out_features=vocab_size\n",
        "        ).to(self.device)\n",
        "    \n",
        "    def init_state(self, batch_size=1):\n",
        "        return (\n",
        "            torch.zeros(1, batch_size, self._config.hidden_size).to(self.device),\n",
        "            torch.zeros(1, batch_size, self._config.hidden_size).to(self.device)\n",
        "        )\n",
        "\n",
        "    def encode(self, x, seq_lengths):\n",
        "        emb = self.enc_dropout_in(self.embedding(x))\n",
        "        emb = torch.nn.utils.rnn.pack_padded_sequence(emb, seq_lengths, batch_first=True, enforce_sorted=False) # https://towardsdatascience.com/61d35642972e\n",
        "        _, state = self.encoder(emb, self.init_state(x.size()[0]))\n",
        "        state = self.enc_dropout_out(torch.cat(state, dim=-1)[0])\n",
        "        mean_logvar = self.connector_mlp(state)\n",
        "        mean, logvar = torch.chunk(mean_logvar, 2, 1)\n",
        "        return mean, logvar\n",
        "    \n",
        "    def decode(self, z, x, seq_lengths, prev_state=None):\n",
        "        if prev_state is None:\n",
        "            prev_state = torch.chunk(self.mlp_linear_layer(z).unsqueeze(0), 2, -1)\n",
        "        emb = self.dec_dropout_in(self.embedding(x))\n",
        "        if self._config.skip_connection:\n",
        "            z_seq = torch.cat([z.unsqueeze(1) for _ in range(emb.size()[1])], dim=1)\n",
        "            emb = torch.cat((emb, z_seq), dim=-1)\n",
        "        emb = torch.nn.utils.rnn.pack_padded_sequence(emb, seq_lengths, batch_first=True, enforce_sorted=False) # https://towardsdatascience.com/61d35642972e\n",
        "        outputs, state = self.decoder(emb, prev_state)\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        outputs = self.dec_dropout_out(outputs)\n",
        "        if self._config.skip_connection:\n",
        "            z_seq = torch.cat([z.unsqueeze(1) for _ in range(outputs.size()[1])], dim=1)\n",
        "            outputs = torch.cat((outputs, z_seq), dim=-1)\n",
        "        logits = self.logits_layer(outputs)\n",
        "        return logits, state\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YZOkyApvNmP"
      },
      "source": [
        "# PID Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6g1VCAvPdJ"
      },
      "source": [
        "class PIDControl():\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"define them out of loop\"\"\"\n",
        "        # self.exp_KL = exp_KL\n",
        "        self.I_k1 = 0.0\n",
        "        self.W_k1 = 0.0\n",
        "        self.e_k1 = 0.0\n",
        "        \n",
        "    def _Kp_fun(self, Err, scale=1):\n",
        "        return 1.0/(1.0 + float(scale)*math.exp(Err))\n",
        "        \n",
        "\n",
        "    def pid(self, exp_KL, kl_loss, Kp=0.001, Ki=-0.001, Kd=0.01):\n",
        "        \"\"\"\n",
        "        position PID algorithm\n",
        "        Input: KL_loss\n",
        "        return: weight for KL loss, beta\n",
        "        \"\"\"\n",
        "        error_k = exp_KL - kl_loss\n",
        "        ## comput U as the control factor\n",
        "        Pk = Kp * self._Kp_fun(error_k)\n",
        "        Ik = self.I_k1 + Ki * error_k\n",
        "\n",
        "        ## window up for integrator\n",
        "        if self.W_k1 < 0 and self.W_k1 > 1:\n",
        "            Ik = self.I_k1\n",
        "            \n",
        "        Wk = Pk + Ik\n",
        "        self.W_k1 = Wk\n",
        "        self.I_k1 = Ik\n",
        "        self.e_k1 = error_k\n",
        "        \n",
        "        ## min and max value\n",
        "        if Wk > 1:\n",
        "            Wk = 1.0\n",
        "        if Wk < 0:\n",
        "            Wk = 0.0\n",
        "        \n",
        "        return Wk\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV3yJUUGvZwz"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSMZHtSCvizg"
      },
      "source": [
        "class Config():\n",
        "  dataset_size = 8750\n",
        "  num_epochs = 100\n",
        "  hidden_size = 256\n",
        "  dec_dropout_in = 0.5\n",
        "  dec_dropout_out = 0.5\n",
        "  enc_dropout_in = 0.\n",
        "  enc_dropout_out = 0.\n",
        "  batch_size = 32\n",
        "  embed_dim = 256\n",
        "  latent_dims = 32\n",
        "  max_vocab = 12000\n",
        "\n",
        "  pid_control = True\n",
        "  skip_connection = True\n",
        "\n",
        "  lr_decay_hparams = {\n",
        "      \"init_lr\": 0.001,\n",
        "      \"threshold\": 2,\n",
        "      \"decay_factor\": 0.5,\n",
        "      \"max_decay\": 5\n",
        "  }\n",
        "\n",
        "  # KL annealing\n",
        "  kl_anneal_hparams = {\n",
        "      \"warm_up\": 10,\n",
        "      \"start\": 0.1\n",
        "  }\n",
        "\n",
        "  initializer_hparams = {\n",
        "      'mean': 0.0,\n",
        "      'std': embed_dim**-0.5,\n",
        "  }\n",
        "\n",
        "  opt_hparams = {\n",
        "      'optimizer': {\n",
        "          'type': 'Adam',\n",
        "          'kwargs': {\n",
        "              'lr': 0.001\n",
        "          }\n",
        "      },\n",
        "      'gradient_clip': {\n",
        "          \"type\": \"clip_grad_norm_\",\n",
        "          \"kwargs\": {\n",
        "              \"max_norm\": 5,\n",
        "              \"norm_type\": 2\n",
        "          }\n",
        "      }\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP2M-MCxUL-7"
      },
      "source": [
        "config = Config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "madW6YSv1jay"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tItIdmTwj-4"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/rekiksab/Yelp-Data-Challenge-2013/master/yelp_challenge/yelp_phoenix_academic_dataset/yelp_academic_dataset_review.json'\n",
        "data = pd.read_json(url, lines=True)\n",
        "data = data.loc[:,['text','stars']]\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "data.loc[:,'text'] = data.loc[:,'text'].map(lambda s: ['<BOS>']+tokenizer(s)+['<EOS>','<PAD>'])\n",
        "\n",
        "counter = Counter()\n",
        "for line in data.loc[:,'text'].values:\n",
        "    counter.update(line)\n",
        "vocab = Vocab(counter, max_size=config.max_vocab)\n",
        "vocab.bos_index = vocab['<BOS>']\n",
        "vocab.eos_index = vocab['<EOS>']\n",
        "vocab.padding_index = vocab['<PAD>']\n",
        "\n",
        "data.loc[:,'text'] = data.loc[:,'text'].map(lambda s: [vocab[token] for token in s[:-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXM0LgELKbq"
      },
      "source": [
        "data_pairs = [(pair[0], pair[1]) for pair in data.loc[:,['text','stars']].values]\n",
        "if len(data_pairs) > config.dataset_size:\n",
        "    data_pairs = random.sample(data_pairs, config.dataset_size)\n",
        "\n",
        "random.shuffle(data_pairs)\n",
        "valid_len = round(0.15 * len(data_pairs))\n",
        "valid_data, test_data, train_data = data_pairs[:valid_len], data_pairs[valid_len:2*valid_len], data_pairs[2*valid_len:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm-HfNP9LUNf"
      },
      "source": [
        "def get_padded_batches(data, batch_size, padding_index):\n",
        "    batches = []\n",
        "    seq_lengths = []\n",
        "    for k in range(batch_size, len(data), batch_size):\n",
        "        batch = [pair[0] for pair in data[k-batch_size:k]]\n",
        "        lengths = [len(text) for text in batch]\n",
        "        padded_length = max(lengths)\n",
        "        for text in batch:\n",
        "            while len(text) < padded_length:\n",
        "                text.append(padding_index)\n",
        "        batches.append(torch.tensor(batch))\n",
        "        seq_lengths.append(torch.tensor(lengths))\n",
        "    return batches, seq_lengths\n",
        "\n",
        "train_batches, train_lengths = get_padded_batches(train_data, config.batch_size, vocab.padding_index)\n",
        "valid_batches, valid_lengths = get_padded_batches(valid_data, config.batch_size, vocab.padding_index)\n",
        "test_batches, test_lengths = get_padded_batches(test_data, config.batch_size, vocab.padding_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK5x-aPqwVOA"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxvnGdq3wWdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "259177ac-8c33-4977-fdfb-5a8f14b99da8"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = {\n",
        "    \"train\": list(zip(train_batches, train_lengths)),\n",
        "    \"valid\": list(zip(valid_batches, valid_lengths)),\n",
        "    \"test\": list(zip(test_batches, test_lengths))\n",
        "}\n",
        "\n",
        "opt_vars = {\n",
        "    'learning_rate': config.lr_decay_hparams[\"init_lr\"],\n",
        "    'best_valid_nll': 1e100,\n",
        "    'steps_not_improved': 0,\n",
        "    'kl_weight': 1.0\n",
        "}\n",
        "\n",
        "decay_cnt = 0\n",
        "max_decay = config.lr_decay_hparams[\"max_decay\"]\n",
        "decay_factor = config.lr_decay_hparams[\"decay_factor\"]\n",
        "decay_ts = config.lr_decay_hparams[\"threshold\"]\n",
        "\n",
        "save_path = './checkpoint.ckpt'\n",
        "\n",
        "model = VAE(config, len(vocab.itos), vocab.padding_index)\n",
        "\n",
        "optimizer = tx.core.get_optimizer(\n",
        "    params=model.parameters(),\n",
        "    hparams=config.opt_hparams)\n",
        "scheduler = ExponentialLR(optimizer, decay_factor)\n",
        "\n",
        "max_iter = min(config.num_epochs*len(train_data)/config.batch_size, 80000)\n",
        "print('max steps:', max_iter)\n",
        "\n",
        "global_steps = {}\n",
        "global_steps['step'] = 0\n",
        "pid = PIDControl()\n",
        "opt_vars[\"kl_weight\"] = 1.0\n",
        "Kp = 0.01\n",
        "Ki = -0.0001\n",
        "exp_kl = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max steps: 76562.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwsjM_00yQiU"
      },
      "source": [
        "def _run_epoch(epoch: int, mode: str, display: int = 10) -> Tuple[Tensor, float]:\n",
        "    if mode == 'train':\n",
        "        model.train()\n",
        "        kl_weight = opt_vars[\"kl_weight\"]\n",
        "    else:\n",
        "        model.eval()\n",
        "        kl_weight = 1.0\n",
        "    \n",
        "    num_words = 0\n",
        "    nll_total = 0.\n",
        "\n",
        "    avg_rec = tx.utils.AverageRecorder()\n",
        "    for batch, seq_lengths in dataset[mode]:\n",
        "        if global_steps['step']>= max_iter:\n",
        "            break\n",
        "        mean, logvar = model.encode(batch.to(model.device), seq_lengths)\n",
        "        dst = MultivariateNormalDiag(loc=mean, scale_diag=torch.exp(0.5 * logvar))\n",
        "        z = dst.sample()\n",
        "        logits, _ = model.decode(mean, batch[:,:-1].to(model.device), seq_lengths-1)\n",
        "        rec_loss = tx.losses.sequence_sparse_softmax_cross_entropy(labels=batch[:,1:].to(model.device), logits=logits, sequence_length=(seq_lengths-1).to(model.device))\n",
        "        kl_loss = kl_divergence(mean, logvar)\n",
        "        total_loss = rec_loss + kl_weight * kl_loss\n",
        "        if mode == \"train\":\n",
        "            pbar.update(1)\n",
        "            global_steps['step'] += 1\n",
        "            if config.pid_control:\n",
        "                kl_weight = pid.pid(exp_kl, kl_loss.item(), Kp, Ki)\n",
        "                opt_vars[\"kl_weight\"] = kl_weight\n",
        "            ## total loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        batch_size = len(batch)\n",
        "        num_words += torch.sum(seq_lengths).item()\n",
        "        nll_total += total_loss.item() * batch_size\n",
        "        avg_rec.add(\n",
        "            [total_loss.item(),\n",
        "              kl_loss.item(),\n",
        "              rec_loss.item()],\n",
        "            batch_size)\n",
        "            \n",
        "        if global_steps['step'] % display == 1 and mode == 'train':\n",
        "            nll = avg_rec.avg(0)\n",
        "            klw = opt_vars[\"kl_weight\"]\n",
        "            KL = avg_rec.avg(1)\n",
        "            rc = avg_rec.avg(2)\n",
        "            \n",
        "    nll = avg_rec.avg(0)\n",
        "    KL = avg_rec.avg(1)\n",
        "    rc = avg_rec.avg(2)\n",
        "    if num_words > 0:\n",
        "        log_ppl = nll_total / num_words\n",
        "        ppl = math.exp(log_ppl)\n",
        "    else:\n",
        "        log_ppl = 100\n",
        "        ppl = math.exp(log_ppl)\n",
        "        nll = 1000\n",
        "        KL = args.exp_kl\n",
        "    \n",
        "    print(f\"\\n{mode}: epoch {epoch}, nll {nll:.4f}, KL {KL:.4f}, \"\n",
        "          f\"rc {rc:.4f}, log_ppl {log_ppl:.4f}, ppl {ppl:.4f}\")\n",
        "    return nll, ppl  # type: ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxMAUE7rytC1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d485994-4153-4253-cf75-116a62346da9"
      },
      "source": [
        "# Counts trainable parameters\n",
        "total_parameters = sum(param.numel() for param in model.parameters())\n",
        "print(f\"{total_parameters} total parameters\")\n",
        "\n",
        "best_nll = best_ppl = 0.\n",
        "\n",
        "## start running model\n",
        "pbar = tqdm(total = int(max_iter))\n",
        "train_losses, val_losses = [], []\n",
        "beta = []\n",
        "for epoch in range(config.num_epochs):\n",
        "    train_nll, _ = _run_epoch(epoch, 'train', display=200)\n",
        "    val_nll, _ = _run_epoch(epoch, 'valid')\n",
        "    test_nll, test_ppl = _run_epoch(epoch, 'test')\n",
        "    train_losses.append(train_nll)\n",
        "    val_losses.append(val_nll)\n",
        "    beta.append(opt_vars[\"kl_weight\"])\n",
        "\n",
        "    if val_nll < opt_vars['best_valid_nll']:\n",
        "        opt_vars['best_valid_nll'] = val_nll\n",
        "        opt_vars['steps_not_improved'] = 0\n",
        "        best_nll = test_nll\n",
        "        best_ppl = test_ppl\n",
        "\n",
        "        states = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"scheduler\": scheduler.state_dict()\n",
        "        }\n",
        "        torch.save(states, save_path)\n",
        "    else:\n",
        "        opt_vars['steps_not_improved'] += 1\n",
        "        if opt_vars['steps_not_improved'] == decay_ts:\n",
        "            old_lr = opt_vars['learning_rate']\n",
        "            opt_vars['learning_rate'] *= decay_factor\n",
        "            opt_vars['steps_not_improved'] = 0\n",
        "            new_lr = opt_vars['learning_rate']\n",
        "            ckpt = torch.load(save_path)\n",
        "            model.load_state_dict(ckpt['model'])\n",
        "            optimizer.load_state_dict(ckpt['optimizer'])\n",
        "            scheduler.load_state_dict(ckpt['scheduler'])\n",
        "            scheduler.step()\n",
        "            print(f\"-----\\nchange lr, old lr: {old_lr}, \"\n",
        "                  f\"new lr: {new_lr}\\n-----\")\n",
        "\n",
        "            decay_cnt += 1\n",
        "            if decay_cnt == max_decay:\n",
        "                break\n",
        "    if global_steps['step'] >= max_iter:\n",
        "        break\n",
        "\n",
        "print(f\"\\nbest testing nll: {best_nll:.4f},\"\n",
        "      f\"best testing ppl {best_ppl:.4f}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/76562 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7259426 total parameters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 2/76562 [00:00<3:01:20,  7.04it/s]\u001b[A\n",
            "  0%|          | 3/76562 [00:00<3:21:26,  6.33it/s]\u001b[A\n",
            "  0%|          | 4/76562 [00:00<3:55:08,  5.43it/s]\u001b[A\n",
            "  0%|          | 5/76562 [00:00<3:35:39,  5.92it/s]\u001b[A\n",
            "  0%|          | 6/76562 [00:01<3:25:37,  6.21it/s]\u001b[A\n",
            "  0%|          | 7/76562 [00:01<3:38:08,  5.85it/s]\u001b[A\n",
            "  0%|          | 8/76562 [00:01<3:49:36,  5.56it/s]\u001b[A\n",
            "  0%|          | 9/76562 [00:01<3:42:29,  5.73it/s]\u001b[A\n",
            "  0%|          | 10/76562 [00:01<3:45:52,  5.65it/s]\u001b[A\n",
            "  0%|          | 11/76562 [00:01<4:05:19,  5.20it/s]\u001b[A\n",
            "  0%|          | 12/76562 [00:02<4:39:21,  4.57it/s]\u001b[A\n",
            "  0%|          | 13/76562 [00:02<5:36:56,  3.79it/s]\u001b[A\n",
            "  0%|          | 14/76562 [00:02<6:01:44,  3.53it/s]\u001b[A\n",
            "  0%|          | 15/76562 [00:03<5:28:54,  3.88it/s]\u001b[A\n",
            "  0%|          | 16/76562 [00:03<4:58:02,  4.28it/s]\u001b[A\n",
            "  0%|          | 17/76562 [00:03<4:43:45,  4.50it/s]\u001b[A\n",
            "  0%|          | 18/76562 [00:03<5:06:16,  4.17it/s]\u001b[A\n",
            "  0%|          | 19/76562 [00:04<4:52:23,  4.36it/s]\u001b[A\n",
            "  0%|          | 20/76562 [00:04<5:04:32,  4.19it/s]\u001b[A\n",
            "  0%|          | 21/76562 [00:04<4:58:52,  4.27it/s]\u001b[A\n",
            "  0%|          | 22/76562 [00:04<4:37:03,  4.60it/s]\u001b[A\n",
            "  0%|          | 23/76562 [00:04<4:41:09,  4.54it/s]\u001b[A\n",
            "  0%|          | 24/76562 [00:05<4:14:40,  5.01it/s]\u001b[A\n",
            "  0%|          | 25/76562 [00:05<4:16:54,  4.97it/s]\u001b[A\n",
            "  0%|          | 26/76562 [00:05<4:22:26,  4.86it/s]\u001b[A\n",
            "  0%|          | 27/76562 [00:05<4:19:09,  4.92it/s]\u001b[A\n",
            "  0%|          | 28/76562 [00:05<4:24:49,  4.82it/s]\u001b[A\n",
            "  0%|          | 29/76562 [00:06<4:21:08,  4.88it/s]\u001b[A\n",
            "  0%|          | 30/76562 [00:06<4:41:50,  4.53it/s]\u001b[A\n",
            "  0%|          | 31/76562 [00:06<4:36:56,  4.61it/s]\u001b[A\n",
            "  0%|          | 32/76562 [00:06<4:40:36,  4.55it/s]\u001b[A\n",
            "  0%|          | 33/76562 [00:07<5:18:57,  4.00it/s]\u001b[A\n",
            "  0%|          | 34/76562 [00:07<5:21:36,  3.97it/s]\u001b[A\n",
            "  0%|          | 35/76562 [00:07<5:27:45,  3.89it/s]\u001b[A\n",
            "  0%|          | 36/76562 [00:07<5:32:12,  3.84it/s]\u001b[A\n",
            "  0%|          | 37/76562 [00:08<5:25:34,  3.92it/s]\u001b[A\n",
            "  0%|          | 38/76562 [00:08<4:49:45,  4.40it/s]\u001b[A\n",
            "  0%|          | 39/76562 [00:08<4:30:12,  4.72it/s]\u001b[A\n",
            "  0%|          | 40/76562 [00:08<4:55:32,  4.32it/s]\u001b[A\n",
            "  0%|          | 41/76562 [00:08<4:39:36,  4.56it/s]\u001b[A\n",
            "  0%|          | 42/76562 [00:09<4:57:03,  4.29it/s]\u001b[A\n",
            "  0%|          | 43/76562 [00:09<4:52:35,  4.36it/s]\u001b[A\n",
            "  0%|          | 44/76562 [00:09<4:38:13,  4.58it/s]\u001b[A\n",
            "  0%|          | 45/76562 [00:09<4:37:20,  4.60it/s]\u001b[A\n",
            "  0%|          | 46/76562 [00:10<4:43:49,  4.49it/s]\u001b[A\n",
            "  0%|          | 47/76562 [00:10<4:45:56,  4.46it/s]\u001b[A\n",
            "  0%|          | 48/76562 [00:10<5:04:54,  4.18it/s]\u001b[A\n",
            "  0%|          | 49/76562 [00:10<4:55:57,  4.31it/s]\u001b[A\n",
            "  0%|          | 50/76562 [00:11<4:58:05,  4.28it/s]\u001b[A\n",
            "  0%|          | 51/76562 [00:11<4:54:47,  4.33it/s]\u001b[A\n",
            "  0%|          | 52/76562 [00:11<5:13:18,  4.07it/s]\u001b[A\n",
            "  0%|          | 53/76562 [00:11<5:44:46,  3.70it/s]\u001b[A\n",
            "  0%|          | 54/76562 [00:12<6:15:19,  3.40it/s]\u001b[A\n",
            "  0%|          | 55/76562 [00:12<6:19:49,  3.36it/s]\u001b[A\n",
            "  0%|          | 56/76562 [00:12<5:48:09,  3.66it/s]\u001b[A\n",
            "  0%|          | 57/76562 [00:13<6:01:43,  3.53it/s]\u001b[A\n",
            "  0%|          | 58/76562 [00:13<5:50:07,  3.64it/s]\u001b[A\n",
            "  0%|          | 59/76562 [00:13<5:26:07,  3.91it/s]\u001b[A\n",
            "  0%|          | 60/76562 [00:13<4:48:39,  4.42it/s]\u001b[A\n",
            "  0%|          | 61/76562 [00:13<4:13:27,  5.03it/s]\u001b[A\n",
            "  0%|          | 62/76562 [00:14<4:20:48,  4.89it/s]\u001b[A\n",
            "  0%|          | 63/76562 [00:14<4:23:58,  4.83it/s]\u001b[A\n",
            "  0%|          | 64/76562 [00:14<4:32:46,  4.67it/s]\u001b[A\n",
            "  0%|          | 65/76562 [00:14<4:44:37,  4.48it/s]\u001b[A\n",
            "  0%|          | 66/76562 [00:14<4:46:30,  4.45it/s]\u001b[A\n",
            "  0%|          | 67/76562 [00:15<4:34:19,  4.65it/s]\u001b[A\n",
            "  0%|          | 68/76562 [00:15<4:50:29,  4.39it/s]\u001b[A\n",
            "  0%|          | 69/76562 [00:15<4:34:35,  4.64it/s]\u001b[A\n",
            "  0%|          | 70/76562 [00:15<4:35:15,  4.63it/s]\u001b[A\n",
            "  0%|          | 71/76562 [00:16<5:15:32,  4.04it/s]\u001b[A\n",
            "  0%|          | 72/76562 [00:16<5:29:25,  3.87it/s]\u001b[A\n",
            "  0%|          | 73/76562 [00:16<5:20:28,  3.98it/s]\u001b[A\n",
            "  0%|          | 74/76562 [00:16<5:11:40,  4.09it/s]\u001b[A\n",
            "  0%|          | 75/76562 [00:17<4:48:09,  4.42it/s]\u001b[A\n",
            "  0%|          | 76/76562 [00:17<4:49:53,  4.40it/s]\u001b[A\n",
            "  0%|          | 77/76562 [00:17<5:18:07,  4.01it/s]\u001b[A\n",
            "  0%|          | 78/76562 [00:17<5:39:39,  3.75it/s]\u001b[A\n",
            "  0%|          | 79/76562 [00:18<5:17:09,  4.02it/s]\u001b[A\n",
            "  0%|          | 80/76562 [00:18<4:39:53,  4.55it/s]\u001b[A\n",
            "  0%|          | 81/76562 [00:18<4:36:07,  4.62it/s]\u001b[A\n",
            "  0%|          | 82/76562 [00:18<5:11:34,  4.09it/s]\u001b[A\n",
            "  0%|          | 83/76562 [00:19<5:41:13,  3.74it/s]\u001b[A\n",
            "  0%|          | 84/76562 [00:19<5:11:30,  4.09it/s]\u001b[A\n",
            "  0%|          | 85/76562 [00:19<4:37:30,  4.59it/s]\u001b[A\n",
            "  0%|          | 86/76562 [00:19<4:20:11,  4.90it/s]\u001b[A\n",
            "  0%|          | 87/76562 [00:19<4:40:25,  4.55it/s]\u001b[A\n",
            "  0%|          | 88/76562 [00:19<3:58:43,  5.34it/s]\u001b[A\n",
            "  0%|          | 89/76562 [00:20<3:53:31,  5.46it/s]\u001b[A\n",
            "  0%|          | 90/76562 [00:20<4:45:21,  4.47it/s]\u001b[A\n",
            "  0%|          | 91/76562 [00:20<4:38:49,  4.57it/s]\u001b[A\n",
            "  0%|          | 92/76562 [00:20<4:25:33,  4.80it/s]\u001b[A\n",
            "  0%|          | 93/76562 [00:21<4:14:10,  5.01it/s]\u001b[A\n",
            "  0%|          | 94/76562 [00:21<4:36:28,  4.61it/s]\u001b[A\n",
            "  0%|          | 95/76562 [00:21<4:58:39,  4.27it/s]\u001b[A\n",
            "  0%|          | 96/76562 [00:21<5:05:29,  4.17it/s]\u001b[A\n",
            "  0%|          | 97/76562 [00:22<5:43:26,  3.71it/s]\u001b[A\n",
            "  0%|          | 98/76562 [00:22<5:02:16,  4.22it/s]\u001b[A\n",
            "  0%|          | 99/76562 [00:22<4:35:38,  4.62it/s]\u001b[A\n",
            "  0%|          | 100/76562 [00:22<4:36:27,  4.61it/s]\u001b[A\n",
            "  0%|          | 101/76562 [00:22<4:30:34,  4.71it/s]\u001b[A\n",
            "  0%|          | 102/76562 [00:23<4:57:46,  4.28it/s]\u001b[A\n",
            "  0%|          | 103/76562 [00:23<5:15:57,  4.03it/s]\u001b[A\n",
            "  0%|          | 104/76562 [00:23<4:58:21,  4.27it/s]\u001b[A\n",
            "  0%|          | 105/76562 [00:23<4:42:19,  4.51it/s]\u001b[A\n",
            "  0%|          | 106/76562 [00:24<4:46:17,  4.45it/s]\u001b[A\n",
            "  0%|          | 107/76562 [00:24<4:24:17,  4.82it/s]\u001b[A\n",
            "  0%|          | 108/76562 [00:24<4:02:08,  5.26it/s]\u001b[A\n",
            "  0%|          | 109/76562 [00:24<4:14:11,  5.01it/s]\u001b[A\n",
            "  0%|          | 110/76562 [00:24<4:16:59,  4.96it/s]\u001b[A\n",
            "  0%|          | 111/76562 [00:25<4:50:18,  4.39it/s]\u001b[A\n",
            "  0%|          | 112/76562 [00:25<4:35:36,  4.62it/s]\u001b[A\n",
            "  0%|          | 113/76562 [00:25<4:27:36,  4.76it/s]\u001b[A\n",
            "  0%|          | 114/76562 [00:25<4:21:02,  4.88it/s]\u001b[A\n",
            "  0%|          | 115/76562 [00:25<4:06:20,  5.17it/s]\u001b[A\n",
            "  0%|          | 116/76562 [00:26<4:32:57,  4.67it/s]\u001b[A\n",
            "  0%|          | 117/76562 [00:26<5:00:47,  4.24it/s]\u001b[A\n",
            "  0%|          | 118/76562 [00:26<5:12:49,  4.07it/s]\u001b[A\n",
            "  0%|          | 119/76562 [00:26<5:14:01,  4.06it/s]\u001b[A\n",
            "  0%|          | 120/76562 [00:27<5:27:28,  3.89it/s]\u001b[A\n",
            "  0%|          | 121/76562 [00:27<4:50:11,  4.39it/s]\u001b[A\n",
            "  0%|          | 122/76562 [00:27<4:26:50,  4.77it/s]\u001b[A\n",
            "  0%|          | 123/76562 [00:27<4:33:20,  4.66it/s]\u001b[A\n",
            "  0%|          | 124/76562 [00:28<4:55:31,  4.31it/s]\u001b[A\n",
            "  0%|          | 125/76562 [00:28<5:13:02,  4.07it/s]\u001b[A\n",
            "  0%|          | 126/76562 [00:28<5:19:29,  3.99it/s]\u001b[A\n",
            "  0%|          | 127/76562 [00:28<5:23:37,  3.94it/s]\u001b[A\n",
            "  0%|          | 128/76562 [00:29<5:06:22,  4.16it/s]\u001b[A\n",
            "  0%|          | 129/76562 [00:29<5:15:01,  4.04it/s]\u001b[A\n",
            "  0%|          | 130/76562 [00:29<4:49:02,  4.41it/s]\u001b[A\n",
            "  0%|          | 131/76562 [00:29<4:37:18,  4.59it/s]\u001b[A\n",
            "  0%|          | 132/76562 [00:29<4:27:54,  4.75it/s]\u001b[A\n",
            "  0%|          | 133/76562 [00:30<4:49:02,  4.41it/s]\u001b[A\n",
            "  0%|          | 134/76562 [00:30<5:11:13,  4.09it/s]\u001b[A\n",
            "  0%|          | 135/76562 [00:30<5:17:13,  4.02it/s]\u001b[A\n",
            "  0%|          | 136/76562 [00:30<5:09:52,  4.11it/s]\u001b[A\n",
            "  0%|          | 137/76562 [00:31<5:51:22,  3.63it/s]\u001b[A\n",
            "  0%|          | 138/76562 [00:31<5:40:10,  3.74it/s]\u001b[A\n",
            "  0%|          | 139/76562 [00:31<6:05:30,  3.48it/s]\u001b[A\n",
            "  0%|          | 140/76562 [00:32<5:40:30,  3.74it/s]\u001b[A\n",
            "  0%|          | 141/76562 [00:32<4:51:43,  4.37it/s]\u001b[A\n",
            "  0%|          | 142/76562 [00:32<4:31:11,  4.70it/s]\u001b[A\n",
            "  0%|          | 143/76562 [00:32<4:23:15,  4.84it/s]\u001b[A\n",
            "  0%|          | 144/76562 [00:32<4:30:18,  4.71it/s]\u001b[A\n",
            "  0%|          | 145/76562 [00:33<5:06:55,  4.15it/s]\u001b[A\n",
            "  0%|          | 146/76562 [00:33<5:01:07,  4.23it/s]\u001b[A\n",
            "  0%|          | 147/76562 [00:33<4:51:07,  4.37it/s]\u001b[A\n",
            "  0%|          | 148/76562 [00:33<4:37:12,  4.59it/s]\u001b[A\n",
            "  0%|          | 149/76562 [00:34<4:54:45,  4.32it/s]\u001b[A\n",
            "  0%|          | 150/76562 [00:34<5:20:14,  3.98it/s]\u001b[A\n",
            "  0%|          | 151/76562 [00:34<5:26:04,  3.91it/s]\u001b[A\n",
            "  0%|          | 152/76562 [00:34<5:05:50,  4.16it/s]\u001b[A\n",
            "  0%|          | 153/76562 [00:34<4:47:24,  4.43it/s]\u001b[A\n",
            "  0%|          | 154/76562 [00:35<4:32:57,  4.67it/s]\u001b[A\n",
            "  0%|          | 155/76562 [00:35<4:19:10,  4.91it/s]\u001b[A\n",
            "  0%|          | 156/76562 [00:35<4:09:17,  5.11it/s]\u001b[A\n",
            "  0%|          | 157/76562 [00:35<3:52:51,  5.47it/s]\u001b[A\n",
            "  0%|          | 158/76562 [00:35<4:06:53,  5.16it/s]\u001b[A\n",
            "  0%|          | 159/76562 [00:36<4:43:18,  4.49it/s]\u001b[A\n",
            "  0%|          | 160/76562 [00:36<5:07:29,  4.14it/s]\u001b[A\n",
            "  0%|          | 161/76562 [00:36<4:58:20,  4.27it/s]\u001b[A\n",
            "  0%|          | 162/76562 [00:36<4:53:21,  4.34it/s]\u001b[A\n",
            "  0%|          | 163/76562 [00:37<4:58:18,  4.27it/s]\u001b[A\n",
            "  0%|          | 164/76562 [00:37<4:45:35,  4.46it/s]\u001b[A\n",
            "  0%|          | 165/76562 [00:37<4:32:34,  4.67it/s]\u001b[A\n",
            "  0%|          | 166/76562 [00:37<4:29:06,  4.73it/s]\u001b[A\n",
            "  0%|          | 167/76562 [00:37<4:15:10,  4.99it/s]\u001b[A\n",
            "  0%|          | 168/76562 [00:38<4:02:57,  5.24it/s]\u001b[A\n",
            "  0%|          | 169/76562 [00:38<4:04:38,  5.20it/s]\u001b[A\n",
            "  0%|          | 170/76562 [00:38<4:22:51,  4.84it/s]\u001b[A\n",
            "  0%|          | 171/76562 [00:38<4:24:55,  4.81it/s]\u001b[A\n",
            "  0%|          | 172/76562 [00:38<4:49:13,  4.40it/s]\u001b[A\n",
            "  0%|          | 173/76562 [00:39<4:29:16,  4.73it/s]\u001b[A\n",
            "  0%|          | 174/76562 [00:39<4:30:48,  4.70it/s]\u001b[A\n",
            "  0%|          | 175/76562 [00:39<4:38:17,  4.57it/s]\u001b[A\n",
            "  0%|          | 176/76562 [00:39<4:38:44,  4.57it/s]\u001b[A\n",
            "  0%|          | 177/76562 [00:40<4:18:40,  4.92it/s]\u001b[A\n",
            "  0%|          | 178/76562 [00:40<4:14:09,  5.01it/s]\u001b[A\n",
            "  0%|          | 179/76562 [00:40<4:36:17,  4.61it/s]\u001b[A\n",
            "  0%|          | 180/76562 [00:40<4:24:12,  4.82it/s]\u001b[A\n",
            "  0%|          | 181/76562 [00:40<4:25:01,  4.80it/s]\u001b[A\n",
            "  0%|          | 182/76562 [00:41<4:35:43,  4.62it/s]\u001b[A\n",
            "  0%|          | 183/76562 [00:41<4:28:44,  4.74it/s]\u001b[A\n",
            "  0%|          | 184/76562 [00:41<4:33:54,  4.65it/s]\u001b[A\n",
            "  0%|          | 185/76562 [00:41<4:29:47,  4.72it/s]\u001b[A\n",
            "  0%|          | 186/76562 [00:41<4:22:52,  4.84it/s]\u001b[A\n",
            "  0%|          | 187/76562 [00:42<4:39:06,  4.56it/s]\u001b[A\n",
            "  0%|          | 188/76562 [00:42<5:42:06,  3.72it/s]\u001b[A\n",
            "  0%|          | 189/76562 [00:42<6:07:32,  3.46it/s]\u001b[A\n",
            "  0%|          | 190/76562 [00:43<5:27:52,  3.88it/s]\u001b[A\n",
            "  0%|          | 191/76562 [00:43<4:56:06,  4.30it/s]\u001b[A\n",
            "  0%|          | 192/76562 [00:43<4:27:14,  4.76it/s]\u001b[A\n",
            "  0%|          | 193/76562 [00:43<4:03:31,  5.23it/s]\u001b[A\n",
            "  0%|          | 194/76562 [00:43<4:08:50,  5.11it/s]\u001b[A\n",
            "  0%|          | 195/76562 [00:43<4:05:39,  5.18it/s]\u001b[A\n",
            "  0%|          | 196/76562 [00:44<3:58:22,  5.34it/s]\u001b[A\n",
            "  0%|          | 197/76562 [00:44<3:48:51,  5.56it/s]\u001b[A\n",
            "  0%|          | 198/76562 [00:44<3:47:09,  5.60it/s]\u001b[A\n",
            "  0%|          | 199/76562 [00:44<3:56:20,  5.39it/s]\u001b[A\n",
            "  0%|          | 200/76562 [00:44<4:01:15,  5.28it/s]\u001b[A\n",
            "  0%|          | 201/76562 [00:45<4:27:47,  4.75it/s]\u001b[A\n",
            "  0%|          | 202/76562 [00:45<4:27:21,  4.76it/s]\u001b[A\n",
            "  0%|          | 203/76562 [00:45<4:18:53,  4.92it/s]\u001b[A\n",
            "  0%|          | 204/76562 [00:45<4:15:33,  4.98it/s]\u001b[A\n",
            "  0%|          | 205/76562 [00:45<4:07:10,  5.15it/s]\u001b[A\n",
            "  0%|          | 206/76562 [00:46<4:25:32,  4.79it/s]\u001b[A\n",
            "  0%|          | 207/76562 [00:46<4:32:45,  4.67it/s]\u001b[A\n",
            "  0%|          | 208/76562 [00:46<4:58:40,  4.26it/s]\u001b[A\n",
            "  0%|          | 209/76562 [00:46<4:33:53,  4.65it/s]\u001b[A\n",
            "  0%|          | 210/76562 [00:47<4:44:43,  4.47it/s]\u001b[A\n",
            "  0%|          | 211/76562 [00:47<4:50:37,  4.38it/s]\u001b[A\n",
            "  0%|          | 212/76562 [00:47<4:37:24,  4.59it/s]\u001b[A\n",
            "  0%|          | 213/76562 [00:47<4:04:54,  5.20it/s]\u001b[A\n",
            "  0%|          | 214/76562 [00:47<4:19:13,  4.91it/s]\u001b[A\n",
            "  0%|          | 215/76562 [00:48<4:11:07,  5.07it/s]\u001b[A\n",
            "  0%|          | 216/76562 [00:48<4:11:50,  5.05it/s]\u001b[A\n",
            "  0%|          | 217/76562 [00:48<4:16:31,  4.96it/s]\u001b[A\n",
            "  0%|          | 218/76562 [00:48<4:15:17,  4.98it/s]\u001b[A\n",
            "  0%|          | 219/76562 [00:48<4:09:19,  5.10it/s]\u001b[A\n",
            "  0%|          | 220/76562 [00:48<4:01:33,  5.27it/s]\u001b[A\n",
            "  0%|          | 221/76562 [00:49<4:27:21,  4.76it/s]\u001b[A\n",
            "  0%|          | 222/76562 [00:49<5:07:17,  4.14it/s]\u001b[A\n",
            "  0%|          | 223/76562 [00:49<4:50:17,  4.38it/s]\u001b[A\n",
            "  0%|          | 224/76562 [00:50<5:12:08,  4.08it/s]\u001b[A\n",
            "  0%|          | 225/76562 [00:50<5:47:00,  3.67it/s]\u001b[A\n",
            "  0%|          | 226/76562 [00:50<5:24:08,  3.93it/s]\u001b[A\n",
            "  0%|          | 227/76562 [00:50<4:57:17,  4.28it/s]\u001b[A\n",
            "  0%|          | 228/76562 [00:50<4:49:01,  4.40it/s]\u001b[A\n",
            "  0%|          | 229/76562 [00:51<5:00:27,  4.23it/s]\u001b[A\n",
            "  0%|          | 230/76562 [00:51<4:45:01,  4.46it/s]\u001b[A\n",
            "  0%|          | 231/76562 [00:51<4:29:10,  4.73it/s]\u001b[A\n",
            "  0%|          | 232/76562 [00:51<4:25:51,  4.79it/s]\u001b[A\n",
            "  0%|          | 233/76562 [00:52<4:37:01,  4.59it/s]\u001b[A\n",
            "  0%|          | 234/76562 [00:52<4:38:49,  4.56it/s]\u001b[A\n",
            "  0%|          | 235/76562 [00:52<4:50:48,  4.37it/s]\u001b[A\n",
            "  0%|          | 236/76562 [00:52<5:20:34,  3.97it/s]\u001b[A\n",
            "  0%|          | 237/76562 [00:53<5:21:08,  3.96it/s]\u001b[A\n",
            "  0%|          | 238/76562 [00:53<5:26:36,  3.89it/s]\u001b[A\n",
            "  0%|          | 239/76562 [00:53<4:58:49,  4.26it/s]\u001b[A\n",
            "  0%|          | 240/76562 [00:53<4:28:13,  4.74it/s]\u001b[A\n",
            "  0%|          | 241/76562 [00:53<4:34:28,  4.63it/s]\u001b[A\n",
            "  0%|          | 242/76562 [00:54<4:54:55,  4.31it/s]\u001b[A\n",
            "  0%|          | 243/76562 [00:54<4:51:56,  4.36it/s]\u001b[A\n",
            "  0%|          | 244/76562 [00:54<5:06:08,  4.15it/s]\u001b[A\n",
            "  0%|          | 245/76562 [00:54<4:35:26,  4.62it/s]\u001b[A\n",
            "  0%|          | 246/76562 [00:55<4:18:44,  4.92it/s]\u001b[A\n",
            "  0%|          | 247/76562 [00:55<4:19:38,  4.90it/s]\u001b[A\n",
            "  0%|          | 248/76562 [00:55<5:07:49,  4.13it/s]\u001b[A\n",
            "  0%|          | 249/76562 [00:55<5:12:23,  4.07it/s]\u001b[A\n",
            "  0%|          | 250/76562 [00:56<5:03:22,  4.19it/s]\u001b[A\n",
            "  0%|          | 251/76562 [00:56<4:45:41,  4.45it/s]\u001b[A\n",
            "  0%|          | 252/76562 [00:56<4:35:21,  4.62it/s]\u001b[A\n",
            "  0%|          | 253/76562 [00:56<4:52:13,  4.35it/s]\u001b[A\n",
            "  0%|          | 254/76562 [00:56<4:49:52,  4.39it/s]\u001b[A\n",
            "  0%|          | 255/76562 [00:57<4:48:39,  4.41it/s]\u001b[A\n",
            "  0%|          | 256/76562 [00:57<4:56:40,  4.29it/s]\u001b[A\n",
            "  0%|          | 257/76562 [00:57<5:03:17,  4.19it/s]\u001b[A\n",
            "  0%|          | 258/76562 [00:57<4:47:54,  4.42it/s]\u001b[A\n",
            "  0%|          | 259/76562 [00:58<5:30:32,  3.85it/s]\u001b[A\n",
            "  0%|          | 260/76562 [00:58<5:17:09,  4.01it/s]\u001b[A\n",
            "  0%|          | 261/76562 [00:58<5:19:14,  3.98it/s]\u001b[A\n",
            "  0%|          | 262/76562 [00:58<5:33:01,  3.82it/s]\u001b[A\n",
            "  0%|          | 263/76562 [00:59<4:51:05,  4.37it/s]\u001b[A\n",
            "  0%|          | 264/76562 [00:59<4:37:54,  4.58it/s]\u001b[A\n",
            "  0%|          | 265/76562 [00:59<4:15:50,  4.97it/s]\u001b[A\n",
            "  0%|          | 266/76562 [00:59<4:00:06,  5.30it/s]\u001b[A\n",
            "  0%|          | 267/76562 [00:59<3:52:39,  5.47it/s]\u001b[A\n",
            "  0%|          | 268/76562 [00:59<3:58:42,  5.33it/s]\u001b[A\n",
            "  0%|          | 269/76562 [01:00<4:03:31,  5.22it/s]\u001b[A\n",
            "  0%|          | 270/76562 [01:00<4:18:51,  4.91it/s]\u001b[A\n",
            "  0%|          | 271/76562 [01:00<4:56:44,  4.28it/s]\u001b[A\n",
            "  0%|          | 272/76562 [01:00<4:59:31,  4.25it/s]\u001b[A\n",
            "  0%|          | 273/76562 [01:01<5:05:39,  4.16it/s]\u001b[A\n",
            "  0%|          | 274/76562 [01:01<4:48:51,  4.40it/s]\u001b[A\n",
            "  0%|          | 275/76562 [01:01<4:40:12,  4.54it/s]\u001b[A\n",
            "  0%|          | 276/76562 [01:01<4:39:24,  4.55it/s]\u001b[A\n",
            "  0%|          | 277/76562 [01:02<5:20:03,  3.97it/s]\u001b[A\n",
            "  0%|          | 278/76562 [01:02<5:24:49,  3.91it/s]\u001b[A\n",
            "  0%|          | 279/76562 [01:02<5:11:14,  4.08it/s]\u001b[A\n",
            "  0%|          | 280/76562 [01:02<5:18:52,  3.99it/s]\u001b[A\n",
            "  0%|          | 281/76562 [01:03<4:47:21,  4.42it/s]\u001b[A\n",
            "  0%|          | 282/76562 [01:03<4:42:31,  4.50it/s]\u001b[A\n",
            "  0%|          | 283/76562 [01:03<4:50:26,  4.38it/s]\u001b[A\n",
            "  0%|          | 284/76562 [01:03<4:44:11,  4.47it/s]\u001b[A\n",
            "  0%|          | 285/76562 [01:03<4:39:16,  4.55it/s]\u001b[A\n",
            "  0%|          | 286/76562 [01:04<4:23:55,  4.82it/s]\u001b[A\n",
            "  0%|          | 287/76562 [01:04<4:35:02,  4.62it/s]\u001b[A\n",
            "  0%|          | 288/76562 [01:04<4:20:00,  4.89it/s]\u001b[A\n",
            "  0%|          | 289/76562 [01:04<4:17:02,  4.95it/s]\u001b[A\n",
            "  0%|          | 290/76562 [01:04<4:13:36,  5.01it/s]\u001b[A\n",
            "  0%|          | 291/76562 [01:05<4:32:21,  4.67it/s]\u001b[A\n",
            "  0%|          | 292/76562 [01:05<4:32:19,  4.67it/s]\u001b[A\n",
            "  0%|          | 293/76562 [01:05<4:11:42,  5.05it/s]\u001b[A\n",
            "  0%|          | 294/76562 [01:05<4:19:18,  4.90it/s]\u001b[A\n",
            "  0%|          | 295/76562 [01:06<4:36:33,  4.60it/s]\u001b[A\n",
            "  0%|          | 296/76562 [01:06<4:39:19,  4.55it/s]\u001b[A\n",
            "  0%|          | 297/76562 [01:06<4:19:17,  4.90it/s]\u001b[A\n",
            "  0%|          | 298/76562 [01:06<4:34:39,  4.63it/s]\u001b[A\n",
            "  0%|          | 299/76562 [01:06<4:35:31,  4.61it/s]\u001b[A\n",
            "  0%|          | 300/76562 [01:07<4:58:03,  4.26it/s]\u001b[A\n",
            "  0%|          | 301/76562 [01:07<4:39:21,  4.55it/s]\u001b[A\n",
            "  0%|          | 302/76562 [01:07<4:40:06,  4.54it/s]\u001b[A\n",
            "  0%|          | 303/76562 [01:07<5:03:29,  4.19it/s]\u001b[A\n",
            "  0%|          | 304/76562 [01:08<5:04:02,  4.18it/s]\u001b[A\n",
            "  0%|          | 305/76562 [01:08<5:09:35,  4.11it/s]\u001b[A\n",
            "  0%|          | 306/76562 [01:08<4:57:43,  4.27it/s]\u001b[A\n",
            "  0%|          | 307/76562 [01:08<4:59:26,  4.24it/s]\u001b[A\n",
            "  0%|          | 308/76562 [01:09<5:38:49,  3.75it/s]\u001b[A\n",
            "  0%|          | 309/76562 [01:09<5:06:10,  4.15it/s]\u001b[A\n",
            "  0%|          | 310/76562 [01:09<4:44:21,  4.47it/s]\u001b[A\n",
            "  0%|          | 311/76562 [01:09<4:36:10,  4.60it/s]\u001b[A\n",
            "  0%|          | 312/76562 [01:09<4:24:07,  4.81it/s]\u001b[A\n",
            "  0%|          | 313/76562 [01:10<4:16:27,  4.96it/s]\u001b[A\n",
            "  0%|          | 314/76562 [01:10<4:16:34,  4.95it/s]\u001b[A\n",
            "  0%|          | 315/76562 [01:10<4:07:39,  5.13it/s]\u001b[A\n",
            "  0%|          | 316/76562 [01:10<4:15:21,  4.98it/s]\u001b[A\n",
            "  0%|          | 317/76562 [01:10<3:52:57,  5.45it/s]\u001b[A\n",
            "  0%|          | 318/76562 [01:10<3:49:14,  5.54it/s]\u001b[A\n",
            "  0%|          | 319/76562 [01:11<3:35:19,  5.90it/s]\u001b[A\n",
            "  0%|          | 320/76562 [01:11<3:43:00,  5.70it/s]\u001b[A\n",
            "  0%|          | 321/76562 [01:11<3:53:07,  5.45it/s]\u001b[A\n",
            "  0%|          | 322/76562 [01:11<3:43:04,  5.70it/s]\u001b[A\n",
            "  0%|          | 323/76562 [01:11<3:57:20,  5.35it/s]\u001b[A\n",
            "  0%|          | 324/76562 [01:12<4:45:09,  4.46it/s]\u001b[A\n",
            "  0%|          | 325/76562 [01:12<4:51:34,  4.36it/s]\u001b[A\n",
            "  0%|          | 326/76562 [01:12<4:54:35,  4.31it/s]\u001b[A\n",
            "  0%|          | 327/76562 [01:12<5:02:06,  4.21it/s]\u001b[A\n",
            "  0%|          | 328/76562 [01:13<4:32:33,  4.66it/s]\u001b[A\n",
            "  0%|          | 329/76562 [01:13<4:08:48,  5.11it/s]\u001b[A\n",
            "  0%|          | 330/76562 [01:13<4:05:45,  5.17it/s]\u001b[A\n",
            "  0%|          | 331/76562 [01:13<3:45:37,  5.63it/s]\u001b[A\n",
            "  0%|          | 332/76562 [01:13<3:38:43,  5.81it/s]\u001b[A\n",
            "  0%|          | 333/76562 [01:13<3:35:56,  5.88it/s]\u001b[A\n",
            "  0%|          | 334/76562 [01:14<3:39:47,  5.78it/s]\u001b[A\n",
            "  0%|          | 335/76562 [01:14<4:25:26,  4.79it/s]\u001b[A\n",
            "  0%|          | 336/76562 [01:14<4:57:36,  4.27it/s]\u001b[A\n",
            "  0%|          | 337/76562 [01:14<4:56:07,  4.29it/s]\u001b[A\n",
            "  0%|          | 338/76562 [01:15<4:48:18,  4.41it/s]\u001b[A\n",
            "  0%|          | 339/76562 [01:15<4:36:14,  4.60it/s]\u001b[A\n",
            "  0%|          | 340/76562 [01:15<4:24:46,  4.80it/s]\u001b[A\n",
            "  0%|          | 341/76562 [01:15<4:10:09,  5.08it/s]\u001b[A\n",
            "  0%|          | 342/76562 [01:15<4:16:19,  4.96it/s]\u001b[A\n",
            "  0%|          | 343/76562 [01:16<4:26:20,  4.77it/s]\u001b[A\n",
            "  0%|          | 344/76562 [01:16<5:07:38,  4.13it/s]\u001b[A\n",
            "  0%|          | 345/76562 [01:16<4:39:31,  4.54it/s]\u001b[A\n",
            "  0%|          | 346/76562 [01:16<4:19:59,  4.89it/s]\u001b[A\n",
            "  0%|          | 347/76562 [01:16<4:26:32,  4.77it/s]\u001b[A\n",
            "  0%|          | 348/76562 [01:17<4:11:29,  5.05it/s]\u001b[A\n",
            "  0%|          | 349/76562 [01:17<4:15:45,  4.97it/s]\u001b[A\n",
            "  0%|          | 350/76562 [01:17<4:43:21,  4.48it/s]\u001b[A\n",
            "  0%|          | 351/76562 [01:17<5:11:54,  4.07it/s]\u001b[A\n",
            "  0%|          | 352/76562 [01:18<4:58:43,  4.25it/s]\u001b[A\n",
            "  0%|          | 353/76562 [01:18<4:58:52,  4.25it/s]\u001b[A\n",
            "  0%|          | 354/76562 [01:18<5:15:47,  4.02it/s]\u001b[A\n",
            "  0%|          | 355/76562 [01:18<5:01:15,  4.22it/s]\u001b[A\n",
            "  0%|          | 356/76562 [01:19<5:16:49,  4.01it/s]\u001b[A\n",
            "  0%|          | 357/76562 [01:19<4:51:28,  4.36it/s]\u001b[A\n",
            "  0%|          | 358/76562 [01:19<4:49:55,  4.38it/s]\u001b[A\n",
            "  0%|          | 359/76562 [01:19<4:36:32,  4.59it/s]\u001b[A\n",
            "  0%|          | 360/76562 [01:19<4:11:54,  5.04it/s]\u001b[A\n",
            "  0%|          | 361/76562 [01:20<4:10:42,  5.07it/s]\u001b[A\n",
            "  0%|          | 362/76562 [01:20<4:39:22,  4.55it/s]\u001b[A\n",
            "  0%|          | 363/76562 [01:20<5:24:47,  3.91it/s]\u001b[A\n",
            "  0%|          | 364/76562 [01:20<5:16:47,  4.01it/s]\u001b[A\n",
            "  0%|          | 365/76562 [01:21<5:43:20,  3.70it/s]\u001b[A\n",
            "  0%|          | 366/76562 [01:21<5:35:42,  3.78it/s]\u001b[A\n",
            "  0%|          | 367/76562 [01:21<5:15:31,  4.02it/s]\u001b[A\n",
            "  0%|          | 368/76562 [01:21<5:18:19,  3.99it/s]\u001b[A\n",
            "  0%|          | 369/76562 [01:22<5:43:01,  3.70it/s]\u001b[A\n",
            "  0%|          | 370/76562 [01:22<5:28:22,  3.87it/s]\u001b[A\n",
            "  0%|          | 371/76562 [01:22<5:48:17,  3.65it/s]\u001b[A\n",
            "  0%|          | 372/76562 [01:23<5:15:52,  4.02it/s]\u001b[A\n",
            "  0%|          | 373/76562 [01:23<5:10:59,  4.08it/s]\u001b[A\n",
            "  0%|          | 374/76562 [01:23<5:25:16,  3.90it/s]\u001b[A\n",
            "  0%|          | 375/76562 [01:23<5:29:33,  3.85it/s]\u001b[A\n",
            "  0%|          | 376/76562 [01:23<5:05:51,  4.15it/s]\u001b[A\n",
            "  0%|          | 377/76562 [01:24<4:43:22,  4.48it/s]\u001b[A\n",
            "  0%|          | 378/76562 [01:24<5:01:02,  4.22it/s]\u001b[A\n",
            "  0%|          | 379/76562 [01:24<4:38:30,  4.56it/s]\u001b[A\n",
            "  0%|          | 380/76562 [01:24<4:22:50,  4.83it/s]\u001b[A\n",
            "  0%|          | 381/76562 [01:25<4:43:15,  4.48it/s]\u001b[A\n",
            "  0%|          | 382/76562 [01:25<4:50:07,  4.38it/s]\u001b[A\n",
            "  1%|          | 383/76562 [01:25<5:08:44,  4.11it/s]\u001b[A\n",
            "  1%|          | 384/76562 [01:25<4:34:07,  4.63it/s]\u001b[A\n",
            "  1%|          | 385/76562 [01:25<4:18:01,  4.92it/s]\u001b[A\n",
            "  1%|          | 386/76562 [01:26<4:20:41,  4.87it/s]\u001b[A\n",
            "  1%|          | 387/76562 [01:26<4:04:42,  5.19it/s]\u001b[A\n",
            "  1%|          | 388/76562 [01:26<4:35:49,  4.60it/s]\u001b[A\n",
            "  1%|          | 389/76562 [01:26<5:05:55,  4.15it/s]\u001b[A\n",
            "  1%|          | 390/76562 [01:27<4:51:16,  4.36it/s]\u001b[A\n",
            "  1%|          | 391/76562 [01:27<4:26:15,  4.77it/s]\u001b[A\n",
            "  1%|          | 392/76562 [01:27<4:15:17,  4.97it/s]\u001b[A\n",
            "  1%|          | 393/76562 [01:27<4:14:18,  4.99it/s]\u001b[A\n",
            "  1%|          | 394/76562 [01:27<4:31:27,  4.68it/s]\u001b[A\n",
            "  1%|          | 395/76562 [01:28<4:24:34,  4.80it/s]\u001b[A\n",
            "  1%|          | 396/76562 [01:28<4:13:52,  5.00it/s]\u001b[A\n",
            "  1%|          | 397/76562 [01:28<4:43:50,  4.47it/s]\u001b[A\n",
            "  1%|          | 398/76562 [01:28<4:38:44,  4.55it/s]\u001b[A\n",
            "  1%|          | 399/76562 [01:28<4:21:43,  4.85it/s]\u001b[A\n",
            "  1%|          | 400/76562 [01:29<4:12:59,  5.02it/s]\u001b[A\n",
            "  1%|          | 401/76562 [01:29<4:37:46,  4.57it/s]\u001b[A\n",
            "  1%|          | 402/76562 [01:29<4:47:38,  4.41it/s]\u001b[A\n",
            "  1%|          | 403/76562 [01:29<4:34:06,  4.63it/s]\u001b[A\n",
            "  1%|          | 404/76562 [01:29<4:06:22,  5.15it/s]\u001b[A\n",
            "  1%|          | 405/76562 [01:30<3:50:35,  5.50it/s]\u001b[A\n",
            "  1%|          | 406/76562 [01:30<3:38:11,  5.82it/s]\u001b[A\n",
            "  1%|          | 407/76562 [01:30<3:56:12,  5.37it/s]\u001b[A\n",
            "  1%|          | 408/76562 [01:30<3:54:01,  5.42it/s]\u001b[A\n",
            "  1%|          | 409/76562 [01:30<3:51:44,  5.48it/s]\u001b[A\n",
            "  1%|          | 410/76562 [01:30<3:47:18,  5.58it/s]\u001b[A\n",
            "  1%|          | 411/76562 [01:31<3:51:43,  5.48it/s]\u001b[A\n",
            "  1%|          | 412/76562 [01:31<3:51:02,  5.49it/s]\u001b[A\n",
            "  1%|          | 413/76562 [01:31<4:01:37,  5.25it/s]\u001b[A\n",
            "  1%|          | 414/76562 [01:31<4:11:23,  5.05it/s]\u001b[A\n",
            "  1%|          | 415/76562 [01:31<4:11:34,  5.04it/s]\u001b[A\n",
            "  1%|          | 416/76562 [01:32<4:40:53,  4.52it/s]\u001b[A\n",
            "  1%|          | 417/76562 [01:32<4:56:10,  4.28it/s]\u001b[A\n",
            "  1%|          | 418/76562 [01:32<5:00:58,  4.22it/s]\u001b[A\n",
            "  1%|          | 419/76562 [01:32<5:08:15,  4.12it/s]\u001b[A\n",
            "  1%|          | 420/76562 [01:33<5:10:44,  4.08it/s]\u001b[A\n",
            "  1%|          | 421/76562 [01:33<5:06:19,  4.14it/s]\u001b[A\n",
            "  1%|          | 422/76562 [01:33<5:12:30,  4.06it/s]\u001b[A\n",
            "  1%|          | 423/76562 [01:34<6:02:02,  3.51it/s]\u001b[A\n",
            "  1%|          | 424/76562 [01:34<6:48:05,  3.11it/s]\u001b[A\n",
            "  1%|          | 425/76562 [01:34<6:22:12,  3.32it/s]\u001b[A\n",
            "  1%|          | 426/76562 [01:35<6:38:17,  3.19it/s]\u001b[A\n",
            "  1%|          | 427/76562 [01:35<5:53:31,  3.59it/s]\u001b[A\n",
            "  1%|          | 428/76562 [01:35<5:26:19,  3.89it/s]\u001b[A\n",
            "  1%|          | 429/76562 [01:35<5:00:27,  4.22it/s]\u001b[A\n",
            "  1%|          | 430/76562 [01:35<4:26:48,  4.76it/s]\u001b[A\n",
            "  1%|          | 431/76562 [01:36<4:45:37,  4.44it/s]\u001b[A\n",
            "  1%|          | 432/76562 [01:36<5:37:11,  3.76it/s]\u001b[A\n",
            "  1%|          | 433/76562 [01:36<6:12:51,  3.40it/s]\u001b[A\n",
            "  1%|          | 434/76562 [01:37<6:04:26,  3.48it/s]\u001b[A\n",
            "  1%|          | 435/76562 [01:37<5:40:31,  3.73it/s]\u001b[A\n",
            "  1%|          | 436/76562 [01:37<4:58:52,  4.25it/s]\u001b[A\n",
            "  1%|          | 437/76562 [01:37<4:35:04,  4.61it/s]\u001b[A\n",
            "  1%|          | 438/76562 [01:37<4:10:58,  5.06it/s]\u001b[A\n",
            "  1%|          | 439/76562 [01:37<4:02:44,  5.23it/s]\u001b[A\n",
            "  1%|          | 440/76562 [01:38<4:08:50,  5.10it/s]\u001b[A\n",
            "  1%|          | 441/76562 [01:38<3:48:02,  5.56it/s]\u001b[A\n",
            "  1%|          | 442/76562 [01:38<3:44:53,  5.64it/s]\u001b[A\n",
            "  1%|          | 443/76562 [01:38<4:16:00,  4.96it/s]\u001b[A\n",
            "  1%|          | 444/76562 [01:39<5:01:26,  4.21it/s]\u001b[A\n",
            "  1%|          | 445/76562 [01:39<4:31:07,  4.68it/s]\u001b[A\n",
            "  1%|          | 446/76562 [01:39<4:51:17,  4.36it/s]\u001b[A\n",
            "  1%|          | 447/76562 [01:39<4:54:27,  4.31it/s]\u001b[A\n",
            "  1%|          | 448/76562 [01:39<4:51:10,  4.36it/s]\u001b[A\n",
            "  1%|          | 449/76562 [01:40<5:21:27,  3.95it/s]\u001b[A\n",
            "  1%|          | 450/76562 [01:40<4:54:05,  4.31it/s]\u001b[A\n",
            "  1%|          | 451/76562 [01:40<5:04:32,  4.17it/s]\u001b[A\n",
            "  1%|          | 452/76562 [01:41<5:19:06,  3.98it/s]\u001b[A\n",
            "  1%|          | 453/76562 [01:41<5:47:44,  3.65it/s]\u001b[A\n",
            "  1%|          | 454/76562 [01:41<5:20:38,  3.96it/s]\u001b[A\n",
            "  1%|          | 455/76562 [01:41<5:07:36,  4.12it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-82a31ef604b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mval_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-97eb1eaf3e05>\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(epoch, mode, display)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mopt_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kl_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m## total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgc9JOzGGf-i"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBR2sxL7ryGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30eecba0-2e39-4046-ff83-adbf6b1e5838"
      },
      "source": [
        "# Load from checkpoint\n",
        "model = VAE(config, len(vocab.itos), vocab.padding_index)\n",
        "ckpt = torch.load(save_path)\n",
        "model.load_state_dict(ckpt['model'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNUYkXKPGh3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268621a0-7ba2-462d-abee-60f0a8707ab2"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "batch_size = config.batch_size\n",
        "\n",
        "test_sentence = 'The service was terrible.'\n",
        "tokens = ['<BOS>']+tokenizer(test_sentence)+['<EOS>']\n",
        "text_ids = torch.tensor([[vocab[token] for token in tokens]])\n",
        "mean, logvar = model.encode(text_ids.to(model.device), torch.tensor([len(tokens)]))\n",
        "dst = MultivariateNormalDiag(loc=mean[0], scale_diag=torch.exp(0.5 * logvar[0]))\n",
        "latent_z = dst.sample((batch_size,))\n",
        "\n",
        "# latent_z = torch.FloatTensor(batch_size, config.latent_dims).uniform_(-1, 1).to(device)\n",
        "# latent_z = torch.randn(batch_size, config.latent_dims).to(device)\n",
        "\n",
        "gen_text = None\n",
        "gen_chars = torch.tensor([[vocab.bos_index] for _ in range(batch_size)])\n",
        "seq_lengths = torch.zeros(batch_size).type(torch.LongTensor)\n",
        "not_done = torch.ones(batch_size).type(torch.LongTensor)\n",
        "state = None\n",
        "while gen_text is None or gen_text.size()[1] < 1000:\n",
        "    logits, state = model.decode(latent_z.to(model.device), gen_chars.to(model.device), torch.ones(batch_size), state)\n",
        "    cdst = Categorical(logits=logits)\n",
        "    gen_chars = cdst.sample()\n",
        "    not_done *= (gen_chars != vocab.eos_index).squeeze().type(torch.LongTensor)\n",
        "    seq_lengths += not_done\n",
        "    if gen_text is None:\n",
        "        gen_text = gen_chars.cpu()\n",
        "    else:\n",
        "        gen_text = torch.cat((gen_text, gen_chars.cpu()), dim=-1)\n",
        "    if torch.sum(not_done) == 0:\n",
        "        break\n",
        "for i,line in enumerate(gen_text):\n",
        "    print(' '.join([vocab.itos[line[j]] for j in range(seq_lengths[i])]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cancel grabbed kelly and waiter reminded them such a pretty jimmy ride on the show of a smaller bar , i got to come here . last single place was overpriced and coconut and had no seasoning ! yes , two hours , i gave it a much of 2 people in the saguaro . they had the <unk> ' tejas and wine at the bar . when we suggested anything we were there , we received to check a few nights later the very enjoyable ambiance <unk> food and one . the restrooms were very good but the restaurant . it one has been five stars just even though i will be back again we live at a rush . the fact that this place is totally worth a <unk> resort . i say the service is great but i love my boba . for one of the mango rolls . eh . the most abysmal person was working . daughter ' s and sushi . it was real and nothing like i attempted to pick the free fried selection , but the fries were delicious . we both had menus , a couple beers and anything on top . needless to the almond filet mignon to go . the fried chicken salad was a taste of special plates on the dinner + well . the fish was very good ! ! sushi . . . he ' s more than beef and after the butternut shrimp in the future i am in a sweet tooth , but my wife had the cheese my bread and pineapple <unk> that i could have a really <unk> cooking red velvet salad . i am not to say that this and a very sturdy joint , which you say they were absolutely made raving . i was in season to that , i did like this . the nasty three before i asked down , she said , but to make it clear any other than the 24 days . we took numerous round , on <unk> , <unk> , every single chair . not particularly calling him it . that did will be the job . i ' m not so giving them making you !\n",
            "this is shopping 90 up on my heart .\n",
            "went with my questions too tired for dinner . maybe the staff ingredients also be always politely . people that i decided to just read my whole experience for lunch .\n",
            "loved to eat here in one day . my boyfriend went to a few feet that i want to give a few things a few steps . it <unk> for the item that was very deep and had according that personally . never picked up . we tried the indian american croissant and tacos . my sister had the squash roast & i must wow it next space - i will go back again and does like 3 )\n",
            "spinato yogurt cuisine arrived cannot beat a monday night ) . i tried the food ! ! we go back , my wife and i was in 4 . so i do order things it was very nice , open the yogurt platter . . . . the green ingredients are large , they also know anywhere else we could have make the match . i have to pick it out to go if this dish isn ' t a lot of 5 kinds of mexican style and the prices are fair .\n",
            "growing ' cap two visits it an dishes , was able to <unk> her - before i came out for happy hour because it was a joke and was full girl ) in a other booking <unk> . the patio <unk> , but was nice to our liking . i ' m not sure from the next time . . now , first of our experience\n",
            "a huge harkins <unk> shop ! thank you go to detail .\n",
            "douchebags smores 75 smith 15% chilada ny spring training blended cut soda ) , bring greater and i needed a plate of desserts and were interested at closing time with a winner ( <unk> ' s tom ' s spring . go to the market ) . <unk> , they had not a offer awesome vegetables and wonderful experience . he picked me your car order the usual fries , which he tasted starting and his smoked sugar , chips but fresh corn and stuck ! and 2 times wait today . . . i can ' t hurt anything else with from another chimi and know the food and better doesn ' t stand home or was frustrating . i mean , my passion , including a philly mess i got the combination . i went simply <unk> . i grew up by that it ' s a lot of options . it feels like yet . on my money , it ' s just weird . i ' d have to go home before the moment . this place knows simply <unk> .\n",
            "i usually give this place less for a cheap spot for you ! !\n",
            "the rates of their west subs are completely good . i really sometimes have never been disappointed in this place , so i really don ' t like if you like fate from that . i ' m my food here again and this was <unk> ! i typically time , i ' m a particular happy hour , and it ' s too much as better impression . especially it was my new favorite store . i actually invited her gifts and cash at to attend calling her order . until i ' ll regret it , i ' ve shopped hut and get a lot . it is hands down in the <unk> community and have one really interesting shop - hubby , there were really more mexican places in one . i said their obvious was very nice to make my child ' s decent job ( and they even spend the football most friendliest <unk> for us ) . in the phoenix , i usually do complete , any other meet in the front of their shirt was turning out . 99 and got pictures to look ! oooh ! it ' s a in-n-out rental produce making sitting behind of looking and more seats and pool one of people who is act with your significant friends for most people who are for my goal ! anyway , <unk> <unk> <unk> consignment ) like a series of rear near the scale of the valley that come up but they are worth the money and i do . no time you get <unk> <unk> ! they also have a lifetime or a <unk> attitude it is the hot cheese on a game of their mall because you don ' t worry if the kind of bite it is going to follow out at $10 . woo , when i walk around and for having those obscure bear , putting down with the air - <unk> from what their stylish people ask <unk> ladies !\n",
            "siu faint face ( 7 <unk> <unk> ( fired , <unk> , etc . 2 entrees . ) , i usually heard a new food . he very salty and the <unk> fell perfectly in <unk> pizza which was amazing ! amazed they were a good assortment of the lettuce ( red pancakes , the best salad , and cranberry tomatoes , which - delish ) , but don ' t be on the side of fez . ps because you will have he even been small straight around to the merry . honestly anything . to just yogurtland it is one of my favorite places , but without <unk> it ' s open fire on on my schedule but and hasn ' t eat in that ? we finally went back to go for a family with my new italian bbq . . then you may find a few extra <unk> and enjoy the flesh or brown dishes . the burgers are 5 years and doesn ' t choose choice for anyone else . i don ' t completely think not a great place . to find the lemon reason and happy crunch ! ! ! i think they just so that ' s a solid craving a pita , you need to have the local oil eats . i salad however i am not paying the same restaurant . the final people were there for a table or gave us to keep any of the darn better dishes in town but whoever was able to be there next time , which just took forever to arizona customers for us in the valley early - and escorted frozen yogurt . 3 off to <unk> a latte , chill . i was sick that the <unk> of the two , and that were both placed ( followed for you stain to the mill section ) and the servers were lucky and fast . the staff is very friendly . ( no clue when the review . . . . . ) , the bowl is <unk> were really quick , but you can ' t beat the way to a big trade . i ' m not completely much smaller than items , and they ' re pretty one kicked on the green noodles . i asked our dog on my order , and were freezing and said this buffet is since they ' re very generous . the smell is sarcastic order , and they tend to find my delight . i don ' t think i can ' t comment in a lot of omelettes . i <unk> ' s el <unk> , and spoken piece ' s milk drinks and how it is fresh they be concession ones that maybe all . will not eat the perfect thing so i see how many chinese restaurants i get out to these in the front ways with <unk> you signed up . my card is gone and i went on in again and i had 2 pieces of air who was towards the planning . i heard , it was $9 . you will find a new paint and i love what they have free misters are good . . . . . . . . ( 6 <unk> per thing ) ( again ) as possible , but no problem is that a guest lacking <unk> , hit up - aka her new drinks card , , and be proud of <unk> and they have sets , and in the wall , and they ' re going to hardly yogurtland to get this experience ! why a place to yourself ' em for the dirty ladies that snagged the same thing , why i want some of the way <unk> ' s <unk> ' s crepes ( which is well staffed ! ) . . . sandwiched this place . this place is just ok but i always actually prefer his ass - more in business , otherwise , blanco will be there - especially for that but a few times i ' ve saved the as well daily visits . they remember also you can tell you which he ' s up to charging cash , <unk> went with your bike and he wanted to enjoy . she ' s never available for $20 from the <unk> wall in frozen changing $5 . whoever ' s cheapest . . . and a bill makes me their own chicken pudding . the choices are always good , but everything was outstanding . the meat and service , and the service was fantastic . they want things . you can say these guys have to be seen the cake with more appeal at tutti . the prices are reasonable , <unk> ! the prices are much better but . it was real so honestly worth it . the smell of the owner don ' t appreciate it , lose $10 , sometimes you ' ll come back to be an intimate , still get your table but has a restaurant itself . the breakfast joint is great . their part is a bit better , so it is a true diner in some of the sandwich . he also had the slimy atmosphere of the bottom menu and the hot food was outstanding with the chef . it ' s always convenient !\n",
            "i probably have not once call this place get a logical impression . it do not bring the project .\n",
            "friendly and helpful and was very impressed together , but hardly will just be fooled by the soul . the place was close to <unk> and finally our landscaping responded . . . i make it not the worst restaurant in scottsdale , but here ' s the actual restaurant i ' ll be going for , but definitely because i love this buffet .\n",
            "shepherd is fair they the beer on this piece of amazing sandwich and so eating of canadian goodness . the tzatziki is more than <unk> , but good . they have always any other amazingly rude . . . but fresh , fresh , organic meat and savory ! i ' m sure that i wouldn ' t let me be a party and i love it if you ' re in the holidays you would never miss it .\n",
            "i love this place because we had a drink and i frequent the table again . the portions are really good but the fries are huge and tender ! it in a lunch , which may be filling . it was a lovely place to check by kraft near both items . we waited before 12 30 in the place too in house . the cafe <unk> were very uninspiring . jo were a bit simple and exactly getting the hostess either . anyway , i went for the gf , used to the drinks which didn ' t have the physical buckets of prices . the appetizers did our server and had the gel ) you can get until the drink order fairly low . they tease a small dish that is quite strange . but there was a lot of fun . our favorites are the fact that it is a good deal . we had spaghetti half off the west side of my soup and the food was good garlic sliced and it was different and said it would have been a little interesting for .\n",
            "our burger was fantastic and great . we had our kids to enjoy the meal . . ordered a green chile burro . overall , it is good . a little confusing . the first pizza there was ok and i ate it . i have one <unk> last week with all sorts of beers and everything is a bit on the counter . they don ' t talk about a buffet of 3 stars for this day . the sandwiches were the lobby they <unk> of the crust . how you still can indeed such . i found it on the menu , but no more cheese was more than $10 . i have been to ruth ' s , so i have more than the meal . i ' ve been to write it because the more cider previously unprofessional but the bartender is fine . my friends was recommended and dancing and a new tenant . that ' s never on the one . i don ' t know what this review . i love this place . pappadeaux , and the one part that in a strip mall is a little more in season . i love this place ! great headphones and pretty lousy one .\n",
            "it doesn ' t get to be picking everything for lunch directly into a good meal . well just so-so . i don ' t will be back it that is to how i am in my opinion , will be too picky . the employees are amazing and <unk> , has some kind of <unk> and owners are far and already cares about the <unk> . if you don ' t like ( upscale ! well ) . my favorite part of the ingredients are the chorizo , which is good for garlic . if i can remain the butter favorite , they get their honey drastically specialty <unk> sandwich but not only yet the yogurt . you don ' t know if you have been meat on certain in due new york . if i tried to sit in line . the cheese 5/5 are what the grilled taco is awesome and flaky and it tastes like a <unk> then . anyway . <unk> is the highlight of the jalapeño colors . it ' s just just ok , but you get a delicious meal , like me for my <unk> to eat someone to be wary of each place to come for some people . not the quality of one sweet with <unk> but <unk> <unk> , and watch the halibut . it ' s what they can get . the first thing i didn ' t find great . i minus the same state of horseradish , my husband and i had a good experience here . my favorites got spicy sausage steak and her soda , but it was amazing when i heard them in the gravy , and the cheese dogs seems to do some rich of the oven . i don ' t go much better than spicy lettuce that right about it , but i don ' t have this similar to hide with an afterthought that would be disgusting . i don ' t have to admit it ' s still too big !\n",
            "i love the flavor of this world . i thought they were messy , more than <unk> . we also had an <unk> burger pizza with vanilla <unk> , which was very good . however i was taken aback on a side salad i spotted that if i came into one plate i thought the edamame cookies were good . my wife works pretty good and the menu was rich and very too . they had fresh mixed greens and chili vinegar . i had to order a dessert between the meatball hamburger . for the reason they opened with the dessert here -- keep me so good )\n",
            "the property is very pleased with all a beer , great stuff ! i ' ve been here where many times , it is attending is it ' s good really really fun . the food was delicious ! good rooms from the restaurant and others , you have a couple of dips , and i have never had an eclectic menu and it ' s easy to sit down and the desserts and the menu was fair . great menu and too spicy . <unk> has an anchor selection of boring , and tasty food . the beer selection is decent . and it is a great hike !\n",
            "i have been here twice and loved my new favorite place . this place doesn ' t make an excellent feeling open for the rest of food for sushi . everything i don ' t recommend it ' s is a cool touch on weeknights the west valley is incredible , its from lunch . and if i is that i get with a living social experience . they do have a special bloody mary type of cheese and beans . fry ' s only bad plus wings were fancy and <unk> ' s too ? taste . good staff , close to a yard when someone ( or you ' re looking for a climb over nearby ) and stay away in the area . a few year to get a detail <unk> , i would just want to stop on . i food is a great cup of my favorite breakfast . but the presentation is ridiculous . exceedingly half day are a great spot for <unk> travel to <unk> of tea . staff is awesome and friendly . the atmosphere is pretty weak and they look too at an .\n",
            "on quality , coffee is right , but especially the second selection of homemade pizza and the valet on their diner choices ! i ' ve had some yogurt fajita for food and a <unk> but it is all plus the night . i ' ve only had spinach tacos with a happy hour fit for the menu , but i ' ve finished your restaurant . it is tucked away from the <unk> , and curries <unk> of you just know , or your <unk> for under my first time ( the hills ends ) . i ' m <unk> taking some and happy <unk> and ok , i felt like their own bottle of my <unk> and worth the job . when i ' ve never been to the fried drunk and it is a bit pricy , but it every time home offers <unk> too delicious . would be a bit more expensive than any other <unk> stores in the valley . it is really warm or good for awhile . what i feel like something like dish . <unk> it is one of this time in terms of scottsdale , 90% of the crowd . the service has very great and satisfying restaurant , and get to sell , your isn ' t little anything to make someone want to keep the humane seem to go here have a little bit over . their patio is fabulous and too cute for drinks and the prices are great . <unk> are a fan of you , and it ' s bar sized . this place is a good customer food !\n",
            "lola dish was absolutely grilled , so i ' d like it again in town . no soup for my food on the menu at dinner . we had our ultimate <unk> and they were started off with free . our party . the woman who got on the menu was great , i will never liked it . i wasn ' t sure why they were warned for a sunday night when we got to fight with cooking an old order ? in popularity we ordered the orange sauce , which was it much -- but not worth the price for some happy hour . quality of us had a nice surprise . they enjoy a nice <unk> , unhelpful latin kind of art room - and they would have been waiting to a living room out - well , and drink , sunday for example and painting or we were swept in . . . whew ( everything in n <unk> meets for example ) . in , 3 than my phone was funny and not careful . . it was my last trip . . . no clue . . , there was a great buffet and found sports bar . they have a strong teams ! highly recommended the <unk> and having my vote .\n",
            "i will probably be back to try !\n",
            "i am in a few times i ' m for the art of my wife & an awesome deal . feels , i will continue to find the park from the <unk> <unk> , when this internet made almost all the best pizza ! !\n",
            "she realized we <unk> on <unk> like some of the time . they were willing to help with next to a foot cleaning . he seemed to find them asking his problems and a decline . other restaurant we got a refill way to eat a few tables and demanded me , but i wasn ' t taking reservations . and i chose my gym . the first issue and the property didn ' t leave them to waste my turn up and either ( when they expected our bill ? ) , i would never find such a bit so fucked up prior to be in all the managers . i like a bar . it was cute you from the mesa . not better than an orgasm , its even tough to see that lots of that are always fast and all the same time ( being taken out of the language ' s thing from a pro , i ' d be able to probably work and i didn ' t agreed on commission i do like that id about 5 stars is 1 )\n",
            "food is great . the staff is nachos as well ( all ham . when my <unk> friend come here <unk> and something ) . i have yet to be disappointed . espresso patty and delicious feel , the formula is amazing and very cheap !\n",
            "burros doggie chickens personalized quality poor drunken asparagus c . . . <unk> , someone and i could ' t be going here . . . . . . but i ordered the first time we could go again , but it is pretty damn to like the <unk> of me and the people . the young grounds is very professional , is friendly , when we at total a <unk> habit and thankfully you . and then what <unk> your air to sit on their customers . i would give me less than a 10 mile radius with the fancy dessert but i agree with the weak accompaniments . i don ' t even bother with them . everyone were at this place , so they have a lot of seating . although my room is a young mason drinker which just lower the booze . the ambiance was lovely and very far as some of the best sandwiches . why each is the owner . you got to <unk> with the salted price is like those people <unk> into some time at all . it seemed very clean . , they didn ' t take reservations one on their horchata . the portions are tasty . the place always have a great view of the main swimming ( but i find that it ' s fits those and basic pictures ) . this isn ' t the best experience if the sitting around the day they were in mesa to make our own loft . my party were free and i wanted to feel on the mesa round . apparently it was easy to take him like cattle , then he did take cash . she exclaimed with temperature and a <unk> in the past two plus <unk> . <unk> , not we have made out what would closed us which was nothing <unk> outside and tasty . i mean <unk> <unk> i felt like the waiter would not have to ask . everything don ' t . it ' s no one person in the kitchen . no doubt you wish they didn ' t have anything or things about tj ' s .\n",
            "one of nice world fro yo . . . . and i see if you ' re in this area ( most eating more . the number of the restaurant is a pleasant in <unk> .\n",
            "great and cheap ! this location is immaculate , serving the art of the <unk> , or the anticipation . there are also a few options that still do not behind him , including sports . they have super helpful seats and glasses set for a fairly notice to help the <unk> <unk> gentle the floor . . . but it ' s hard to say , or where your day patron give them a box and they have a certain blandness <unk> you ' ll fall into or completely peace and even though i ' m a fan , they use of establishment , you can not bring a $600 open for these days .\n",
            "olive male flag was nothing special . ) i asked someone else not to be it finger . i didn ' t remember she would call if i was making a yes , there is no filling ( thinner ) , pepper <unk> sauce , but it ' s not well-seasoned yumminess and out . it was hot ( ex with <unk> veggies ) and i told him and <unk> , and i was told it . . and , i tried the bacon . hanny ' s self sound isn ' t fast and seasoned homemade and the sweet potato tasted as fresh . well , the burger was very good and very tasty . made our waitress about a good idea for a small feeling that the workers were very super excellent as well , and they didn ' t tremendously busy if you are wondering what i loved it if i know they ' d only need to pay his <unk> - just $2 so many rice bowls for me . now in the city proves all , the vegan has a little pricy or $3 - our server came for sure we would keep his back classes and tell her we had so either 3 . at first <unk> i met a little , to return all of those , i would be <unk> only having to prepare them again .\n",
            "fast <unk> brunch . sandwich ! ! ) , with the first . it was indeed good . great suggestions , cool pizza . don ' t waste there is more appetizing than there .\n",
            "great pub located around this place . i like this place , but mr by a party who has lost cash , and more helpful with a good room , this store is for my first visit . janet they had a very quick consultation and good company there , more claustrophobic around . . . . tired different booths when they are in <unk> of the phoenix harkins awesome , uses <unk> and not looking beer , or a can , so you may recognize going to get brunch and <unk> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT5SOrm_OMhO"
      },
      "source": [
        "# Mutual information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXwWiZSuEzjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d311fd-ac8f-4480-a5b6-6e8657697621"
      },
      "source": [
        "def mutual_information(model, dataset, N1=10000, N2=100):\n",
        "    model.eval()\n",
        "\n",
        "    kl1 = None\n",
        "    z2 = None\n",
        "    for batch, seq_lengths in dataset:\n",
        "        mean, logvar = model.encode(batch.to(model.device), seq_lengths)\n",
        "        logstd = logvar / 2\n",
        "        dst = MultivariateNormalDiag(loc=mean, scale_diag=torch.exp(logstd))\n",
        "        z1 = dst.sample((N1,))\n",
        "        batch_z2 = torch.cat(list(dst.sample((N2,))))\n",
        "        if z2 is None:\n",
        "            z2 = batch_z2\n",
        "        else:\n",
        "            z2 = torch.cat((z2, batch_z2))\n",
        "        batch_kl1 = torch.mean(torch.sum(((z1 ** 2 - ((z1 - mean) / torch.exp(logstd)) ** 2) / 2) - logstd, dim=-1), dim=0)\n",
        "        if kl1 is None:\n",
        "            kl1 = batch_kl1\n",
        "        else:\n",
        "            kl1 = torch.cat((kl1, batch_kl1))\n",
        "    kl1 = torch.mean(kl1)\n",
        "    \n",
        "    kl2 = None\n",
        "    for batch, seq_lengths in dataset:\n",
        "        mean, logvar = model.encode(batch.to(model.device), seq_lengths)\n",
        "        logstd = logvar / 2\n",
        "        mean = mean.unsqueeze(1)\n",
        "        logstd = logstd.unsqueeze(1)\n",
        "        batch_kl2 = torch.mean(torch.sum(((z2 ** 2 - ((z2 - mean) / torch.exp(logstd)) ** 2) / 2) - logstd, dim=-1), dim=-1)\n",
        "        if kl2 is None:\n",
        "            kl2 = batch_kl2\n",
        "        else:\n",
        "            kl2 = torch.cat((kl2, batch_kl2))\n",
        "    kl2 = torch.mean(kl2)\n",
        "\n",
        "    return kl1 - kl2\n",
        "\n",
        "with torch.no_grad():\n",
        "    print(mutual_information(model, dataset['train']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.3526)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}