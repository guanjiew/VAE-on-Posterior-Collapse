{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ControlVAE Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanjiew/csc412_vae/blob/main/ControlVAE_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYAruimtNmL"
      },
      "source": [
        "Code is adapted from [the original ControlVAE repository](https://github.com/shj1987/ControlVAE-ICML2020/tree/master/Language_modeling/Text_gen_PTB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfLIsQFIvrMn"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W-1FfEitYz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a241a018-1afe-40d1-bea4-f23d9060b9d3"
      },
      "source": [
        "%pip install texar-pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting texar-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/7e/20aa39ee9d19dcd1a1c0db4dad878088cfba216f45aeaa8fa89615ef46c0/texar_pytorch-0.1.2.post1-py3-none-any.whl (434kB)\n",
            "\r\u001b[K     |▊                               | 10kB 11.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 9.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30kB 6.0MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 6.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 112kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 143kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 153kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 174kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 194kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 215kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 225kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 235kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 245kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 256kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 266kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 276kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 286kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 296kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 307kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 317kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 327kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 337kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 348kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 358kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 368kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 378kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 389kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 399kB 5.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 409kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419kB 5.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 430kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 440kB 5.2MB/s \n",
            "\u001b[?25hCollecting mypy-extensions\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (20.9)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: regex>=2018.01.10 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2.23.0)\n",
            "Requirement already satisfied: numpy<=1.19.5,>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (1.19.5)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=19.0->texar-pytorch) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (1.24.3)\n",
            "Installing collected packages: mypy-extensions, funcsigs, sentencepiece, texar-pytorch\n",
            "Successfully installed funcsigs-1.0.2 mypy-extensions-0.4.3 sentencepiece-0.1.91 texar-pytorch-0.1.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA-j84Kot-hG"
      },
      "source": [
        "import math, os, random\n",
        "from collections import Counter\n",
        "from typing import Any, Dict, Optional, Tuple, Union\n",
        "random.seed(42)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "import texar.torch as tx\n",
        "from texar.torch.custom import MultivariateNormalDiag\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVNAghBeu8TC"
      },
      "source": [
        "# VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9_L26Qgu_5o"
      },
      "source": [
        "def dim_kl_divergence(means: Tensor, logvars: Tensor) -> Tensor:\n",
        "    \"\"\"Compute the KL divergence between Gaussian distribution\n",
        "    \"\"\"\n",
        "    kl_cost = -0.5 * (logvars - means ** 2 -\n",
        "                      torch.exp(logvars) + 1.0)\n",
        "    kl_cost = torch.mean(kl_cost, 0)\n",
        "    return kl_cost\n",
        "\n",
        "def kl_divergence(means: Tensor, logvars: Tensor) -> Tensor:\n",
        "    return torch.sum(dim_kl_divergence(means, logvars))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ckhxqwvITp"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "\n",
        "    def __init__(self, config_model, vocab_size, padding_idx):\n",
        "        super().__init__()\n",
        "        # Model architecture\n",
        "        self._config = config_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=self._config.embed_dim,\n",
        "            padding_idx=padding_idx\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.enc_dropout_in = nn.Dropout(self._config.enc_dropout_in).to(self.device)\n",
        "        \n",
        "        self.encoder = nn.LSTM(\n",
        "            input_size=self._config.embed_dim,\n",
        "            hidden_size=self._config.hidden_size,\n",
        "            batch_first=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.enc_dropout_out = nn.Dropout(self._config.enc_dropout_out).to(self.device)\n",
        "\n",
        "        self.connector_mlp = nn.Linear(\n",
        "            in_features=self._config.hidden_size * 2,\n",
        "            out_features=self._config.latent_dims * 2\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.mlp_linear_layer = nn.Linear(\n",
        "            in_features=self._config.latent_dims,\n",
        "            out_features=self._config.hidden_size * 2\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.dec_dropout_in = nn.Dropout(self._config.dec_dropout_in).to(self.device)\n",
        "\n",
        "        lstm_input = self._config.embed_dim\n",
        "        logits_input = self._config.hidden_size\n",
        "        if self._config.skip_connection:\n",
        "            lstm_input += self._config.latent_dims\n",
        "            logits_input += self._config.latent_dims\n",
        "\n",
        "        self.decoder = nn.LSTM(\n",
        "            input_size=lstm_input,\n",
        "            hidden_size=self._config.hidden_size,\n",
        "            batch_first=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.dec_dropout_out = nn.Dropout(self._config.dec_dropout_out).to(self.device)\n",
        "\n",
        "        self.logits_layer = nn.Linear(\n",
        "            in_features=logits_input,\n",
        "            out_features=vocab_size\n",
        "        ).to(self.device)\n",
        "    \n",
        "    def init_state(self, batch_size=1):\n",
        "        return (\n",
        "            torch.zeros(1, batch_size, self._config.hidden_size).to(self.device),\n",
        "            torch.zeros(1, batch_size, self._config.hidden_size).to(self.device)\n",
        "        )\n",
        "\n",
        "    def encode(self, x, seq_lengths):\n",
        "        emb = self.enc_dropout_in(self.embedding(x))\n",
        "        emb = torch.nn.utils.rnn.pack_padded_sequence(emb, seq_lengths, batch_first=True, enforce_sorted=False) # https://towardsdatascience.com/61d35642972e\n",
        "        _, state = self.encoder(emb, self.init_state(x.size()[0]))\n",
        "        state = self.enc_dropout_out(torch.cat(state, dim=-1)[0])\n",
        "        mean_logvar = self.connector_mlp(state)\n",
        "        mean, logvar = torch.chunk(mean_logvar, 2, 1)\n",
        "        return mean, logvar\n",
        "    \n",
        "    def decode(self, z, x, seq_lengths, prev_state=None):\n",
        "        if prev_state is None:\n",
        "            prev_state = torch.chunk(self.mlp_linear_layer(z).unsqueeze(0), 2, -1)\n",
        "        emb = self.dec_dropout_in(self.embedding(x))\n",
        "        if self._config.skip_connection:\n",
        "            z_seq = torch.cat([z.unsqueeze(1) for _ in range(emb.size()[1])], dim=1)\n",
        "            emb = torch.cat((emb, z_seq), dim=-1)\n",
        "        emb = torch.nn.utils.rnn.pack_padded_sequence(emb, seq_lengths, batch_first=True, enforce_sorted=False) # https://towardsdatascience.com/61d35642972e\n",
        "        outputs, state = self.decoder(emb, prev_state)\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        outputs = self.dec_dropout_out(outputs)\n",
        "        if self._config.skip_connection:\n",
        "            z_seq = torch.cat([z.unsqueeze(1) for _ in range(outputs.size()[1])], dim=1)\n",
        "            outputs = torch.cat((outputs, z_seq), dim=-1)\n",
        "        logits = self.logits_layer(outputs)\n",
        "        return logits, state\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YZOkyApvNmP"
      },
      "source": [
        "# PID Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6g1VCAvPdJ"
      },
      "source": [
        "class PIDControl():\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"define them out of loop\"\"\"\n",
        "        # self.exp_KL = exp_KL\n",
        "        self.I_k1 = 0.0\n",
        "        self.W_k1 = 0.0\n",
        "        self.e_k1 = 0.0\n",
        "        \n",
        "    def _Kp_fun(self, Err, scale=1):\n",
        "        return 1.0/(1.0 + float(scale)*math.exp(Err))\n",
        "        \n",
        "\n",
        "    def pid(self, exp_KL, kl_loss, Kp=0.001, Ki=-0.001, Kd=0.01):\n",
        "        \"\"\"\n",
        "        position PID algorithm\n",
        "        Input: KL_loss\n",
        "        return: weight for KL loss, beta\n",
        "        \"\"\"\n",
        "        error_k = exp_KL - kl_loss\n",
        "        ## comput U as the control factor\n",
        "        Pk = Kp * self._Kp_fun(error_k)\n",
        "        Ik = self.I_k1 + Ki * error_k\n",
        "\n",
        "        ## window up for integrator\n",
        "        if self.W_k1 < 0 and self.W_k1 > 1:\n",
        "            Ik = self.I_k1\n",
        "            \n",
        "        Wk = Pk + Ik\n",
        "        self.W_k1 = Wk\n",
        "        self.I_k1 = Ik\n",
        "        self.e_k1 = error_k\n",
        "        \n",
        "        ## min and max value\n",
        "        if Wk > 1:\n",
        "            Wk = 1.0\n",
        "        if Wk < 0:\n",
        "            Wk = 0.0\n",
        "        \n",
        "        return Wk\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV3yJUUGvZwz"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSMZHtSCvizg"
      },
      "source": [
        "class Config():\n",
        "  dataset_size = 8750\n",
        "  num_epochs = 100\n",
        "  hidden_size = 256\n",
        "  dec_dropout_in = 0.5\n",
        "  dec_dropout_out = 0.5\n",
        "  enc_dropout_in = 0.\n",
        "  enc_dropout_out = 0.\n",
        "  batch_size = 32\n",
        "  embed_dim = 256\n",
        "  latent_dims = 32\n",
        "  max_vocab = 12000\n",
        "\n",
        "  pid_control = True\n",
        "  skip_connection = True\n",
        "\n",
        "  lr_decay_hparams = {\n",
        "      \"init_lr\": 0.001,\n",
        "      \"threshold\": 2,\n",
        "      \"decay_factor\": 0.5,\n",
        "      \"max_decay\": 5\n",
        "  }\n",
        "\n",
        "  # KL annealing\n",
        "  kl_anneal_hparams = {\n",
        "      \"warm_up\": 10,\n",
        "      \"start\": 0.1\n",
        "  }\n",
        "\n",
        "  initializer_hparams = {\n",
        "      'mean': 0.0,\n",
        "      'std': embed_dim**-0.5,\n",
        "  }\n",
        "\n",
        "  opt_hparams = {\n",
        "      'optimizer': {\n",
        "          'type': 'Adam',\n",
        "          'kwargs': {\n",
        "              'lr': 0.001\n",
        "          }\n",
        "      },\n",
        "      'gradient_clip': {\n",
        "          \"type\": \"clip_grad_norm_\",\n",
        "          \"kwargs\": {\n",
        "              \"max_norm\": 5,\n",
        "              \"norm_type\": 2\n",
        "          }\n",
        "      }\n",
        "  }"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP2M-MCxUL-7"
      },
      "source": [
        "config = Config()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "madW6YSv1jay"
      },
      "source": [
        "# Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tItIdmTwj-4"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/rekiksab/Yelp-Data-Challenge-2013/master/yelp_challenge/yelp_phoenix_academic_dataset/yelp_academic_dataset_review.json'\n",
        "data = pd.read_json(url, lines=True)\n",
        "data = data.loc[:,['text','stars']]\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "data.loc[:,'text'] = data.loc[:,'text'].map(lambda s: ['<BOS>']+tokenizer(s)+['<EOS>','<PAD>'])\n",
        "\n",
        "counter = Counter()\n",
        "for line in data.loc[:,'text'].values:\n",
        "    counter.update(line)\n",
        "vocab = Vocab(counter, max_size=config.max_vocab)\n",
        "vocab.bos_index = vocab['<BOS>']\n",
        "vocab.eos_index = vocab['<EOS>']\n",
        "vocab.padding_index = vocab['<PAD>']\n",
        "\n",
        "data.loc[:,'text'] = data.loc[:,'text'].map(lambda s: [vocab[token] for token in s[:-1]])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXM0LgELKbq"
      },
      "source": [
        "data_pairs = [(pair[0], pair[1]) for pair in data.loc[:,['text','stars']].values]\n",
        "if len(data_pairs) > config.dataset_size:\n",
        "    data_pairs = random.sample(data_pairs, config.dataset_size)\n",
        "\n",
        "random.shuffle(data_pairs)\n",
        "valid_len = round(0.15 * len(data_pairs))\n",
        "valid_data, test_data, train_data = data_pairs[:valid_len], data_pairs[valid_len:2*valid_len], data_pairs[2*valid_len:]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm-HfNP9LUNf"
      },
      "source": [
        "def get_padded_batches(data, batch_size, padding_index):\n",
        "    batches = []\n",
        "    seq_lengths = []\n",
        "    for k in range(batch_size, len(data), batch_size):\n",
        "        batch = [pair[0] for pair in data[k-batch_size:k]]\n",
        "        lengths = [len(text) for text in batch]\n",
        "        padded_length = max(lengths)\n",
        "        for text in batch:\n",
        "            while len(text) < padded_length:\n",
        "                text.append(padding_index)\n",
        "        batches.append(torch.tensor(batch))\n",
        "        seq_lengths.append(torch.tensor(lengths))\n",
        "    return batches, seq_lengths\n",
        "\n",
        "train_batches, train_lengths = get_padded_batches(train_data, config.batch_size, vocab.padding_index)\n",
        "valid_batches, valid_lengths = get_padded_batches(valid_data, config.batch_size, vocab.padding_index)\n",
        "test_batches, test_lengths = get_padded_batches(test_data, config.batch_size, vocab.padding_index)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfpX6s_j4x_C"
      },
      "source": [
        "dataset = {\n",
        "    \"train\": list(zip(train_batches, train_lengths)),\n",
        "    \"valid\": list(zip(valid_batches, valid_lengths)),\n",
        "    \"test\": list(zip(test_batches, test_lengths))\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK5x-aPqwVOA"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxvnGdq3wWdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b42423ba-3b34-43e5-e235-70bcc30af8d2"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "opt_vars = {\n",
        "    'learning_rate': config.lr_decay_hparams[\"init_lr\"],\n",
        "    'best_valid_nll': 1e100,\n",
        "    'steps_not_improved': 0,\n",
        "    'kl_weight': 1.0\n",
        "}\n",
        "\n",
        "decay_cnt = 0\n",
        "max_decay = config.lr_decay_hparams[\"max_decay\"]\n",
        "decay_factor = config.lr_decay_hparams[\"decay_factor\"]\n",
        "decay_ts = config.lr_decay_hparams[\"threshold\"]\n",
        "\n",
        "save_path = './checkpoint.ckpt'\n",
        "\n",
        "model = VAE(config, len(vocab.itos), vocab.padding_index)\n",
        "\n",
        "optimizer = tx.core.get_optimizer(\n",
        "    params=model.parameters(),\n",
        "    hparams=config.opt_hparams)\n",
        "scheduler = ExponentialLR(optimizer, decay_factor)\n",
        "\n",
        "max_iter = min(config.num_epochs*len(train_data)/config.batch_size, 80000)\n",
        "print('max steps:', max_iter)\n",
        "\n",
        "global_steps = {}\n",
        "global_steps['step'] = 0\n",
        "pid = PIDControl()\n",
        "opt_vars[\"kl_weight\"] = 1.0\n",
        "Kp = 0.01\n",
        "Ki = -0.0001\n",
        "exp_kl = 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max steps: 19143.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwsjM_00yQiU"
      },
      "source": [
        "def _run_epoch(epoch: int, mode: str, display: int = 10) -> Tuple[Tensor, float]:\n",
        "    if mode == 'train':\n",
        "        model.train()\n",
        "        kl_weight = opt_vars[\"kl_weight\"]\n",
        "    else:\n",
        "        model.eval()\n",
        "        kl_weight = 1.0\n",
        "    \n",
        "    num_words = 0\n",
        "    nll_total = 0.\n",
        "\n",
        "    avg_rec = tx.utils.AverageRecorder()\n",
        "    for batch, seq_lengths in dataset[mode]:\n",
        "        if global_steps['step']>= max_iter:\n",
        "            break\n",
        "        mean, logvar = model.encode(batch.to(model.device), seq_lengths)\n",
        "        dst = MultivariateNormalDiag(loc=mean, scale_diag=torch.exp(0.5 * logvar))\n",
        "        z = dst.sample()\n",
        "        logits, _ = model.decode(mean, batch[:,:-1].to(model.device), seq_lengths-1)\n",
        "        rec_loss = tx.losses.sequence_sparse_softmax_cross_entropy(labels=batch[:,1:].to(model.device), logits=logits, sequence_length=(seq_lengths-1).to(model.device))\n",
        "        kl_loss = kl_divergence(mean, logvar)\n",
        "        total_loss = rec_loss + kl_weight * kl_loss\n",
        "        if mode == \"train\":\n",
        "            pbar.update(1)\n",
        "            global_steps['step'] += 1\n",
        "            if config.pid_control:\n",
        "                kl_weight = pid.pid(exp_kl, kl_loss.item(), Kp, Ki)\n",
        "                opt_vars[\"kl_weight\"] = kl_weight\n",
        "            ## total loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        batch_size = len(batch)\n",
        "        num_words += torch.sum(seq_lengths).item()\n",
        "        nll_total += total_loss.item() * batch_size\n",
        "        avg_rec.add(\n",
        "            [total_loss.item(),\n",
        "              kl_loss.item(),\n",
        "              rec_loss.item()],\n",
        "            batch_size)\n",
        "            \n",
        "        if global_steps['step'] % display == 1 and mode == 'train':\n",
        "            nll = avg_rec.avg(0)\n",
        "            klw = opt_vars[\"kl_weight\"]\n",
        "            KL = avg_rec.avg(1)\n",
        "            rc = avg_rec.avg(2)\n",
        "            \n",
        "    nll = avg_rec.avg(0)\n",
        "    KL = avg_rec.avg(1)\n",
        "    rc = avg_rec.avg(2)\n",
        "    if num_words > 0:\n",
        "        log_ppl = nll_total / num_words\n",
        "        ppl = math.exp(log_ppl)\n",
        "    else:\n",
        "        log_ppl = 100\n",
        "        ppl = math.exp(log_ppl)\n",
        "        nll = 1000\n",
        "        KL = args.exp_kl\n",
        "    \n",
        "    print(f\"\\n{mode}: epoch {epoch}, nll {nll:.4f}, KL {KL:.4f}, \"\n",
        "          f\"rc {rc:.4f}, log_ppl {log_ppl:.4f}, ppl {ppl:.4f}\")\n",
        "    return nll, ppl  # type: ignore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxMAUE7rytC1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d485994-4153-4253-cf75-116a62346da9"
      },
      "source": [
        "# Counts trainable parameters\n",
        "total_parameters = sum(param.numel() for param in model.parameters())\n",
        "print(f\"{total_parameters} total parameters\")\n",
        "\n",
        "best_nll = best_ppl = 0.\n",
        "\n",
        "## start running model\n",
        "pbar = tqdm(total = int(max_iter))\n",
        "train_losses, val_losses = [], []\n",
        "beta = []\n",
        "for epoch in range(config.num_epochs):\n",
        "    train_nll, _ = _run_epoch(epoch, 'train', display=200)\n",
        "    val_nll, _ = _run_epoch(epoch, 'valid')\n",
        "    test_nll, test_ppl = _run_epoch(epoch, 'test')\n",
        "    train_losses.append(train_nll)\n",
        "    val_losses.append(val_nll)\n",
        "    beta.append(opt_vars[\"kl_weight\"])\n",
        "\n",
        "    if val_nll < opt_vars['best_valid_nll']:\n",
        "        opt_vars['best_valid_nll'] = val_nll\n",
        "        opt_vars['steps_not_improved'] = 0\n",
        "        best_nll = test_nll\n",
        "        best_ppl = test_ppl\n",
        "\n",
        "        states = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"scheduler\": scheduler.state_dict()\n",
        "        }\n",
        "        torch.save(states, save_path)\n",
        "    else:\n",
        "        opt_vars['steps_not_improved'] += 1\n",
        "        if opt_vars['steps_not_improved'] == decay_ts:\n",
        "            old_lr = opt_vars['learning_rate']\n",
        "            opt_vars['learning_rate'] *= decay_factor\n",
        "            opt_vars['steps_not_improved'] = 0\n",
        "            new_lr = opt_vars['learning_rate']\n",
        "            ckpt = torch.load(save_path)\n",
        "            model.load_state_dict(ckpt['model'])\n",
        "            optimizer.load_state_dict(ckpt['optimizer'])\n",
        "            scheduler.load_state_dict(ckpt['scheduler'])\n",
        "            scheduler.step()\n",
        "            print(f\"-----\\nchange lr, old lr: {old_lr}, \"\n",
        "                  f\"new lr: {new_lr}\\n-----\")\n",
        "\n",
        "            decay_cnt += 1\n",
        "            if decay_cnt == max_decay:\n",
        "                break\n",
        "    if global_steps['step'] >= max_iter:\n",
        "        break\n",
        "\n",
        "print(f\"\\nbest testing nll: {best_nll:.4f},\"\n",
        "      f\"best testing ppl {best_ppl:.4f}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/76562 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7259426 total parameters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 2/76562 [00:00<3:01:20,  7.04it/s]\u001b[A\n",
            "  0%|          | 3/76562 [00:00<3:21:26,  6.33it/s]\u001b[A\n",
            "  0%|          | 4/76562 [00:00<3:55:08,  5.43it/s]\u001b[A\n",
            "  0%|          | 5/76562 [00:00<3:35:39,  5.92it/s]\u001b[A\n",
            "  0%|          | 6/76562 [00:01<3:25:37,  6.21it/s]\u001b[A\n",
            "  0%|          | 7/76562 [00:01<3:38:08,  5.85it/s]\u001b[A\n",
            "  0%|          | 8/76562 [00:01<3:49:36,  5.56it/s]\u001b[A\n",
            "  0%|          | 9/76562 [00:01<3:42:29,  5.73it/s]\u001b[A\n",
            "  0%|          | 10/76562 [00:01<3:45:52,  5.65it/s]\u001b[A\n",
            "  0%|          | 11/76562 [00:01<4:05:19,  5.20it/s]\u001b[A\n",
            "  0%|          | 12/76562 [00:02<4:39:21,  4.57it/s]\u001b[A\n",
            "  0%|          | 13/76562 [00:02<5:36:56,  3.79it/s]\u001b[A\n",
            "  0%|          | 14/76562 [00:02<6:01:44,  3.53it/s]\u001b[A\n",
            "  0%|          | 15/76562 [00:03<5:28:54,  3.88it/s]\u001b[A\n",
            "  0%|          | 16/76562 [00:03<4:58:02,  4.28it/s]\u001b[A\n",
            "  0%|          | 17/76562 [00:03<4:43:45,  4.50it/s]\u001b[A\n",
            "  0%|          | 18/76562 [00:03<5:06:16,  4.17it/s]\u001b[A\n",
            "  0%|          | 19/76562 [00:04<4:52:23,  4.36it/s]\u001b[A\n",
            "  0%|          | 20/76562 [00:04<5:04:32,  4.19it/s]\u001b[A\n",
            "  0%|          | 21/76562 [00:04<4:58:52,  4.27it/s]\u001b[A\n",
            "  0%|          | 22/76562 [00:04<4:37:03,  4.60it/s]\u001b[A\n",
            "  0%|          | 23/76562 [00:04<4:41:09,  4.54it/s]\u001b[A\n",
            "  0%|          | 24/76562 [00:05<4:14:40,  5.01it/s]\u001b[A\n",
            "  0%|          | 25/76562 [00:05<4:16:54,  4.97it/s]\u001b[A\n",
            "  0%|          | 26/76562 [00:05<4:22:26,  4.86it/s]\u001b[A\n",
            "  0%|          | 27/76562 [00:05<4:19:09,  4.92it/s]\u001b[A\n",
            "  0%|          | 28/76562 [00:05<4:24:49,  4.82it/s]\u001b[A\n",
            "  0%|          | 29/76562 [00:06<4:21:08,  4.88it/s]\u001b[A\n",
            "  0%|          | 30/76562 [00:06<4:41:50,  4.53it/s]\u001b[A\n",
            "  0%|          | 31/76562 [00:06<4:36:56,  4.61it/s]\u001b[A\n",
            "  0%|          | 32/76562 [00:06<4:40:36,  4.55it/s]\u001b[A\n",
            "  0%|          | 33/76562 [00:07<5:18:57,  4.00it/s]\u001b[A\n",
            "  0%|          | 34/76562 [00:07<5:21:36,  3.97it/s]\u001b[A\n",
            "  0%|          | 35/76562 [00:07<5:27:45,  3.89it/s]\u001b[A\n",
            "  0%|          | 36/76562 [00:07<5:32:12,  3.84it/s]\u001b[A\n",
            "  0%|          | 37/76562 [00:08<5:25:34,  3.92it/s]\u001b[A\n",
            "  0%|          | 38/76562 [00:08<4:49:45,  4.40it/s]\u001b[A\n",
            "  0%|          | 39/76562 [00:08<4:30:12,  4.72it/s]\u001b[A\n",
            "  0%|          | 40/76562 [00:08<4:55:32,  4.32it/s]\u001b[A\n",
            "  0%|          | 41/76562 [00:08<4:39:36,  4.56it/s]\u001b[A\n",
            "  0%|          | 42/76562 [00:09<4:57:03,  4.29it/s]\u001b[A\n",
            "  0%|          | 43/76562 [00:09<4:52:35,  4.36it/s]\u001b[A\n",
            "  0%|          | 44/76562 [00:09<4:38:13,  4.58it/s]\u001b[A\n",
            "  0%|          | 45/76562 [00:09<4:37:20,  4.60it/s]\u001b[A\n",
            "  0%|          | 46/76562 [00:10<4:43:49,  4.49it/s]\u001b[A\n",
            "  0%|          | 47/76562 [00:10<4:45:56,  4.46it/s]\u001b[A\n",
            "  0%|          | 48/76562 [00:10<5:04:54,  4.18it/s]\u001b[A\n",
            "  0%|          | 49/76562 [00:10<4:55:57,  4.31it/s]\u001b[A\n",
            "  0%|          | 50/76562 [00:11<4:58:05,  4.28it/s]\u001b[A\n",
            "  0%|          | 51/76562 [00:11<4:54:47,  4.33it/s]\u001b[A\n",
            "  0%|          | 52/76562 [00:11<5:13:18,  4.07it/s]\u001b[A\n",
            "  0%|          | 53/76562 [00:11<5:44:46,  3.70it/s]\u001b[A\n",
            "  0%|          | 54/76562 [00:12<6:15:19,  3.40it/s]\u001b[A\n",
            "  0%|          | 55/76562 [00:12<6:19:49,  3.36it/s]\u001b[A\n",
            "  0%|          | 56/76562 [00:12<5:48:09,  3.66it/s]\u001b[A\n",
            "  0%|          | 57/76562 [00:13<6:01:43,  3.53it/s]\u001b[A\n",
            "  0%|          | 58/76562 [00:13<5:50:07,  3.64it/s]\u001b[A\n",
            "  0%|          | 59/76562 [00:13<5:26:07,  3.91it/s]\u001b[A\n",
            "  0%|          | 60/76562 [00:13<4:48:39,  4.42it/s]\u001b[A\n",
            "  0%|          | 61/76562 [00:13<4:13:27,  5.03it/s]\u001b[A\n",
            "  0%|          | 62/76562 [00:14<4:20:48,  4.89it/s]\u001b[A\n",
            "  0%|          | 63/76562 [00:14<4:23:58,  4.83it/s]\u001b[A\n",
            "  0%|          | 64/76562 [00:14<4:32:46,  4.67it/s]\u001b[A\n",
            "  0%|          | 65/76562 [00:14<4:44:37,  4.48it/s]\u001b[A\n",
            "  0%|          | 66/76562 [00:14<4:46:30,  4.45it/s]\u001b[A\n",
            "  0%|          | 67/76562 [00:15<4:34:19,  4.65it/s]\u001b[A\n",
            "  0%|          | 68/76562 [00:15<4:50:29,  4.39it/s]\u001b[A\n",
            "  0%|          | 69/76562 [00:15<4:34:35,  4.64it/s]\u001b[A\n",
            "  0%|          | 70/76562 [00:15<4:35:15,  4.63it/s]\u001b[A\n",
            "  0%|          | 71/76562 [00:16<5:15:32,  4.04it/s]\u001b[A\n",
            "  0%|          | 72/76562 [00:16<5:29:25,  3.87it/s]\u001b[A\n",
            "  0%|          | 73/76562 [00:16<5:20:28,  3.98it/s]\u001b[A\n",
            "  0%|          | 74/76562 [00:16<5:11:40,  4.09it/s]\u001b[A\n",
            "  0%|          | 75/76562 [00:17<4:48:09,  4.42it/s]\u001b[A\n",
            "  0%|          | 76/76562 [00:17<4:49:53,  4.40it/s]\u001b[A\n",
            "  0%|          | 77/76562 [00:17<5:18:07,  4.01it/s]\u001b[A\n",
            "  0%|          | 78/76562 [00:17<5:39:39,  3.75it/s]\u001b[A\n",
            "  0%|          | 79/76562 [00:18<5:17:09,  4.02it/s]\u001b[A\n",
            "  0%|          | 80/76562 [00:18<4:39:53,  4.55it/s]\u001b[A\n",
            "  0%|          | 81/76562 [00:18<4:36:07,  4.62it/s]\u001b[A\n",
            "  0%|          | 82/76562 [00:18<5:11:34,  4.09it/s]\u001b[A\n",
            "  0%|          | 83/76562 [00:19<5:41:13,  3.74it/s]\u001b[A\n",
            "  0%|          | 84/76562 [00:19<5:11:30,  4.09it/s]\u001b[A\n",
            "  0%|          | 85/76562 [00:19<4:37:30,  4.59it/s]\u001b[A\n",
            "  0%|          | 86/76562 [00:19<4:20:11,  4.90it/s]\u001b[A\n",
            "  0%|          | 87/76562 [00:19<4:40:25,  4.55it/s]\u001b[A\n",
            "  0%|          | 88/76562 [00:19<3:58:43,  5.34it/s]\u001b[A\n",
            "  0%|          | 89/76562 [00:20<3:53:31,  5.46it/s]\u001b[A\n",
            "  0%|          | 90/76562 [00:20<4:45:21,  4.47it/s]\u001b[A\n",
            "  0%|          | 91/76562 [00:20<4:38:49,  4.57it/s]\u001b[A\n",
            "  0%|          | 92/76562 [00:20<4:25:33,  4.80it/s]\u001b[A\n",
            "  0%|          | 93/76562 [00:21<4:14:10,  5.01it/s]\u001b[A\n",
            "  0%|          | 94/76562 [00:21<4:36:28,  4.61it/s]\u001b[A\n",
            "  0%|          | 95/76562 [00:21<4:58:39,  4.27it/s]\u001b[A\n",
            "  0%|          | 96/76562 [00:21<5:05:29,  4.17it/s]\u001b[A\n",
            "  0%|          | 97/76562 [00:22<5:43:26,  3.71it/s]\u001b[A\n",
            "  0%|          | 98/76562 [00:22<5:02:16,  4.22it/s]\u001b[A\n",
            "  0%|          | 99/76562 [00:22<4:35:38,  4.62it/s]\u001b[A\n",
            "  0%|          | 100/76562 [00:22<4:36:27,  4.61it/s]\u001b[A\n",
            "  0%|          | 101/76562 [00:22<4:30:34,  4.71it/s]\u001b[A\n",
            "  0%|          | 102/76562 [00:23<4:57:46,  4.28it/s]\u001b[A\n",
            "  0%|          | 103/76562 [00:23<5:15:57,  4.03it/s]\u001b[A\n",
            "  0%|          | 104/76562 [00:23<4:58:21,  4.27it/s]\u001b[A\n",
            "  0%|          | 105/76562 [00:23<4:42:19,  4.51it/s]\u001b[A\n",
            "  0%|          | 106/76562 [00:24<4:46:17,  4.45it/s]\u001b[A\n",
            "  0%|          | 107/76562 [00:24<4:24:17,  4.82it/s]\u001b[A\n",
            "  0%|          | 108/76562 [00:24<4:02:08,  5.26it/s]\u001b[A\n",
            "  0%|          | 109/76562 [00:24<4:14:11,  5.01it/s]\u001b[A\n",
            "  0%|          | 110/76562 [00:24<4:16:59,  4.96it/s]\u001b[A\n",
            "  0%|          | 111/76562 [00:25<4:50:18,  4.39it/s]\u001b[A\n",
            "  0%|          | 112/76562 [00:25<4:35:36,  4.62it/s]\u001b[A\n",
            "  0%|          | 113/76562 [00:25<4:27:36,  4.76it/s]\u001b[A\n",
            "  0%|          | 114/76562 [00:25<4:21:02,  4.88it/s]\u001b[A\n",
            "  0%|          | 115/76562 [00:25<4:06:20,  5.17it/s]\u001b[A\n",
            "  0%|          | 116/76562 [00:26<4:32:57,  4.67it/s]\u001b[A\n",
            "  0%|          | 117/76562 [00:26<5:00:47,  4.24it/s]\u001b[A\n",
            "  0%|          | 118/76562 [00:26<5:12:49,  4.07it/s]\u001b[A\n",
            "  0%|          | 119/76562 [00:26<5:14:01,  4.06it/s]\u001b[A\n",
            "  0%|          | 120/76562 [00:27<5:27:28,  3.89it/s]\u001b[A\n",
            "  0%|          | 121/76562 [00:27<4:50:11,  4.39it/s]\u001b[A\n",
            "  0%|          | 122/76562 [00:27<4:26:50,  4.77it/s]\u001b[A\n",
            "  0%|          | 123/76562 [00:27<4:33:20,  4.66it/s]\u001b[A\n",
            "  0%|          | 124/76562 [00:28<4:55:31,  4.31it/s]\u001b[A\n",
            "  0%|          | 125/76562 [00:28<5:13:02,  4.07it/s]\u001b[A\n",
            "  0%|          | 126/76562 [00:28<5:19:29,  3.99it/s]\u001b[A\n",
            "  0%|          | 127/76562 [00:28<5:23:37,  3.94it/s]\u001b[A\n",
            "  0%|          | 128/76562 [00:29<5:06:22,  4.16it/s]\u001b[A\n",
            "  0%|          | 129/76562 [00:29<5:15:01,  4.04it/s]\u001b[A\n",
            "  0%|          | 130/76562 [00:29<4:49:02,  4.41it/s]\u001b[A\n",
            "  0%|          | 131/76562 [00:29<4:37:18,  4.59it/s]\u001b[A\n",
            "  0%|          | 132/76562 [00:29<4:27:54,  4.75it/s]\u001b[A\n",
            "  0%|          | 133/76562 [00:30<4:49:02,  4.41it/s]\u001b[A\n",
            "  0%|          | 134/76562 [00:30<5:11:13,  4.09it/s]\u001b[A\n",
            "  0%|          | 135/76562 [00:30<5:17:13,  4.02it/s]\u001b[A\n",
            "  0%|          | 136/76562 [00:30<5:09:52,  4.11it/s]\u001b[A\n",
            "  0%|          | 137/76562 [00:31<5:51:22,  3.63it/s]\u001b[A\n",
            "  0%|          | 138/76562 [00:31<5:40:10,  3.74it/s]\u001b[A\n",
            "  0%|          | 139/76562 [00:31<6:05:30,  3.48it/s]\u001b[A\n",
            "  0%|          | 140/76562 [00:32<5:40:30,  3.74it/s]\u001b[A\n",
            "  0%|          | 141/76562 [00:32<4:51:43,  4.37it/s]\u001b[A\n",
            "  0%|          | 142/76562 [00:32<4:31:11,  4.70it/s]\u001b[A\n",
            "  0%|          | 143/76562 [00:32<4:23:15,  4.84it/s]\u001b[A\n",
            "  0%|          | 144/76562 [00:32<4:30:18,  4.71it/s]\u001b[A\n",
            "  0%|          | 145/76562 [00:33<5:06:55,  4.15it/s]\u001b[A\n",
            "  0%|          | 146/76562 [00:33<5:01:07,  4.23it/s]\u001b[A\n",
            "  0%|          | 147/76562 [00:33<4:51:07,  4.37it/s]\u001b[A\n",
            "  0%|          | 148/76562 [00:33<4:37:12,  4.59it/s]\u001b[A\n",
            "  0%|          | 149/76562 [00:34<4:54:45,  4.32it/s]\u001b[A\n",
            "  0%|          | 150/76562 [00:34<5:20:14,  3.98it/s]\u001b[A\n",
            "  0%|          | 151/76562 [00:34<5:26:04,  3.91it/s]\u001b[A\n",
            "  0%|          | 152/76562 [00:34<5:05:50,  4.16it/s]\u001b[A\n",
            "  0%|          | 153/76562 [00:34<4:47:24,  4.43it/s]\u001b[A\n",
            "  0%|          | 154/76562 [00:35<4:32:57,  4.67it/s]\u001b[A\n",
            "  0%|          | 155/76562 [00:35<4:19:10,  4.91it/s]\u001b[A\n",
            "  0%|          | 156/76562 [00:35<4:09:17,  5.11it/s]\u001b[A\n",
            "  0%|          | 157/76562 [00:35<3:52:51,  5.47it/s]\u001b[A\n",
            "  0%|          | 158/76562 [00:35<4:06:53,  5.16it/s]\u001b[A\n",
            "  0%|          | 159/76562 [00:36<4:43:18,  4.49it/s]\u001b[A\n",
            "  0%|          | 160/76562 [00:36<5:07:29,  4.14it/s]\u001b[A\n",
            "  0%|          | 161/76562 [00:36<4:58:20,  4.27it/s]\u001b[A\n",
            "  0%|          | 162/76562 [00:36<4:53:21,  4.34it/s]\u001b[A\n",
            "  0%|          | 163/76562 [00:37<4:58:18,  4.27it/s]\u001b[A\n",
            "  0%|          | 164/76562 [00:37<4:45:35,  4.46it/s]\u001b[A\n",
            "  0%|          | 165/76562 [00:37<4:32:34,  4.67it/s]\u001b[A\n",
            "  0%|          | 166/76562 [00:37<4:29:06,  4.73it/s]\u001b[A\n",
            "  0%|          | 167/76562 [00:37<4:15:10,  4.99it/s]\u001b[A\n",
            "  0%|          | 168/76562 [00:38<4:02:57,  5.24it/s]\u001b[A\n",
            "  0%|          | 169/76562 [00:38<4:04:38,  5.20it/s]\u001b[A\n",
            "  0%|          | 170/76562 [00:38<4:22:51,  4.84it/s]\u001b[A\n",
            "  0%|          | 171/76562 [00:38<4:24:55,  4.81it/s]\u001b[A\n",
            "  0%|          | 172/76562 [00:38<4:49:13,  4.40it/s]\u001b[A\n",
            "  0%|          | 173/76562 [00:39<4:29:16,  4.73it/s]\u001b[A\n",
            "  0%|          | 174/76562 [00:39<4:30:48,  4.70it/s]\u001b[A\n",
            "  0%|          | 175/76562 [00:39<4:38:17,  4.57it/s]\u001b[A\n",
            "  0%|          | 176/76562 [00:39<4:38:44,  4.57it/s]\u001b[A\n",
            "  0%|          | 177/76562 [00:40<4:18:40,  4.92it/s]\u001b[A\n",
            "  0%|          | 178/76562 [00:40<4:14:09,  5.01it/s]\u001b[A\n",
            "  0%|          | 179/76562 [00:40<4:36:17,  4.61it/s]\u001b[A\n",
            "  0%|          | 180/76562 [00:40<4:24:12,  4.82it/s]\u001b[A\n",
            "  0%|          | 181/76562 [00:40<4:25:01,  4.80it/s]\u001b[A\n",
            "  0%|          | 182/76562 [00:41<4:35:43,  4.62it/s]\u001b[A\n",
            "  0%|          | 183/76562 [00:41<4:28:44,  4.74it/s]\u001b[A\n",
            "  0%|          | 184/76562 [00:41<4:33:54,  4.65it/s]\u001b[A\n",
            "  0%|          | 185/76562 [00:41<4:29:47,  4.72it/s]\u001b[A\n",
            "  0%|          | 186/76562 [00:41<4:22:52,  4.84it/s]\u001b[A\n",
            "  0%|          | 187/76562 [00:42<4:39:06,  4.56it/s]\u001b[A\n",
            "  0%|          | 188/76562 [00:42<5:42:06,  3.72it/s]\u001b[A\n",
            "  0%|          | 189/76562 [00:42<6:07:32,  3.46it/s]\u001b[A\n",
            "  0%|          | 190/76562 [00:43<5:27:52,  3.88it/s]\u001b[A\n",
            "  0%|          | 191/76562 [00:43<4:56:06,  4.30it/s]\u001b[A\n",
            "  0%|          | 192/76562 [00:43<4:27:14,  4.76it/s]\u001b[A\n",
            "  0%|          | 193/76562 [00:43<4:03:31,  5.23it/s]\u001b[A\n",
            "  0%|          | 194/76562 [00:43<4:08:50,  5.11it/s]\u001b[A\n",
            "  0%|          | 195/76562 [00:43<4:05:39,  5.18it/s]\u001b[A\n",
            "  0%|          | 196/76562 [00:44<3:58:22,  5.34it/s]\u001b[A\n",
            "  0%|          | 197/76562 [00:44<3:48:51,  5.56it/s]\u001b[A\n",
            "  0%|          | 198/76562 [00:44<3:47:09,  5.60it/s]\u001b[A\n",
            "  0%|          | 199/76562 [00:44<3:56:20,  5.39it/s]\u001b[A\n",
            "  0%|          | 200/76562 [00:44<4:01:15,  5.28it/s]\u001b[A\n",
            "  0%|          | 201/76562 [00:45<4:27:47,  4.75it/s]\u001b[A\n",
            "  0%|          | 202/76562 [00:45<4:27:21,  4.76it/s]\u001b[A\n",
            "  0%|          | 203/76562 [00:45<4:18:53,  4.92it/s]\u001b[A\n",
            "  0%|          | 204/76562 [00:45<4:15:33,  4.98it/s]\u001b[A\n",
            "  0%|          | 205/76562 [00:45<4:07:10,  5.15it/s]\u001b[A\n",
            "  0%|          | 206/76562 [00:46<4:25:32,  4.79it/s]\u001b[A\n",
            "  0%|          | 207/76562 [00:46<4:32:45,  4.67it/s]\u001b[A\n",
            "  0%|          | 208/76562 [00:46<4:58:40,  4.26it/s]\u001b[A\n",
            "  0%|          | 209/76562 [00:46<4:33:53,  4.65it/s]\u001b[A\n",
            "  0%|          | 210/76562 [00:47<4:44:43,  4.47it/s]\u001b[A\n",
            "  0%|          | 211/76562 [00:47<4:50:37,  4.38it/s]\u001b[A\n",
            "  0%|          | 212/76562 [00:47<4:37:24,  4.59it/s]\u001b[A\n",
            "  0%|          | 213/76562 [00:47<4:04:54,  5.20it/s]\u001b[A\n",
            "  0%|          | 214/76562 [00:47<4:19:13,  4.91it/s]\u001b[A\n",
            "  0%|          | 215/76562 [00:48<4:11:07,  5.07it/s]\u001b[A\n",
            "  0%|          | 216/76562 [00:48<4:11:50,  5.05it/s]\u001b[A\n",
            "  0%|          | 217/76562 [00:48<4:16:31,  4.96it/s]\u001b[A\n",
            "  0%|          | 218/76562 [00:48<4:15:17,  4.98it/s]\u001b[A\n",
            "  0%|          | 219/76562 [00:48<4:09:19,  5.10it/s]\u001b[A\n",
            "  0%|          | 220/76562 [00:48<4:01:33,  5.27it/s]\u001b[A\n",
            "  0%|          | 221/76562 [00:49<4:27:21,  4.76it/s]\u001b[A\n",
            "  0%|          | 222/76562 [00:49<5:07:17,  4.14it/s]\u001b[A\n",
            "  0%|          | 223/76562 [00:49<4:50:17,  4.38it/s]\u001b[A\n",
            "  0%|          | 224/76562 [00:50<5:12:08,  4.08it/s]\u001b[A\n",
            "  0%|          | 225/76562 [00:50<5:47:00,  3.67it/s]\u001b[A\n",
            "  0%|          | 226/76562 [00:50<5:24:08,  3.93it/s]\u001b[A\n",
            "  0%|          | 227/76562 [00:50<4:57:17,  4.28it/s]\u001b[A\n",
            "  0%|          | 228/76562 [00:50<4:49:01,  4.40it/s]\u001b[A\n",
            "  0%|          | 229/76562 [00:51<5:00:27,  4.23it/s]\u001b[A\n",
            "  0%|          | 230/76562 [00:51<4:45:01,  4.46it/s]\u001b[A\n",
            "  0%|          | 231/76562 [00:51<4:29:10,  4.73it/s]\u001b[A\n",
            "  0%|          | 232/76562 [00:51<4:25:51,  4.79it/s]\u001b[A\n",
            "  0%|          | 233/76562 [00:52<4:37:01,  4.59it/s]\u001b[A\n",
            "  0%|          | 234/76562 [00:52<4:38:49,  4.56it/s]\u001b[A\n",
            "  0%|          | 235/76562 [00:52<4:50:48,  4.37it/s]\u001b[A\n",
            "  0%|          | 236/76562 [00:52<5:20:34,  3.97it/s]\u001b[A\n",
            "  0%|          | 237/76562 [00:53<5:21:08,  3.96it/s]\u001b[A\n",
            "  0%|          | 238/76562 [00:53<5:26:36,  3.89it/s]\u001b[A\n",
            "  0%|          | 239/76562 [00:53<4:58:49,  4.26it/s]\u001b[A\n",
            "  0%|          | 240/76562 [00:53<4:28:13,  4.74it/s]\u001b[A\n",
            "  0%|          | 241/76562 [00:53<4:34:28,  4.63it/s]\u001b[A\n",
            "  0%|          | 242/76562 [00:54<4:54:55,  4.31it/s]\u001b[A\n",
            "  0%|          | 243/76562 [00:54<4:51:56,  4.36it/s]\u001b[A\n",
            "  0%|          | 244/76562 [00:54<5:06:08,  4.15it/s]\u001b[A\n",
            "  0%|          | 245/76562 [00:54<4:35:26,  4.62it/s]\u001b[A\n",
            "  0%|          | 246/76562 [00:55<4:18:44,  4.92it/s]\u001b[A\n",
            "  0%|          | 247/76562 [00:55<4:19:38,  4.90it/s]\u001b[A\n",
            "  0%|          | 248/76562 [00:55<5:07:49,  4.13it/s]\u001b[A\n",
            "  0%|          | 249/76562 [00:55<5:12:23,  4.07it/s]\u001b[A\n",
            "  0%|          | 250/76562 [00:56<5:03:22,  4.19it/s]\u001b[A\n",
            "  0%|          | 251/76562 [00:56<4:45:41,  4.45it/s]\u001b[A\n",
            "  0%|          | 252/76562 [00:56<4:35:21,  4.62it/s]\u001b[A\n",
            "  0%|          | 253/76562 [00:56<4:52:13,  4.35it/s]\u001b[A\n",
            "  0%|          | 254/76562 [00:56<4:49:52,  4.39it/s]\u001b[A\n",
            "  0%|          | 255/76562 [00:57<4:48:39,  4.41it/s]\u001b[A\n",
            "  0%|          | 256/76562 [00:57<4:56:40,  4.29it/s]\u001b[A\n",
            "  0%|          | 257/76562 [00:57<5:03:17,  4.19it/s]\u001b[A\n",
            "  0%|          | 258/76562 [00:57<4:47:54,  4.42it/s]\u001b[A\n",
            "  0%|          | 259/76562 [00:58<5:30:32,  3.85it/s]\u001b[A\n",
            "  0%|          | 260/76562 [00:58<5:17:09,  4.01it/s]\u001b[A\n",
            "  0%|          | 261/76562 [00:58<5:19:14,  3.98it/s]\u001b[A\n",
            "  0%|          | 262/76562 [00:58<5:33:01,  3.82it/s]\u001b[A\n",
            "  0%|          | 263/76562 [00:59<4:51:05,  4.37it/s]\u001b[A\n",
            "  0%|          | 264/76562 [00:59<4:37:54,  4.58it/s]\u001b[A\n",
            "  0%|          | 265/76562 [00:59<4:15:50,  4.97it/s]\u001b[A\n",
            "  0%|          | 266/76562 [00:59<4:00:06,  5.30it/s]\u001b[A\n",
            "  0%|          | 267/76562 [00:59<3:52:39,  5.47it/s]\u001b[A\n",
            "  0%|          | 268/76562 [00:59<3:58:42,  5.33it/s]\u001b[A\n",
            "  0%|          | 269/76562 [01:00<4:03:31,  5.22it/s]\u001b[A\n",
            "  0%|          | 270/76562 [01:00<4:18:51,  4.91it/s]\u001b[A\n",
            "  0%|          | 271/76562 [01:00<4:56:44,  4.28it/s]\u001b[A\n",
            "  0%|          | 272/76562 [01:00<4:59:31,  4.25it/s]\u001b[A\n",
            "  0%|          | 273/76562 [01:01<5:05:39,  4.16it/s]\u001b[A\n",
            "  0%|          | 274/76562 [01:01<4:48:51,  4.40it/s]\u001b[A\n",
            "  0%|          | 275/76562 [01:01<4:40:12,  4.54it/s]\u001b[A\n",
            "  0%|          | 276/76562 [01:01<4:39:24,  4.55it/s]\u001b[A\n",
            "  0%|          | 277/76562 [01:02<5:20:03,  3.97it/s]\u001b[A\n",
            "  0%|          | 278/76562 [01:02<5:24:49,  3.91it/s]\u001b[A\n",
            "  0%|          | 279/76562 [01:02<5:11:14,  4.08it/s]\u001b[A\n",
            "  0%|          | 280/76562 [01:02<5:18:52,  3.99it/s]\u001b[A\n",
            "  0%|          | 281/76562 [01:03<4:47:21,  4.42it/s]\u001b[A\n",
            "  0%|          | 282/76562 [01:03<4:42:31,  4.50it/s]\u001b[A\n",
            "  0%|          | 283/76562 [01:03<4:50:26,  4.38it/s]\u001b[A\n",
            "  0%|          | 284/76562 [01:03<4:44:11,  4.47it/s]\u001b[A\n",
            "  0%|          | 285/76562 [01:03<4:39:16,  4.55it/s]\u001b[A\n",
            "  0%|          | 286/76562 [01:04<4:23:55,  4.82it/s]\u001b[A\n",
            "  0%|          | 287/76562 [01:04<4:35:02,  4.62it/s]\u001b[A\n",
            "  0%|          | 288/76562 [01:04<4:20:00,  4.89it/s]\u001b[A\n",
            "  0%|          | 289/76562 [01:04<4:17:02,  4.95it/s]\u001b[A\n",
            "  0%|          | 290/76562 [01:04<4:13:36,  5.01it/s]\u001b[A\n",
            "  0%|          | 291/76562 [01:05<4:32:21,  4.67it/s]\u001b[A\n",
            "  0%|          | 292/76562 [01:05<4:32:19,  4.67it/s]\u001b[A\n",
            "  0%|          | 293/76562 [01:05<4:11:42,  5.05it/s]\u001b[A\n",
            "  0%|          | 294/76562 [01:05<4:19:18,  4.90it/s]\u001b[A\n",
            "  0%|          | 295/76562 [01:06<4:36:33,  4.60it/s]\u001b[A\n",
            "  0%|          | 296/76562 [01:06<4:39:19,  4.55it/s]\u001b[A\n",
            "  0%|          | 297/76562 [01:06<4:19:17,  4.90it/s]\u001b[A\n",
            "  0%|          | 298/76562 [01:06<4:34:39,  4.63it/s]\u001b[A\n",
            "  0%|          | 299/76562 [01:06<4:35:31,  4.61it/s]\u001b[A\n",
            "  0%|          | 300/76562 [01:07<4:58:03,  4.26it/s]\u001b[A\n",
            "  0%|          | 301/76562 [01:07<4:39:21,  4.55it/s]\u001b[A\n",
            "  0%|          | 302/76562 [01:07<4:40:06,  4.54it/s]\u001b[A\n",
            "  0%|          | 303/76562 [01:07<5:03:29,  4.19it/s]\u001b[A\n",
            "  0%|          | 304/76562 [01:08<5:04:02,  4.18it/s]\u001b[A\n",
            "  0%|          | 305/76562 [01:08<5:09:35,  4.11it/s]\u001b[A\n",
            "  0%|          | 306/76562 [01:08<4:57:43,  4.27it/s]\u001b[A\n",
            "  0%|          | 307/76562 [01:08<4:59:26,  4.24it/s]\u001b[A\n",
            "  0%|          | 308/76562 [01:09<5:38:49,  3.75it/s]\u001b[A\n",
            "  0%|          | 309/76562 [01:09<5:06:10,  4.15it/s]\u001b[A\n",
            "  0%|          | 310/76562 [01:09<4:44:21,  4.47it/s]\u001b[A\n",
            "  0%|          | 311/76562 [01:09<4:36:10,  4.60it/s]\u001b[A\n",
            "  0%|          | 312/76562 [01:09<4:24:07,  4.81it/s]\u001b[A\n",
            "  0%|          | 313/76562 [01:10<4:16:27,  4.96it/s]\u001b[A\n",
            "  0%|          | 314/76562 [01:10<4:16:34,  4.95it/s]\u001b[A\n",
            "  0%|          | 315/76562 [01:10<4:07:39,  5.13it/s]\u001b[A\n",
            "  0%|          | 316/76562 [01:10<4:15:21,  4.98it/s]\u001b[A\n",
            "  0%|          | 317/76562 [01:10<3:52:57,  5.45it/s]\u001b[A\n",
            "  0%|          | 318/76562 [01:10<3:49:14,  5.54it/s]\u001b[A\n",
            "  0%|          | 319/76562 [01:11<3:35:19,  5.90it/s]\u001b[A\n",
            "  0%|          | 320/76562 [01:11<3:43:00,  5.70it/s]\u001b[A\n",
            "  0%|          | 321/76562 [01:11<3:53:07,  5.45it/s]\u001b[A\n",
            "  0%|          | 322/76562 [01:11<3:43:04,  5.70it/s]\u001b[A\n",
            "  0%|          | 323/76562 [01:11<3:57:20,  5.35it/s]\u001b[A\n",
            "  0%|          | 324/76562 [01:12<4:45:09,  4.46it/s]\u001b[A\n",
            "  0%|          | 325/76562 [01:12<4:51:34,  4.36it/s]\u001b[A\n",
            "  0%|          | 326/76562 [01:12<4:54:35,  4.31it/s]\u001b[A\n",
            "  0%|          | 327/76562 [01:12<5:02:06,  4.21it/s]\u001b[A\n",
            "  0%|          | 328/76562 [01:13<4:32:33,  4.66it/s]\u001b[A\n",
            "  0%|          | 329/76562 [01:13<4:08:48,  5.11it/s]\u001b[A\n",
            "  0%|          | 330/76562 [01:13<4:05:45,  5.17it/s]\u001b[A\n",
            "  0%|          | 331/76562 [01:13<3:45:37,  5.63it/s]\u001b[A\n",
            "  0%|          | 332/76562 [01:13<3:38:43,  5.81it/s]\u001b[A\n",
            "  0%|          | 333/76562 [01:13<3:35:56,  5.88it/s]\u001b[A\n",
            "  0%|          | 334/76562 [01:14<3:39:47,  5.78it/s]\u001b[A\n",
            "  0%|          | 335/76562 [01:14<4:25:26,  4.79it/s]\u001b[A\n",
            "  0%|          | 336/76562 [01:14<4:57:36,  4.27it/s]\u001b[A\n",
            "  0%|          | 337/76562 [01:14<4:56:07,  4.29it/s]\u001b[A\n",
            "  0%|          | 338/76562 [01:15<4:48:18,  4.41it/s]\u001b[A\n",
            "  0%|          | 339/76562 [01:15<4:36:14,  4.60it/s]\u001b[A\n",
            "  0%|          | 340/76562 [01:15<4:24:46,  4.80it/s]\u001b[A\n",
            "  0%|          | 341/76562 [01:15<4:10:09,  5.08it/s]\u001b[A\n",
            "  0%|          | 342/76562 [01:15<4:16:19,  4.96it/s]\u001b[A\n",
            "  0%|          | 343/76562 [01:16<4:26:20,  4.77it/s]\u001b[A\n",
            "  0%|          | 344/76562 [01:16<5:07:38,  4.13it/s]\u001b[A\n",
            "  0%|          | 345/76562 [01:16<4:39:31,  4.54it/s]\u001b[A\n",
            "  0%|          | 346/76562 [01:16<4:19:59,  4.89it/s]\u001b[A\n",
            "  0%|          | 347/76562 [01:16<4:26:32,  4.77it/s]\u001b[A\n",
            "  0%|          | 348/76562 [01:17<4:11:29,  5.05it/s]\u001b[A\n",
            "  0%|          | 349/76562 [01:17<4:15:45,  4.97it/s]\u001b[A\n",
            "  0%|          | 350/76562 [01:17<4:43:21,  4.48it/s]\u001b[A\n",
            "  0%|          | 351/76562 [01:17<5:11:54,  4.07it/s]\u001b[A\n",
            "  0%|          | 352/76562 [01:18<4:58:43,  4.25it/s]\u001b[A\n",
            "  0%|          | 353/76562 [01:18<4:58:52,  4.25it/s]\u001b[A\n",
            "  0%|          | 354/76562 [01:18<5:15:47,  4.02it/s]\u001b[A\n",
            "  0%|          | 355/76562 [01:18<5:01:15,  4.22it/s]\u001b[A\n",
            "  0%|          | 356/76562 [01:19<5:16:49,  4.01it/s]\u001b[A\n",
            "  0%|          | 357/76562 [01:19<4:51:28,  4.36it/s]\u001b[A\n",
            "  0%|          | 358/76562 [01:19<4:49:55,  4.38it/s]\u001b[A\n",
            "  0%|          | 359/76562 [01:19<4:36:32,  4.59it/s]\u001b[A\n",
            "  0%|          | 360/76562 [01:19<4:11:54,  5.04it/s]\u001b[A\n",
            "  0%|          | 361/76562 [01:20<4:10:42,  5.07it/s]\u001b[A\n",
            "  0%|          | 362/76562 [01:20<4:39:22,  4.55it/s]\u001b[A\n",
            "  0%|          | 363/76562 [01:20<5:24:47,  3.91it/s]\u001b[A\n",
            "  0%|          | 364/76562 [01:20<5:16:47,  4.01it/s]\u001b[A\n",
            "  0%|          | 365/76562 [01:21<5:43:20,  3.70it/s]\u001b[A\n",
            "  0%|          | 366/76562 [01:21<5:35:42,  3.78it/s]\u001b[A\n",
            "  0%|          | 367/76562 [01:21<5:15:31,  4.02it/s]\u001b[A\n",
            "  0%|          | 368/76562 [01:21<5:18:19,  3.99it/s]\u001b[A\n",
            "  0%|          | 369/76562 [01:22<5:43:01,  3.70it/s]\u001b[A\n",
            "  0%|          | 370/76562 [01:22<5:28:22,  3.87it/s]\u001b[A\n",
            "  0%|          | 371/76562 [01:22<5:48:17,  3.65it/s]\u001b[A\n",
            "  0%|          | 372/76562 [01:23<5:15:52,  4.02it/s]\u001b[A\n",
            "  0%|          | 373/76562 [01:23<5:10:59,  4.08it/s]\u001b[A\n",
            "  0%|          | 374/76562 [01:23<5:25:16,  3.90it/s]\u001b[A\n",
            "  0%|          | 375/76562 [01:23<5:29:33,  3.85it/s]\u001b[A\n",
            "  0%|          | 376/76562 [01:23<5:05:51,  4.15it/s]\u001b[A\n",
            "  0%|          | 377/76562 [01:24<4:43:22,  4.48it/s]\u001b[A\n",
            "  0%|          | 378/76562 [01:24<5:01:02,  4.22it/s]\u001b[A\n",
            "  0%|          | 379/76562 [01:24<4:38:30,  4.56it/s]\u001b[A\n",
            "  0%|          | 380/76562 [01:24<4:22:50,  4.83it/s]\u001b[A\n",
            "  0%|          | 381/76562 [01:25<4:43:15,  4.48it/s]\u001b[A\n",
            "  0%|          | 382/76562 [01:25<4:50:07,  4.38it/s]\u001b[A\n",
            "  1%|          | 383/76562 [01:25<5:08:44,  4.11it/s]\u001b[A\n",
            "  1%|          | 384/76562 [01:25<4:34:07,  4.63it/s]\u001b[A\n",
            "  1%|          | 385/76562 [01:25<4:18:01,  4.92it/s]\u001b[A\n",
            "  1%|          | 386/76562 [01:26<4:20:41,  4.87it/s]\u001b[A\n",
            "  1%|          | 387/76562 [01:26<4:04:42,  5.19it/s]\u001b[A\n",
            "  1%|          | 388/76562 [01:26<4:35:49,  4.60it/s]\u001b[A\n",
            "  1%|          | 389/76562 [01:26<5:05:55,  4.15it/s]\u001b[A\n",
            "  1%|          | 390/76562 [01:27<4:51:16,  4.36it/s]\u001b[A\n",
            "  1%|          | 391/76562 [01:27<4:26:15,  4.77it/s]\u001b[A\n",
            "  1%|          | 392/76562 [01:27<4:15:17,  4.97it/s]\u001b[A\n",
            "  1%|          | 393/76562 [01:27<4:14:18,  4.99it/s]\u001b[A\n",
            "  1%|          | 394/76562 [01:27<4:31:27,  4.68it/s]\u001b[A\n",
            "  1%|          | 395/76562 [01:28<4:24:34,  4.80it/s]\u001b[A\n",
            "  1%|          | 396/76562 [01:28<4:13:52,  5.00it/s]\u001b[A\n",
            "  1%|          | 397/76562 [01:28<4:43:50,  4.47it/s]\u001b[A\n",
            "  1%|          | 398/76562 [01:28<4:38:44,  4.55it/s]\u001b[A\n",
            "  1%|          | 399/76562 [01:28<4:21:43,  4.85it/s]\u001b[A\n",
            "  1%|          | 400/76562 [01:29<4:12:59,  5.02it/s]\u001b[A\n",
            "  1%|          | 401/76562 [01:29<4:37:46,  4.57it/s]\u001b[A\n",
            "  1%|          | 402/76562 [01:29<4:47:38,  4.41it/s]\u001b[A\n",
            "  1%|          | 403/76562 [01:29<4:34:06,  4.63it/s]\u001b[A\n",
            "  1%|          | 404/76562 [01:29<4:06:22,  5.15it/s]\u001b[A\n",
            "  1%|          | 405/76562 [01:30<3:50:35,  5.50it/s]\u001b[A\n",
            "  1%|          | 406/76562 [01:30<3:38:11,  5.82it/s]\u001b[A\n",
            "  1%|          | 407/76562 [01:30<3:56:12,  5.37it/s]\u001b[A\n",
            "  1%|          | 408/76562 [01:30<3:54:01,  5.42it/s]\u001b[A\n",
            "  1%|          | 409/76562 [01:30<3:51:44,  5.48it/s]\u001b[A\n",
            "  1%|          | 410/76562 [01:30<3:47:18,  5.58it/s]\u001b[A\n",
            "  1%|          | 411/76562 [01:31<3:51:43,  5.48it/s]\u001b[A\n",
            "  1%|          | 412/76562 [01:31<3:51:02,  5.49it/s]\u001b[A\n",
            "  1%|          | 413/76562 [01:31<4:01:37,  5.25it/s]\u001b[A\n",
            "  1%|          | 414/76562 [01:31<4:11:23,  5.05it/s]\u001b[A\n",
            "  1%|          | 415/76562 [01:31<4:11:34,  5.04it/s]\u001b[A\n",
            "  1%|          | 416/76562 [01:32<4:40:53,  4.52it/s]\u001b[A\n",
            "  1%|          | 417/76562 [01:32<4:56:10,  4.28it/s]\u001b[A\n",
            "  1%|          | 418/76562 [01:32<5:00:58,  4.22it/s]\u001b[A\n",
            "  1%|          | 419/76562 [01:32<5:08:15,  4.12it/s]\u001b[A\n",
            "  1%|          | 420/76562 [01:33<5:10:44,  4.08it/s]\u001b[A\n",
            "  1%|          | 421/76562 [01:33<5:06:19,  4.14it/s]\u001b[A\n",
            "  1%|          | 422/76562 [01:33<5:12:30,  4.06it/s]\u001b[A\n",
            "  1%|          | 423/76562 [01:34<6:02:02,  3.51it/s]\u001b[A\n",
            "  1%|          | 424/76562 [01:34<6:48:05,  3.11it/s]\u001b[A\n",
            "  1%|          | 425/76562 [01:34<6:22:12,  3.32it/s]\u001b[A\n",
            "  1%|          | 426/76562 [01:35<6:38:17,  3.19it/s]\u001b[A\n",
            "  1%|          | 427/76562 [01:35<5:53:31,  3.59it/s]\u001b[A\n",
            "  1%|          | 428/76562 [01:35<5:26:19,  3.89it/s]\u001b[A\n",
            "  1%|          | 429/76562 [01:35<5:00:27,  4.22it/s]\u001b[A\n",
            "  1%|          | 430/76562 [01:35<4:26:48,  4.76it/s]\u001b[A\n",
            "  1%|          | 431/76562 [01:36<4:45:37,  4.44it/s]\u001b[A\n",
            "  1%|          | 432/76562 [01:36<5:37:11,  3.76it/s]\u001b[A\n",
            "  1%|          | 433/76562 [01:36<6:12:51,  3.40it/s]\u001b[A\n",
            "  1%|          | 434/76562 [01:37<6:04:26,  3.48it/s]\u001b[A\n",
            "  1%|          | 435/76562 [01:37<5:40:31,  3.73it/s]\u001b[A\n",
            "  1%|          | 436/76562 [01:37<4:58:52,  4.25it/s]\u001b[A\n",
            "  1%|          | 437/76562 [01:37<4:35:04,  4.61it/s]\u001b[A\n",
            "  1%|          | 438/76562 [01:37<4:10:58,  5.06it/s]\u001b[A\n",
            "  1%|          | 439/76562 [01:37<4:02:44,  5.23it/s]\u001b[A\n",
            "  1%|          | 440/76562 [01:38<4:08:50,  5.10it/s]\u001b[A\n",
            "  1%|          | 441/76562 [01:38<3:48:02,  5.56it/s]\u001b[A\n",
            "  1%|          | 442/76562 [01:38<3:44:53,  5.64it/s]\u001b[A\n",
            "  1%|          | 443/76562 [01:38<4:16:00,  4.96it/s]\u001b[A\n",
            "  1%|          | 444/76562 [01:39<5:01:26,  4.21it/s]\u001b[A\n",
            "  1%|          | 445/76562 [01:39<4:31:07,  4.68it/s]\u001b[A\n",
            "  1%|          | 446/76562 [01:39<4:51:17,  4.36it/s]\u001b[A\n",
            "  1%|          | 447/76562 [01:39<4:54:27,  4.31it/s]\u001b[A\n",
            "  1%|          | 448/76562 [01:39<4:51:10,  4.36it/s]\u001b[A\n",
            "  1%|          | 449/76562 [01:40<5:21:27,  3.95it/s]\u001b[A\n",
            "  1%|          | 450/76562 [01:40<4:54:05,  4.31it/s]\u001b[A\n",
            "  1%|          | 451/76562 [01:40<5:04:32,  4.17it/s]\u001b[A\n",
            "  1%|          | 452/76562 [01:41<5:19:06,  3.98it/s]\u001b[A\n",
            "  1%|          | 453/76562 [01:41<5:47:44,  3.65it/s]\u001b[A\n",
            "  1%|          | 454/76562 [01:41<5:20:38,  3.96it/s]\u001b[A\n",
            "  1%|          | 455/76562 [01:41<5:07:36,  4.12it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-82a31ef604b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mval_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtest_nll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-97eb1eaf3e05>\u001b[0m in \u001b[0;36m_run_epoch\u001b[0;34m(epoch, mode, display)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mopt_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kl_weight\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkl_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m## total loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drT84cGfhzWu"
      },
      "source": [
        "plt.plot(train_losses, 'b-', label=\"Train\")\n",
        "plt.plot(val_losses, 'g-', label=\"Validation\")\n",
        "plt.title(\"ControlVAE Text Generation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Negative ELBO\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYVl-Wp0h6NZ"
      },
      "source": [
        "plt.plot(beta)\n",
        "plt.title(\"ControlVAE Text Generation Beta Parameter\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Beta\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgc9JOzGGf-i"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBR2sxL7ryGn",
        "outputId": "f675ee57-3c46-45fd-9229-5ea0951ba48f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load from checkpoint\n",
        "save_path = './checkpoint.ckpt'\n",
        "model = VAE(config, len(vocab.itos), vocab.padding_index)\n",
        "ckpt = torch.load(save_path)\n",
        "model.load_state_dict(ckpt['model'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNUYkXKPGh3K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4021d795-d69a-446e-fec2-46cf116257ea"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "batch_size = config.batch_size\n",
        "\n",
        "test_sentence = 'The service was terrible.'\n",
        "tokens = ['<BOS>']+tokenizer(test_sentence)+['<EOS>']\n",
        "text_ids = torch.tensor([[vocab[token] for token in tokens]])\n",
        "mean, logvar = model.encode(text_ids.to(model.device), torch.tensor([len(tokens)]))\n",
        "dst = MultivariateNormalDiag(loc=mean[0], scale_diag=torch.exp(0.5 * logvar[0]))\n",
        "latent_z = dst.sample((batch_size,))\n",
        "\n",
        "# latent_z = torch.FloatTensor(batch_size, config.latent_dims).uniform_(-1, 1).to(device)\n",
        "# latent_z = torch.randn(batch_size, config.latent_dims).to(device)\n",
        "\n",
        "gen_text = None\n",
        "gen_chars = torch.tensor([[vocab.bos_index] for _ in range(batch_size)])\n",
        "seq_lengths = torch.zeros(batch_size).type(torch.LongTensor)\n",
        "not_done = torch.ones(batch_size).type(torch.LongTensor)\n",
        "state = None\n",
        "while gen_text is None or gen_text.size()[1] < 1000:\n",
        "    logits, state = model.decode(latent_z.to(model.device), gen_chars.to(model.device), torch.ones(batch_size), state)\n",
        "    cdst = Categorical(logits=logits)\n",
        "    gen_chars = cdst.sample()\n",
        "    not_done *= (gen_chars != vocab.eos_index).squeeze().type(torch.LongTensor)\n",
        "    seq_lengths += not_done\n",
        "    if gen_text is None:\n",
        "        gen_text = gen_chars.cpu()\n",
        "    else:\n",
        "        gen_text = torch.cat((gen_text, gen_chars.cpu()), dim=-1)\n",
        "    if torch.sum(not_done) == 0:\n",
        "        break\n",
        "for i,line in enumerate(gen_text):\n",
        "    print(' '.join([vocab.itos[line[j]] for j in range(seq_lengths[i])]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "always said i never recommend drift the pancakes , but\n",
            "the service is pretty good . it is friendly and very quiet . the place is average - it has two guys and i particularly really like they didn ' t eat alcohol suggesting . the service was pretty good . if it ' s nothing to stumble away without vegetables for lunch but still run out and just want new food . for a while it for the staff i could eat cleaning service and a good value .\n",
            "carne asada meal . they know what i have better at me way at the time i tell you get a drive thru .\n",
            "this place isn ' t very good . i ' ve seen at the area , the onions are small , if they you ordered the roof room . the beer was also good too . it had a club for 6 percent and pastor a juice to dance . the server itself was nice but nothing special . the brew , one out i don ' t like the onion rings , - fries and added macadamia . overall , <unk> and some good muffins .\n",
            "i wandered around this place recently and noticed me , i want to take myself . the del rey cajun chicken is fantastic ! america ' s with <unk> , the prices are missing . it put me right as a mile , i don ' t forget to go at the pants . the service was still over the and <unk> it tell me what i ' ve not tried . , i am <unk> about my eyes . if you want to go online in the back ' s , this is not what i mean is the restaurant from my mouth .\n",
            "the entree was a nice combo of barbecue for lunch after quite a couple of <unk> <unk> . the chicken was fantastic for my breakfast !\n",
            "returned to see our dad told him . for sure we were asked for seattle for good or cold filiberto .\n",
            "lovely was too many good guys around it was just so i had the banana machines ( shrimp and chocolate cake ) and i didn ' t seem to stop for my cousin . i cant be syrupy for this location for the week -\n",
            "tried this place to stop per place for lunch and bought a month on the comedy sandwich .\n",
            "judging voices coming back ! ) it gets divine to pick up a glass of different pulled . super cozy glass . excellent food = private pleasure as quick ! !\n",
            "so glad to keep the valley .\n",
            "went here here with my girlfriend and group <unk> left for a group of 7 years ago . i receive pad sports paul , make it off a drink for a buffet lunch or without quality service to come . unfortunately , i was impressed with the dismal here , it for this location because it was a great convenience for me and moved for our money and prices charged me service . i took a senior more entree , but it was extra crowded .\n",
            "portions united 2 food for pho . and we dropped off when they wait to even find food poisoning . he ' s 4/5 on the menu and hard with patrons orders !\n",
            "version of soggy cooked was served we which are cooked . my 1 burger burrito was okay and overcooked . but honestly if they mario hot serving is off it ill not bad . our eggs and putting 4 food combined . 15 minutes terrible we wont eat at the fast service for brunch . i eat when i drove there <unk> chewy get 0 .\n",
            "hot wok rocks ) they came out with all they split that it clean it time with herb bread on the hot sauce . i moved with a striking dish and another polo roll . i think it may seem a bit , it wasn ' t bad - refills it is good .\n",
            "antique transportation front to stay with kids throughout your job , than a lot of people who works to stay on sanctuary they typically stay here for from the own location , although they sent me free letters and shuttle travelers games . the sliders indicated actually martinis , half devils and drunk , lines , through <unk> , extra coffee , pool , celebrity , basketball and take swim , easy + air . swim on tv , they were blown away and keeping 2 00 . the accent is positively near the entire store , and easy to stay with customers for an appointment , john once to making sure it ' s polish and order .\n",
            "i think the food is mediocre , but not disappointing . quality service was ok , this is the waitress in front of their menu , and this place is on the menu . . . . came to a little bag and chili pork and they had bamboo pulled ( which is the special . ) . look . . . hmmm . . , dirty and good service . just make sure something overpowering things ordering with it and <unk> . they were starved on the menu . on the menu for the carne asada food and tomato and shrimp . i was pleasantly bland and waited for the appetizer and the maggiano cream . the burgers are <unk> , but not good . the tomatillo sits for me of the ingredients . the rice was ok , i ordered a woman on mac with provolone . or taste - a few steak for real meal now . maybe i doubt the meat was pork . my favorite pasta is truly fresh and had those bread with it . the broth was generous and i know it . the menu can be laid out and again . ) the atmosphere was quick , and efficient . if you ' re looking for another meal to say , the fried egg is beef , it can make this one mistake . . . i would have had tomatos and the bun tacos and that ' s is the coffee steak ! the sandwich was excellent and good . . . they ' re very good . some cheese <unk> , a few . i can eat the place and said stingray .\n",
            "drinks , each . sadly , this is probably the most amount of mouth-watering , but that ' s it ' s <unk> <unk> . decent service , their inexpensive brick system the phone was very large , and the guest seemed impressive , so i ' m ecstatic out of the time , and they have line . check something customers , , helpful , number , not your typical nonsense as our family and craft . now , they have an female cannoli , apparently you just needed to run out from your order , , certainly this place is because . it ' s extremely intimate to valley , and make sure they moved to our table . and that ' s good ? he will be <unk> here , so it isn ' t exactly .\n",
            "pin tutti desires upgrade castle meh if you must eat a dollar you thinking it is supposed to be look . no thing just feed this place .\n",
            "drunk & clearly and cooks .\n",
            "driven seasoning bagged sweet potato fries and the fries were average . four guys window meal was scared on average fast .\n",
            "major service courses as well , the food was not real , though , not any of this gyros , due to be an <unk> estate , not worth this visit .\n",
            "the pita jungle you walk in and use the description flavors and helped so the packaging was kelly too sweet . overall and the pool have truly advertising flavor on the ice cream but i know that i must say i have a extra korean quality for my favorite breakfast joints !\n",
            "i paid a whole party <unk> coupon ! it ' s also a to steep down and they ' re charged a picnic and a soda . they were truly cool and kids . but for this particular little gem that makes you not for an unique accented . the bartenders is nice and the little group are amazing !\n",
            "i think my friend had been to michael a couple weeks ago when i walked to the bar and all the kitchen was really generous . and there are great . i shall have a chance . the food was good and all i actually got a drink . they had a great lunch , but i asked if i could just do ! i started on . this was the bomb . have really been able to get my computers , but their car was ok . so long ! my wife looked my friend was prompt <unk> with some shoulder sauce . she at the dining room and turned by over the devil and drinks are good . there are 2 30p to live in some of a white staff .\n",
            "inexpensive sausages ( pink ? ) or had being a huge fan and a whole purchase of the bill seemed to save you item . unless you lose some bikes here , you won ' t any justify figure ( and dives ) . overall did not sure better for a lil patty than lots of heels !\n",
            "its good . you get a refund will not pass it at it . better than the life does then , what you expect it too . apparently even they do get them here is not the service you . it was their <unk> 3 hours too bad but not much like it on tap , but it took awhile too bad ( cue the <unk> <unk> , <unk> <unk> and the place you call them ? ) but i said that like its a nap , stick on it & then 6 people . while they have an artist that tasted that you know what you want . <unk> beckett minus your <unk> <unk> app ? there is a table and will get the act mad pool degrees bill down .\n",
            "mechanic half shorts cupcakes by 3 hours too long service after closing work ! !\n",
            "buckwheat qualities level incentive fry burger .\n",
            "parking several breakfast breakfast ? minus small coffee , going to ballpark , food . it was ok , though it was packed with djs & outdated . it starts to tuck . 00 , but after ? heck ? ? ? sounds , not bad service ? it ' s horrible , , since it gets perfect .\n",
            "did not take away .\n",
            "what is not 10 bbq business is <unk> but not dead and getting the place to get searching .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT5SOrm_OMhO"
      },
      "source": [
        "# Mutual information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXwWiZSuEzjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f57455-b016-40d1-f02d-8561e5dc6f4e"
      },
      "source": [
        "def mutual_information(model, dataset, N1=10000, N2=100):\n",
        "    model.eval()\n",
        "\n",
        "    kl1 = None\n",
        "    z2 = None\n",
        "    for batch, seq_lengths in dataset:\n",
        "        mean, logvar = model.encode(batch.to(model.device), seq_lengths)\n",
        "        logstd = logvar / 2\n",
        "        dst = MultivariateNormalDiag(loc=mean, scale_diag=torch.exp(logstd))\n",
        "        z1 = dst.sample((N1,))\n",
        "        batch_z2 = torch.cat(list(dst.sample((N2,))))\n",
        "        if z2 is None:\n",
        "            z2 = batch_z2\n",
        "        else:\n",
        "            z2 = torch.cat((z2, batch_z2))\n",
        "        batch_kl1 = torch.mean(torch.sum(((z1 ** 2 - ((z1 - mean) / torch.exp(logstd)) ** 2) / 2) - logstd, dim=-1), dim=0)\n",
        "        if kl1 is None:\n",
        "            kl1 = batch_kl1\n",
        "        else:\n",
        "            kl1 = torch.cat((kl1, batch_kl1))\n",
        "    kl1 = torch.mean(kl1)\n",
        "    \n",
        "    kl2 = None\n",
        "    for batch, seq_lengths in dataset:\n",
        "        mean, logvar = model.encode(batch.to(model.device), seq_lengths)\n",
        "        logstd = logvar / 2\n",
        "        mean = mean.unsqueeze(1)\n",
        "        logstd = logstd.unsqueeze(1)\n",
        "        batch_kl2 = torch.mean(torch.sum(((z2 ** 2 - ((z2 - mean) / torch.exp(logstd)) ** 2) / 2) - logstd, dim=-1), dim=-1)\n",
        "        if kl2 is None:\n",
        "            kl2 = batch_kl2\n",
        "        else:\n",
        "            kl2 = torch.cat((kl2, batch_kl2))\n",
        "    kl2 = torch.mean(kl2)\n",
        "\n",
        "    return kl1 - kl2\n",
        "\n",
        "with torch.no_grad():\n",
        "    print(mutual_information(model, dataset['test']))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(13.6122)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioYShNo-XiPa"
      },
      "source": [
        "# $\\epsilon$-$\\delta$ collapse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "iVzvyJrmXhUC",
        "outputId": "78e31eae-e6cc-439b-b1b0-32dd3ede6956"
      },
      "source": [
        "def ed_collapse(model, dataset, delta):\n",
        "    epsilons = torch.arange(0, 1, 0.01).to(model.device)\n",
        "    epsilons_size = epsilons.size(0)\n",
        "    rep_epsilons = epsilons.view(epsilons_size, 1).repeat(1, config.latent_dims)\n",
        "    all_zs_lt_epsilon = torch.zeros(epsilons_size, config.latent_dims).to(model.device)\n",
        "\n",
        "    batch_count = 0\n",
        "    for batch, seq_lengths in dataset:\n",
        "        batch_count += 1\n",
        "        mean, logvar = model.encode(batch.to(model.device), seq_lengths)\n",
        "        dim_wise_kld = dim_kl_divergence(mean, logvar)  # dim_wise_kld dim = z_dim\n",
        "        zs_lt_epsilon = dim_wise_kld.repeat(epsilons_size, 1) < rep_epsilons\n",
        "        all_zs_lt_epsilon += zs_lt_epsilon\n",
        "    \n",
        "    prop_lt_epsilon = torch.div(all_zs_lt_epsilon, batch_count)\n",
        "    collapsed = prop_lt_epsilon >= delta\n",
        "    prop_collapsed = torch.div(torch.sum(collapsed, dim=1), config.latent_dims)\n",
        "    return epsilons, prop_collapsed\n",
        "\n",
        "with torch.no_grad():\n",
        "    delta = 0.5\n",
        "    epsilons, prop_collapsed = ed_collapse(model, dataset['test'], delta)\n",
        "    plt.plot(epsilons, prop_collapsed)\n",
        "    plt.title(\"ControlSkipVAE Text Generation ε-δ Collapse (δ=0.5)\")\n",
        "    plt.xlabel(\"ε\")\n",
        "    plt.ylabel(\"Collapse %\")\n",
        "    plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}