{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ControlVAE Text Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanjiew/csc412_vae/blob/main/ControlVAE_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azYAruimtNmL"
      },
      "source": [
        "Code is adapted from [the original ControlVAE repository](https://github.com/shj1987/ControlVAE-ICML2020/tree/master/Language_modeling/Text_gen_PTB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfLIsQFIvrMn"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W-1FfEitYz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd98d58d-0a8e-43c0-a84d-2df546037626"
      },
      "source": [
        "%pip install texar-pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting texar-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/7e/20aa39ee9d19dcd1a1c0db4dad878088cfba216f45aeaa8fa89615ef46c0/texar_pytorch-0.1.2.post1-py3-none-any.whl (434kB)\n",
            "\r\u001b[K     |▊                               | 10kB 22.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 15.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30kB 13.3MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 12.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 81kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 143kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 153kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 163kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 174kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 184kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 194kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 204kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 215kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 225kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 235kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 245kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 256kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 266kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 276kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 286kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 296kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 307kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 317kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 327kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 337kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 348kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 358kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 368kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 378kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 389kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 399kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 409kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 419kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 430kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 440kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (20.9)\n",
            "Requirement already satisfied: numpy<=1.19.5,>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2.23.0)\n",
            "Requirement already satisfied: regex>=2018.01.10 in /usr/local/lib/python3.7/dist-packages (from texar-pytorch) (2019.12.20)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 17.3MB/s \n",
            "\u001b[?25hCollecting mypy-extensions\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/eb/975c7c080f3223a5cdaff09612f3a5221e4ba534f7039db34c35d95fa6a5/mypy_extensions-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=19.0->texar-pytorch) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->texar-pytorch) (1.24.3)\n",
            "Installing collected packages: funcsigs, sentencepiece, mypy-extensions, texar-pytorch\n",
            "Successfully installed funcsigs-1.0.2 mypy-extensions-0.4.3 sentencepiece-0.1.91 texar-pytorch-0.1.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA-j84Kot-hG"
      },
      "source": [
        "import math, os\n",
        "from typing import Any, Dict, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "import texar.torch as tx\n",
        "from texar.torch.custom import MultivariateNormalDiag\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChlkZ6psvt0B"
      },
      "source": [
        "# Loading the dataset (TODO: Load and preprocess Yelp dataset instead)\n",
        "\n",
        "# data_path = \"./simple-examples/data\"\n",
        "# train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
        "# if not os.path.exists(train_path):\n",
        "#     url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz'\n",
        "#     tx.data.maybe_download(url, './', extract=True)\n",
        "\n",
        "# train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
        "# vocab_path = os.path.join(data_path, \"vocab.txt\")\n",
        "# word_to_id = tx.data.make_vocab(\n",
        "#     train_path, return_type=\"dict\")\n",
        "\n",
        "# with open(vocab_path, 'w') as fvocab:\n",
        "#     for word in word_to_id:\n",
        "#         fvocab.write(\"%s\\n\" % word)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tItIdmTwj-4",
        "outputId": "830d0fee-0631-4758-fad5-893ceb833b45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Loading and preprocessing yelp review data\n",
        "\n",
        "%mkdir -p /content/simple-examples/data/\n",
        "%cd /content\n",
        "\n",
        "data_path = \"./simple-examples/data\"\n",
        "train_path = os.path.join(data_path, \"train.txt\")\n",
        "validate_path = os.path.join(data_path, \"validate.txt\")\n",
        "test_path = os.path.join(data_path, \"test.txt\")\n",
        "if not (os.path.exists(train_path) or os.path.exists(validate_path) or os.path.exists(test_path)):\n",
        "  url = 'https://raw.githubusercontent.com/rekiksab/Yelp-Data-Challenge-2013/master/yelp_challenge/yelp_phoenix_academic_dataset/yelp_academic_dataset_review.json'\n",
        "  df = pd.read_json(url, lines=True)\n",
        "  text = df['text']\n",
        "  percent = .01\n",
        "  train, validate, test = np.split(text.sample(frac=percent, random_state=42), [int(.6*percent*len(text)), int(.8*percent*len(text))])\n",
        "  np.savetxt(train_path, train.values, fmt='%s')\n",
        "  np.savetxt(validate_path, validate.values, fmt='%s')\n",
        "  np.savetxt(test_path, test.values, fmt='%s')\n",
        "  train_path = os.path.join(data_path, \"train.txt\")\n",
        "  validate_path = os.path.join(data_path, \"validate.txt\")\n",
        "  test_path = os.path.join(data_path, \"test.txt\")\n",
        "\n",
        "vocab_path = os.path.join(data_path, \"vocab.txt\")\n",
        "word_to_id = tx.data.make_vocab(\n",
        "    train_path, return_type=\"dict\")\n",
        "\n",
        "with open(vocab_path, 'w') as fvocab:\n",
        "    for word in word_to_id:\n",
        "        fvocab.write(\"%s\\n\" % word)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVNAghBeu8TC"
      },
      "source": [
        "# VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9_L26Qgu_5o"
      },
      "source": [
        "def kl_divergence(means: Tensor, logvars: Tensor) -> Tensor:\n",
        "    \"\"\"Compute the KL divergence between Gaussian distribution\n",
        "    \"\"\"\n",
        "    kl_cost = -0.5 * (logvars - means ** 2 -\n",
        "                      torch.exp(logvars) + 1.0)\n",
        "    kl_cost = torch.mean(kl_cost, 0)\n",
        "    return torch.sum(kl_cost)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3ckhxqwvITp"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    _latent_z: Tensor\n",
        "\n",
        "    def __init__(self, vocab_size: int, config_model):\n",
        "        super().__init__()\n",
        "        # Model architecture\n",
        "        self._config = config_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.encoder_w_embedder = tx.modules.WordEmbedder(\n",
        "            vocab_size=vocab_size, hparams=config_model.enc_emb_hparams)\n",
        "\n",
        "        self.encoder = tx.modules.UnidirectionalRNNEncoder[tx.core.LSTMState](\n",
        "            input_size=self.encoder_w_embedder.dim,\n",
        "            hparams={\n",
        "                \"rnn_cell\": config_model.enc_cell_hparams,\n",
        "            })\n",
        "\n",
        "        self.decoder_w_embedder = tx.modules.WordEmbedder(\n",
        "            vocab_size=vocab_size, hparams=config_model.dec_emb_hparams)\n",
        "\n",
        "        if config_model.decoder_type == \"lstm\":\n",
        "            self.lstm_decoder = tx.modules.BasicRNNDecoder(\n",
        "                input_size=(self.decoder_w_embedder.dim +\n",
        "                            config_model.latent_dims),\n",
        "                vocab_size=vocab_size,\n",
        "                token_embedder=self._embed_fn_rnn,\n",
        "                hparams={\"rnn_cell\": config_model.dec_cell_hparams})\n",
        "            sum_state_size = self.lstm_decoder.cell.hidden_size * 2\n",
        "\n",
        "        elif config_model.decoder_type == 'transformer':\n",
        "            # position embedding\n",
        "            self.decoder_p_embedder = tx.modules.SinusoidsPositionEmbedder(\n",
        "                position_size=config_model.max_pos,\n",
        "                hparams=config_model.dec_pos_emb_hparams)\n",
        "            # decoder\n",
        "            self.transformer_decoder = tx.modules.TransformerDecoder(\n",
        "                # tie word embedding with output layer\n",
        "                output_layer=self.decoder_w_embedder.embedding,\n",
        "                token_pos_embedder=self._embed_fn_transformer,\n",
        "                hparams=config_model.trans_hparams)\n",
        "            sum_state_size = self._config.dec_emb_hparams[\"dim\"]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Decoder type must be 'lstm' or 'transformer'\")\n",
        "\n",
        "        self.connector_mlp = tx.modules.MLPTransformConnector(\n",
        "            config_model.latent_dims * 2,\n",
        "            linear_layer_dim=self.encoder.cell.hidden_size * 2)\n",
        "\n",
        "        self.mlp_linear_layer = nn.Linear(\n",
        "            config_model.latent_dims, sum_state_size)\n",
        "\n",
        "    def forward(self,  # type: ignore\n",
        "                data_batch: tx.data.Batch,\n",
        "                kl_weight: float, start_tokens: torch.LongTensor,\n",
        "                end_token: int) -> Dict[str, Tensor]:\n",
        "        # encoder -> connector -> decoder\n",
        "        text_ids = data_batch[\"text_ids\"].to(self.device)\n",
        "        mean, logvar = self.encode(text_ids, data_batch[\"length\"].to(self.device))\n",
        "        kl_loss = kl_divergence(mean, logvar)\n",
        "        dst = MultivariateNormalDiag(\n",
        "            loc=mean, scale_diag=torch.exp(0.5 * logvar))\n",
        "\n",
        "        latent_z = dst.rsample()\n",
        "        helper = None\n",
        "        if self._config.decoder_type == \"lstm\":\n",
        "            helper = self.lstm_decoder.create_helper(\n",
        "                decoding_strategy=\"train_greedy\",\n",
        "                start_tokens=start_tokens,\n",
        "                end_token=end_token)\n",
        "\n",
        "        # decode\n",
        "        seq_lengths = data_batch[\"length\"].to(self.device) - 1\n",
        "        outputs = self.decode(\n",
        "            helper=helper, latent_z=latent_z,\n",
        "            text_ids=text_ids[:, :-1], seq_lengths=seq_lengths)\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Losses & train ops\n",
        "        rc_loss = tx.losses.sequence_sparse_softmax_cross_entropy(\n",
        "            labels=text_ids[:, 1:], logits=logits,\n",
        "            sequence_length=seq_lengths)\n",
        "\n",
        "        nll = rc_loss + kl_weight * kl_loss\n",
        "\n",
        "        ret = {\n",
        "            \"nll\": nll,\n",
        "            \"kl_loss\": kl_loss,\n",
        "            \"rc_loss\": rc_loss,\n",
        "            \"lengths\": seq_lengths,\n",
        "        }\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def _embed_fn_rnn(self, tokens: torch.LongTensor) -> Tensor:\n",
        "        r\"\"\"Generates word embeddings\n",
        "        \"\"\"\n",
        "        embedding = self.decoder_w_embedder(tokens)\n",
        "        latent_z = self._latent_z\n",
        "        if len(embedding.size()) > 2:\n",
        "            latent_z = latent_z.unsqueeze(0).repeat(tokens.size(0), 1, 1)\n",
        "        return torch.cat([embedding, latent_z], dim=-1)\n",
        "\n",
        "    def _embed_fn_transformer(self,\n",
        "                              tokens: torch.LongTensor,\n",
        "                              positions: torch.LongTensor) -> Tensor:\n",
        "        r\"\"\"Generates word embeddings combined with positional embeddings\n",
        "        \"\"\"\n",
        "        output_p_embed = self.decoder_p_embedder(positions)\n",
        "        output_w_embed = self.decoder_w_embedder(tokens)\n",
        "        output_w_embed = output_w_embed * self._config.hidden_size ** 0.5\n",
        "        output_embed = output_w_embed + output_p_embed\n",
        "        return output_embed\n",
        "    \n",
        "    def encode(self, text_ids, seq_lengths):\n",
        "        input_embed = self.encoder_w_embedder(text_ids)\n",
        "        _, encoder_states = self.encoder(\n",
        "            input_embed,\n",
        "            sequence_length=seq_lengths)\n",
        "        mean_logvar = self.connector_mlp(encoder_states)\n",
        "        mean, logvar = torch.chunk(mean_logvar, 2, 1)\n",
        "        return mean, logvar\n",
        "\n",
        "    @property\n",
        "    def decoder(self) -> tx.modules.DecoderBase:\n",
        "        if self._config.decoder_type == \"lstm\":\n",
        "            return self.lstm_decoder\n",
        "        else:\n",
        "            return self.transformer_decoder\n",
        "\n",
        "    def decode(self,\n",
        "               helper: Optional[tx.modules.Helper],\n",
        "               latent_z: Tensor,\n",
        "               text_ids: Optional[torch.LongTensor] = None,\n",
        "               seq_lengths: Optional[Tensor] = None,\n",
        "               max_decoding_length: Optional[int] = None) \\\n",
        "            -> Union[tx.modules.BasicRNNDecoderOutput,\n",
        "                     tx.modules.TransformerDecoderOutput]:\n",
        "        self._latent_z = latent_z\n",
        "        fc_output = self.mlp_linear_layer(latent_z)\n",
        "\n",
        "        if self._config.decoder_type == \"lstm\":\n",
        "            lstm_states = torch.chunk(fc_output, 2, dim=1)\n",
        "            outputs, _, _ = self.lstm_decoder(\n",
        "                initial_state=lstm_states,\n",
        "                inputs=text_ids,\n",
        "                helper=helper,\n",
        "                sequence_length=seq_lengths,\n",
        "                max_decoding_length=max_decoding_length)\n",
        "        else:\n",
        "            transformer_states = fc_output.unsqueeze(1)\n",
        "            outputs = self.transformer_decoder(\n",
        "                inputs=text_ids,\n",
        "                memory=transformer_states,\n",
        "                memory_sequence_length=torch.ones(transformer_states.size(0)),\n",
        "                helper=helper,\n",
        "                max_decoding_length=max_decoding_length)\n",
        "        return outputs\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YZOkyApvNmP"
      },
      "source": [
        "# PID Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YI6g1VCAvPdJ"
      },
      "source": [
        "class PIDControl():\n",
        "    \"\"\"docstring for ClassName\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"define them out of loop\"\"\"\n",
        "        # self.exp_KL = exp_KL\n",
        "        self.I_k1 = 0.0\n",
        "        self.W_k1 = 0.0\n",
        "        self.e_k1 = 0.0\n",
        "        \n",
        "    def _Kp_fun(self, Err, scale=1):\n",
        "        return 1.0/(1.0 + float(scale)*math.exp(Err))\n",
        "        \n",
        "\n",
        "    def pid(self, exp_KL, kl_loss, Kp=0.001, Ki=-0.001, Kd=0.01):\n",
        "        \"\"\"\n",
        "        position PID algorithm\n",
        "        Input: KL_loss\n",
        "        return: weight for KL loss, beta\n",
        "        \"\"\"\n",
        "        error_k = exp_KL - kl_loss\n",
        "        ## comput U as the control factor\n",
        "        Pk = Kp * self._Kp_fun(error_k)\n",
        "        Ik = self.I_k1 + Ki * error_k\n",
        "\n",
        "        ## window up for integrator\n",
        "        if self.W_k1 < 0 and self.W_k1 > 1:\n",
        "            Ik = self.I_k1\n",
        "            \n",
        "        Wk = Pk + Ik\n",
        "        self.W_k1 = Wk\n",
        "        self.I_k1 = Ik\n",
        "        self.e_k1 = error_k\n",
        "        \n",
        "        ## min and max value\n",
        "        if Wk > 1:\n",
        "            Wk = 1.0\n",
        "        if Wk < 0:\n",
        "            Wk = 0.0\n",
        "        \n",
        "        return Wk\n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV3yJUUGvZwz"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSMZHtSCvizg"
      },
      "source": [
        "class Config():\n",
        "  dataset = \"ptb\"\n",
        "  num_epochs = 100\n",
        "  hidden_size = 256\n",
        "  dec_dropout_in = 0.5\n",
        "  dec_dropout_out = 0.5\n",
        "  enc_dropout_in = 0.\n",
        "  enc_dropout_out = 0.\n",
        "  word_keep_prob = 0.5\n",
        "  batch_size = 32\n",
        "  embed_dim = 256\n",
        "\n",
        "  latent_dims = 32\n",
        "\n",
        "  lr_decay_hparams = {\n",
        "      \"init_lr\": 0.001,\n",
        "      \"threshold\": 2,\n",
        "      \"decay_factor\": 0.5,\n",
        "      \"max_decay\": 5\n",
        "  }\n",
        "\n",
        "\n",
        "  decoder_type = 'lstm'\n",
        "\n",
        "  enc_cell_hparams = {\n",
        "      \"type\": \"LSTMCell\",\n",
        "      \"kwargs\": {\n",
        "          \"num_units\": hidden_size,\n",
        "          \"bias\": 0.\n",
        "      },\n",
        "      \"dropout\": {\"output_keep_prob\": 1. - enc_dropout_out},\n",
        "      \"num_layers\": 1\n",
        "  }\n",
        "\n",
        "  dec_cell_hparams = {\n",
        "      \"type\": \"LSTMCell\",\n",
        "      \"kwargs\": {\n",
        "          \"num_units\": hidden_size,\n",
        "          \"bias\": 0.,\n",
        "      },\n",
        "      \"dropout\": {\"output_keep_prob\": 1. - dec_dropout_out},\n",
        "      \"num_layers\": 1,\n",
        "  }\n",
        "\n",
        "  enc_emb_hparams = {\n",
        "      'name': 'lookup_table',\n",
        "      \"dim\": embed_dim,\n",
        "      \"dropout_rate\": enc_dropout_in,\n",
        "      'initializer': {\n",
        "          'type': 'normal_',\n",
        "          'kwargs': {\n",
        "              'mean': 0.0,\n",
        "              'std': embed_dim**-0.5,\n",
        "          },\n",
        "      }\n",
        "  }\n",
        "\n",
        "  dec_emb_hparams = {\n",
        "      'name': 'lookup_table',\n",
        "      \"dim\": embed_dim,\n",
        "      \"dropout_rate\": dec_dropout_in,\n",
        "      'initializer': {\n",
        "          'type': 'normal_',\n",
        "          'kwargs': {\n",
        "              'mean': 0.0,\n",
        "              'std': embed_dim**-0.5,\n",
        "          },\n",
        "      }\n",
        "  }\n",
        "\n",
        "  # KL annealing\n",
        "  kl_anneal_hparams = {\n",
        "      \"warm_up\": 10,\n",
        "      \"start\": 0.1\n",
        "  }\n",
        "\n",
        "  train_data_hparams = {\n",
        "      \"num_epochs\": 1,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"seed\": 123,\n",
        "      \"dataset\": {\n",
        "          \"files\": './simple-examples/data/train.txt',\n",
        "          \"vocab_file\": './simple-examples/data/vocab.txt'\n",
        "      }\n",
        "  }\n",
        "\n",
        "  val_data_hparams = {\n",
        "      \"num_epochs\": 1,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"seed\": 123,\n",
        "      \"dataset\": {\n",
        "          \"files\": './simple-examples/data/validate.txt',\n",
        "          \"vocab_file\": './simple-examples/data/vocab.txt'\n",
        "      }\n",
        "  }\n",
        "\n",
        "  test_data_hparams = {\n",
        "      \"num_epochs\": 1,\n",
        "      \"batch_size\": batch_size,\n",
        "      \"dataset\": {\n",
        "          \"files\": './simple-examples/data/test.txt',\n",
        "          \"vocab_file\": './simple-examples/data/vocab.txt'\n",
        "      }\n",
        "  }\n",
        "\n",
        "  opt_hparams = {\n",
        "      'optimizer': {\n",
        "          'type': 'Adam',\n",
        "          'kwargs': {\n",
        "              'lr': 0.001\n",
        "          }\n",
        "      },\n",
        "      'gradient_clip': {\n",
        "          \"type\": \"clip_grad_norm_\",\n",
        "          \"kwargs\": {\n",
        "              \"max_norm\": 5,\n",
        "              \"norm_type\": 2\n",
        "          }\n",
        "      }\n",
        "  }"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK5x-aPqwVOA"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxvnGdq3wWdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29862768-0b5b-44e0-93a5-5b42524c9212"
      },
      "source": [
        "config = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_data = tx.data.MonoTextData(config.train_data_hparams, device=torch.device(\"cpu\"))\n",
        "val_data = tx.data.MonoTextData(config.val_data_hparams, device=torch.device(\"cpu\"))\n",
        "test_data = tx.data.MonoTextData(config.test_data_hparams, device=torch.device(\"cpu\"))\n",
        "\n",
        "iterator = tx.data.DataIterator({\"train\": train_data, \"valid\": val_data, \"test\": test_data})\n",
        "\n",
        "opt_vars = {\n",
        "    'learning_rate': config.lr_decay_hparams[\"init_lr\"],\n",
        "    'best_valid_nll': 1e100,\n",
        "    'steps_not_improved': 0,\n",
        "    'kl_weight': config.kl_anneal_hparams[\"start\"]\n",
        "}\n",
        "\n",
        "decay_cnt = 0\n",
        "max_decay = config.lr_decay_hparams[\"max_decay\"]\n",
        "decay_factor = config.lr_decay_hparams[\"decay_factor\"]\n",
        "decay_ts = config.lr_decay_hparams[\"threshold\"]\n",
        "\n",
        "save_path = './checkpoint.ckpt'\n",
        "\n",
        "anneal_r = 1.0 / (config.kl_anneal_hparams[\"warm_up\"] * (len(train_data) / config.batch_size))\n",
        "\n",
        "vocab = train_data.vocab\n",
        "model = VAE(train_data.vocab.size, config)\n",
        "model.to(device)\n",
        "\n",
        "start_tokens = torch.full(\n",
        "    (config.batch_size,),\n",
        "    vocab.bos_token_id,\n",
        "    dtype=torch.long).to(device)\n",
        "end_token = vocab.eos_token_id\n",
        "optimizer = tx.core.get_optimizer(\n",
        "    params=model.parameters(),\n",
        "    hparams=config.opt_hparams)\n",
        "scheduler = ExponentialLR(optimizer, decay_factor)\n",
        "\n",
        "max_iter = min(config.num_epochs*len(train_data)/config.batch_size, 80000)\n",
        "print('max steps:', max_iter)\n",
        "\n",
        "global_steps = {}\n",
        "global_steps['step'] = 0\n",
        "pid = PIDControl()\n",
        "opt_vars[\"kl_weight\"] = 0.0\n",
        "Kp = 0.01\n",
        "Ki = -0.0001\n",
        "exp_kl = 0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max steps: 18690.625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwsjM_00yQiU"
      },
      "source": [
        "def _run_epoch(epoch: int, mode: str, display: int = 10) -> Tuple[Tensor, float]:\n",
        "    iterator.switch_to_dataset(mode)\n",
        "\n",
        "    if mode == 'train':\n",
        "        model.train()\n",
        "        kl_weight = opt_vars[\"kl_weight\"]\n",
        "    else:\n",
        "        model.eval()\n",
        "        kl_weight = 1.0\n",
        "    \n",
        "    num_words = 0\n",
        "    nll_total = 0.\n",
        "\n",
        "    avg_rec = tx.utils.AverageRecorder()\n",
        "    for batch in iterator:\n",
        "        ## run model to get loss function\n",
        "        if global_steps['step']>= max_iter:\n",
        "            break\n",
        "        ret = model(batch, kl_weight, start_tokens, end_token)\n",
        "        if mode == \"train\":\n",
        "            pbar.update(1)\n",
        "            global_steps['step'] += 1\n",
        "            kl_loss = ret['kl_loss'].item()\n",
        "            rec_loss = ret['rc_loss'].item()\n",
        "            total_loss = ret[\"nll\"].item()\n",
        "            kl_weight = pid.pid(exp_kl, kl_loss, Kp, Ki)\n",
        "\n",
        "            opt_vars[\"kl_weight\"] = kl_weight\n",
        "            \n",
        "            ## total loss\n",
        "            ret[\"nll\"].backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        batch_size = len(ret[\"lengths\"])\n",
        "        num_words += torch.sum(ret[\"lengths\"]).item()\n",
        "        nll_total += ret[\"nll\"].item() * batch_size\n",
        "        avg_rec.add(\n",
        "            [ret[\"nll\"].item(),\n",
        "              ret[\"kl_loss\"].item(),\n",
        "              ret[\"rc_loss\"].item()],\n",
        "            batch_size)\n",
        "            \n",
        "        if global_steps['step'] % display == 1 and mode == 'train':\n",
        "            nll = avg_rec.avg(0)\n",
        "            klw = opt_vars[\"kl_weight\"]\n",
        "            KL = avg_rec.avg(1)\n",
        "            rc = avg_rec.avg(2)\n",
        "            \n",
        "    nll = avg_rec.avg(0)\n",
        "    KL = avg_rec.avg(1)\n",
        "    rc = avg_rec.avg(2)\n",
        "    if num_words > 0:\n",
        "        log_ppl = nll_total / num_words\n",
        "        ppl = math.exp(log_ppl)\n",
        "    else:\n",
        "        log_ppl = 100\n",
        "        ppl = math.exp(log_ppl)\n",
        "        nll = 1000\n",
        "        KL = args.exp_kl\n",
        "    \n",
        "    print(f\"\\n{mode}: epoch {epoch}, nll {nll:.4f}, KL {KL:.4f}, \"\n",
        "          f\"rc {rc:.4f}, log_ppl {log_ppl:.4f}, ppl {ppl:.4f}\")\n",
        "    return nll, ppl  # type: ignore"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxMAUE7rytC1",
        "outputId": "10756407-e9a2-4ad2-8be6-25b4a21abd6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Counts trainable parameters\n",
        "total_parameters = sum(param.numel() for param in model.parameters())\n",
        "print(f\"{total_parameters} total parameters\")\n",
        "\n",
        "best_nll = best_ppl = 0.\n",
        "\n",
        "## start running model\n",
        "pbar = tqdm(total = int(max_iter))\n",
        "for epoch in range(config.num_epochs):\n",
        "    _, _ = _run_epoch(epoch, 'train', display=200)\n",
        "    val_nll, _ = _run_epoch(epoch, 'valid')\n",
        "    test_nll, test_ppl = _run_epoch(epoch, 'test')\n",
        "\n",
        "    if val_nll < opt_vars['best_valid_nll']:\n",
        "        opt_vars['best_valid_nll'] = val_nll\n",
        "        opt_vars['steps_not_improved'] = 0\n",
        "        best_nll = test_nll\n",
        "        best_ppl = test_ppl\n",
        "\n",
        "        states = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"scheduler\": scheduler.state_dict()\n",
        "        }\n",
        "        torch.save(states, save_path)\n",
        "    else:\n",
        "        opt_vars['steps_not_improved'] += 1\n",
        "        if opt_vars['steps_not_improved'] == decay_ts:\n",
        "            old_lr = opt_vars['learning_rate']\n",
        "            opt_vars['learning_rate'] *= decay_factor\n",
        "            opt_vars['steps_not_improved'] = 0\n",
        "            new_lr = opt_vars['learning_rate']\n",
        "            ckpt = torch.load(save_path)\n",
        "            model.load_state_dict(ckpt['model'])\n",
        "            optimizer.load_state_dict(ckpt['optimizer'])\n",
        "            scheduler.load_state_dict(ckpt['scheduler'])\n",
        "            scheduler.step()\n",
        "            print(f\"-----\\nchange lr, old lr: {old_lr}, \"\n",
        "                  f\"new lr: {new_lr}\\n-----\")\n",
        "\n",
        "            decay_cnt += 1\n",
        "            if decay_cnt == max_decay:\n",
        "                break\n",
        "    if global_steps['step'] >= max_iter:\n",
        "        break\n",
        "\n",
        "print(f\"\\nbest testing nll: {best_nll:.4f},\"\n",
        "      f\"best testing ppl {best_ppl:.4f}\\n\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/18690 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "18768856 total parameters\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 187/18690 [01:03<1:37:53,  3.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 0, nll 242.1186, KL 8.4507, rc 241.4173, log_ppl 7.5939, ppl 1986.1229\n",
            "\n",
            "valid: epoch 0, nll 217.5254, KL 1.2186, rc 216.3068, log_ppl 7.1137, ppl 1228.6390\n",
            "\n",
            "test: epoch 0, nll 212.4831, KL 1.2257, rc 211.2574, log_ppl 7.1226, ppl 1239.6523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 374/18690 [02:23<1:38:35,  3.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 1, nll 219.0069, KL 0.9301, rc 218.8452, log_ppl 6.8691, ppl 962.0360\n",
            "\n",
            "valid: epoch 1, nll 204.9556, KL 0.9211, rc 204.0344, log_ppl 6.7026, ppl 814.5178\n",
            "\n",
            "test: epoch 1, nll 200.2742, KL 0.9212, rc 199.3530, log_ppl 6.7133, ppl 823.3134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 561/18690 [03:43<1:49:40,  2.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 2, nll 205.5673, KL 1.1363, rc 205.3474, log_ppl 6.4475, ppl 631.1379\n",
            "\n",
            "valid: epoch 2, nll 198.8487, KL 1.1501, rc 197.6987, log_ppl 6.5029, ppl 667.0646\n",
            "\n",
            "test: epoch 2, nll 194.2608, KL 1.1413, rc 193.1195, log_ppl 6.5118, ppl 673.0120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 748/18690 [05:03<1:40:31,  2.97it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 3, nll 197.4882, KL 1.2958, rc 197.2076, log_ppl 6.1941, ppl 489.8639\n",
            "\n",
            "valid: epoch 3, nll 196.3364, KL 1.1455, rc 195.1909, log_ppl 6.4207, ppl 614.4490\n",
            "\n",
            "test: epoch 3, nll 191.8794, KL 1.1473, rc 190.7321, log_ppl 6.4319, ppl 621.3756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  5%|▌         | 935/18690 [06:23<2:01:44,  2.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 4, nll 191.5834, KL 1.3136, rc 191.2669, log_ppl 6.0089, ppl 407.0467\n",
            "\n",
            "valid: epoch 4, nll 195.0562, KL 1.1105, rc 193.9457, log_ppl 6.3789, ppl 589.2567\n",
            "\n",
            "test: epoch 4, nll 190.5888, KL 1.1193, rc 189.4695, log_ppl 6.3887, ppl 595.0675\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|▌         | 1122/18690 [07:43<1:30:17,  3.24it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 5, nll 186.6979, KL 1.3128, rc 186.3494, log_ppl 5.8557, ppl 349.2173\n",
            "\n",
            "valid: epoch 5, nll 194.7297, KL 1.0741, rc 193.6556, log_ppl 6.3682, ppl 582.9982\n",
            "\n",
            "test: epoch 5, nll 190.2367, KL 1.0827, rc 189.1540, log_ppl 6.3769, ppl 588.0848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  7%|▋         | 1309/18690 [09:04<1:30:58,  3.18it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 6, nll 182.4448, KL 1.2814, rc 182.0736, log_ppl 5.7223, ppl 305.6067\n",
            "\n",
            "valid: epoch 6, nll 194.9913, KL 1.1892, rc 193.8022, log_ppl 6.3767, ppl 588.0067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 1310/18690 [09:20<24:24:30,  5.06s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 6, nll 190.5193, KL 1.1967, rc 189.3226, log_ppl 6.3863, ppl 593.6828\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  8%|▊         | 1496/18690 [10:23<1:33:50,  3.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 7, nll 178.5352, KL 1.2816, rc 178.1332, log_ppl 5.5997, ppl 270.3389\n",
            "\n",
            "valid: epoch 7, nll 194.8460, KL 1.1167, rc 193.7293, log_ppl 6.3720, ppl 585.2186\n",
            "\n",
            "test: epoch 7, nll 190.4121, KL 1.1241, rc 189.2880, log_ppl 6.3827, ppl 591.5520\n",
            "-----\n",
            "change lr, old lr: 0.001, new lr: 0.0005\n",
            "-----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|▉         | 1683/18690 [11:41<1:36:30,  2.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 8, nll 181.8517, KL 1.1839, rc 181.4533, log_ppl 5.7037, ppl 299.9743\n",
            "\n",
            "valid: epoch 8, nll 194.6275, KL 0.9629, rc 193.6646, log_ppl 6.3648, ppl 581.0527\n",
            "\n",
            "test: epoch 8, nll 190.1776, KL 0.9678, rc 189.2099, log_ppl 6.3749, ppl 586.9218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 1870/18690 [13:01<1:45:32,  2.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 9, nll 179.3956, KL 1.1839, rc 178.9709, log_ppl 5.6267, ppl 277.7334\n",
            "\n",
            "valid: epoch 9, nll 194.7425, KL 1.0081, rc 193.7344, log_ppl 6.3686, ppl 583.2413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 1871/18690 [13:18<25:14:58,  5.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 9, nll 190.3735, KL 1.0144, rc 189.3591, log_ppl 6.3815, ppl 590.7874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 11%|█         | 2057/18690 [14:20<2:18:18,  2.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 10, nll 177.2370, KL 1.1741, rc 176.7900, log_ppl 5.5590, ppl 259.5525\n",
            "\n",
            "valid: epoch 10, nll 195.0363, KL 1.0577, rc 193.9786, log_ppl 6.3782, ppl 588.8726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 2058/18690 [14:37<25:10:44,  5.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 10, nll 190.5787, KL 1.0633, rc 189.5154, log_ppl 6.3883, ppl 594.8666\n",
            "-----\n",
            "change lr, old lr: 0.0005, new lr: 0.00025\n",
            "-----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 2244/18690 [15:40<1:39:06,  2.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 11, nll 179.0867, KL 1.0991, rc 178.6450, log_ppl 5.6170, ppl 275.0553\n",
            "\n",
            "valid: epoch 11, nll 194.8005, KL 0.9810, rc 193.8195, log_ppl 6.3705, ppl 584.3491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 2245/18690 [15:56<23:29:13,  5.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 11, nll 190.3333, KL 0.9894, rc 189.3439, log_ppl 6.3801, ppl 589.9931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 2431/18690 [17:00<1:33:28,  2.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 12, nll 177.8710, KL 1.1247, rc 177.3957, log_ppl 5.5788, ppl 264.7656\n",
            "\n",
            "valid: epoch 12, nll 194.8330, KL 0.9301, rc 193.9029, log_ppl 6.3716, ppl 584.9704\n",
            "\n",
            "test: epoch 12, nll 190.4129, KL 0.9352, rc 189.4776, log_ppl 6.3828, ppl 591.5680\n",
            "-----\n",
            "change lr, old lr: 0.00025, new lr: 0.000125\n",
            "-----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 2618/18690 [18:21<1:18:39,  3.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 13, nll 179.1431, KL 1.0663, rc 178.6707, log_ppl 5.6187, ppl 275.5425\n",
            "\n",
            "valid: epoch 13, nll 194.7159, KL 0.9482, rc 193.7677, log_ppl 6.3677, ppl 582.7336\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 2619/18690 [18:38<22:56:54,  5.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 13, nll 190.2822, KL 0.9533, rc 189.3288, log_ppl 6.3784, ppl 588.9825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 2805/18690 [19:41<1:29:48,  2.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 14, nll 177.9657, KL 1.0513, rc 177.4791, log_ppl 5.5818, ppl 265.5528\n",
            "\n",
            "valid: epoch 14, nll 194.8098, KL 0.8750, rc 193.9348, log_ppl 6.3708, ppl 584.5272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 2806/18690 [19:58<22:46:24,  5.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 14, nll 190.3667, KL 0.8809, rc 189.4858, log_ppl 6.3812, ppl 590.6535\n",
            "-----\n",
            "change lr, old lr: 0.000125, new lr: 6.25e-05\n",
            "-----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 2992/18690 [21:01<1:05:00,  4.02it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 15, nll 179.2369, KL 1.0301, rc 178.7402, log_ppl 5.6217, ppl 276.3546\n",
            "\n",
            "valid: epoch 15, nll 194.6984, KL 0.9015, rc 193.7969, log_ppl 6.3672, ppl 582.4015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 2993/18690 [21:18<21:49:06,  5.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test: epoch 15, nll 190.3200, KL 0.9072, rc 189.4128, log_ppl 6.3797, ppl 589.7292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 3179/18690 [22:21<1:12:29,  3.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: epoch 16, nll 177.9588, KL 0.9827, rc 177.4665, log_ppl 5.5816, ppl 265.4952\n",
            "\n",
            "valid: epoch 16, nll 194.8302, KL 0.8232, rc 194.0070, log_ppl 6.3715, ppl 584.9162\n",
            "\n",
            "test: epoch 16, nll 190.3695, KL 0.8280, rc 189.5415, log_ppl 6.3813, ppl 590.7082\n",
            "-----\n",
            "change lr, old lr: 6.25e-05, new lr: 3.125e-05\n",
            "-----\n",
            "\n",
            "best testing nll: 190.1776,best testing ppl 586.9218\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgc9JOzGGf-i"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNUYkXKPGh3K",
        "outputId": "7cfdd56b-e7cc-4cf1-d5fc-d033f755c0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.eval()\n",
        "\n",
        "batch_size = train_data.batch_size\n",
        "\n",
        "test_sentence = 'The service was terrible.'\n",
        "text_ids = torch.tensor(vocab.map_tokens_to_ids_py([(test_sentence+' <EOS>').split()])).cuda()\n",
        "mean, logvar = model.encode(text_ids, torch.tensor([5]).cuda())\n",
        "dst = MultivariateNormalDiag(loc=mean[0], scale_diag=torch.exp(logvar[0]))\n",
        "latent_z = dst.sample((batch_size,))\n",
        "\n",
        "# latent_z = torch.FloatTensor(batch_size, config.latent_dims).uniform_(-1, 1).to(device)\n",
        "# latent_z = torch.randn(batch_size, config.latent_dims).to(device)\n",
        "\n",
        "helper = model.decoder.create_helper(\n",
        "    decoding_strategy='infer_sample',\n",
        "    start_tokens=start_tokens,\n",
        "    end_token=end_token)\n",
        "outputs = model.decode(\n",
        "    helper=helper,\n",
        "    latent_z=latent_z,\n",
        "    max_decoding_length=100)\n",
        "\n",
        "if config.decoder_type == \"transformer\":\n",
        "    outputs = outputs[0]\n",
        "\n",
        "\n",
        "sample_tokens = vocab.map_ids_to_tokens_py(outputs.sample_id.cpu())\n",
        "for sent in sample_tokens:\n",
        "    sent = tx.utils.compat_as_text(list(sent))\n",
        "    end_id = len(sent)\n",
        "    if vocab.eos_token in sent:\n",
        "        end_id = sent.index(vocab.eos_token)\n",
        "    print(' '.join(sent[:end_id + 1]) + '\\n')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My food is fresh and no was two selections of just disappointed. The portions did have another convenient bike adobo, food polishes and I did appears some again..maybe assume I will not will have time it, on a convenient owned afternoon dark med overpriced), <EOS>\n",
            "\n",
            "We have found bad days with an experts for her toward both and she recommended across a person? with huge amount with us. The workers came in (every flavors and needed though. <EOS>\n",
            "\n",
            "The <EOS>\n",
            "\n",
            "It's adapt 10:30 fast food, fairly wasnt asking out to get to this to be expensive people, isn't a dessert can find the street around a friend's crowd toast. It's dry and those sugar next in myself atmosphere. This place is friendly. We were extremely sometime as much toppings did for no good mojito AND the mistake is quickly. The biscuits and - fast, bad. <EOS>\n",
            "\n",
            "near ahead of pho, that were very really toppings: and chose: shaker I'm stage growing to them to try the diet of Scottsdale before the dishes were cos for friendly and Short icon in stands. <EOS>\n",
            "\n",
            "This place is a small place to hush deal, we seem work to give how for head, but how you want with a bargain variety of fresh sliced restaurants and cheese... and jam. (which just a just like walking as you're about an much time we gave coming at the bar owner. I've still severe service, it is no highlight and swapped tea at this while. I have wanted by his website for our chocolate, and order a told us that I say him on your friends. <EOS>\n",
            "\n",
            "The Don't their Boba was expressing one of the mind but again. airport is not a bit morning, many other workers at the Tempe whatev. outing, roll and had roll primary (taking ONE timing. <EOS>\n",
            "\n",
            "Then how this am kind of 5 25 light and AND your Hollindase temping since the frosty I've pretty opened at the heck of a lake that nice and/or 2\" and tender from Echo tending that if you coming at a turkey years and a great special & nice sauce and the summer were knowledgeable, and earshot. <EOS>\n",
            "\n",
            "We agree and Pei accent!! store. <EOS>\n",
            "\n",
            "Not first happy plaza. <EOS>\n",
            "\n",
            "1) I believe I love me for the theater with the classes but do a better great burrito full of payment. (well, Paradise is very meat which especially above hands would be volume to start. Every just a freshly full as the driver had in her own accompanied for Sun streusel but it was good, at the larger WA to get being away to taramosalata - an dessert up seat but I stop there when it was dinner too. But I got it! <EOS>\n",
            "\n",
            "Spinato's seems to much forward with delicious! <EOS>\n",
            "\n",
            "We serve there, by the most (save well for your St joint. <EOS>\n",
            "\n",
            "The space was perfect, and too damn dreadful. The forgotten. She also ask who have seen a cheese chilled in me. It's very out but for course even surprisingly interested to certain the ramen Salmon! Since you have way the new off a table and Can't did never have been looking forward for 6pm and I surprised it's a good divey A few patio which wasn't nice in food. A good was a perfect quesadilla for our gut experience. <EOS>\n",
            "\n",
            "First Would go to. <EOS>\n",
            "\n",
            "Well we found the (Duh)- tree of the side, wine (at since they were excellent. <EOS>\n",
            "\n",
            "Equis potstickers and happy here as well. <EOS>\n",
            "\n",
            "I think this place was a swim items and food night, at the beer and make a patrons has hopeful relatives this arrived about optional), and are, and thinking that we were had. Owner. We came at a Greek salad in the grill. The lady was \"AYYYY\" muffin is if you up everything because they were based in a meal I decided to come for them sitting again. I see I still ordered one really shy at downtown charging the counter Personally, the wine spot. Tsao BBQ very ornaments. I had if it it was surprised as frozen service and\n",
            "\n",
            "Simple, for two also. It's not nowhere by the Rarely extra cool in the brisket. I recommend it at the peaches their downside (at said, it was close to squeeze what she was one of course we were reasonably cuarenta It's for market... when I stole two present than comped no mood back in home have here out a phenominal. we won't allowed it so decent minutes we don't have the low, from birria of it establishment realized he will find a lunch at their Phoenix Um thou. <EOS>\n",
            "\n",
            "In our tell and be fraggin' a bit on good Sauce or do turned back to B****, I would be creamy conviently standard because you working load it looked drinks to particularly find. <EOS>\n",
            "\n",
            "Now, I didn't take that his chairs, order to the window funny or food as that made a day. I thought we did. <EOS>\n",
            "\n",
            "leaving in which we went nearly on settings choice and believe I can some creatures I get no place who complete forced to go to if you do eaten playing exactly yet with something spooning most seat up for cold than many '2'. oil of it were a this right, immediately like it was changed enough a taste days high more lovely table. The food has great than show to find soon. It found coming here so easy for such a standout as night, they who ordered trying to you!). <EOS>\n",
            "\n",
            "I have were ample wi overwhelmed and I really even so CUSTOMER performances. that that Bevin that's you actually took Village. <EOS>\n",
            "\n",
            "I tried the waitress away, and far food met with a normal entree and overall you're out of the gals asked ask but I was dry, my Personal Cinnamon evenings for neon or twice! <EOS>\n",
            "\n",
            "My Taquitos is for a leave but Friendly along and 5W! pot. option - cash with a reviewers to get this place. The second used big you can spend more. My list. I lost susan. so if they were in a map I wanted it would handed the compliment to come here as there of the attention of my textural security. <EOS>\n",
            "\n",
            "Well, staying for Mesa and I thought I am not wait on more great side of roasted flare <EOS>\n",
            "\n",
            "And what I wanted to PastaBAR down getting people we wanted to find about my do? they only are jalapeño polished Italian years and a little original Definitely got shredded happy and one while gone more away and a lousy man at the Heart salt (which David felt some year really reasonable, but not difficult to what you order anything in questions. Try something and no okay <EOS>\n",
            "\n",
            "I've been here with there. This is all several service. <EOS>\n",
            "\n",
            "I should get a group but spice someday, genuine. <EOS>\n",
            "\n",
            "Anyway, I picked up a table and completely tables. The spot! Great Salad was truly helpful and refilled every expensive than taking our prosciutto/apple lunch room. It was get in someplace a bad burning implement and town. walked techniques slipped and it took liquor late. We should just give what you are over the Chicago, it was good, but not great good. Very service I often and he'd want out on their lenghty pork -- he was nice, so appreciated the spicy style reviews out my concept and he take to \"no\" upcharge to want a big bunch of area!\n",
            "\n",
            "With a least size. <EOS>\n",
            "\n",
            "This cakes in) me about about he Free Monte Great group had a bit shop served by the spicy cakes and views as because and now kisra, greeted the door should Eye fancy Goldman fun. <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}