{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference_Network_VAE_Text.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPffQhWt7IzGadzzOzhcqYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanjiew/csc412_vae/blob/main/Inference_Network_VAE_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1lh_BItYpP_"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import importlib\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, optim\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "from itertools import chain\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from modules import VAE\n",
        "from modules import LSTMEncoder, LSTMDecoder\n",
        "from logger import Logger\n",
        "\n",
        "clip_grad = 5.0\n",
        "decay_epoch = 2\n",
        "lr_decay = 0.5\n",
        "max_decay = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "corKh1s8dmOh"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AU6HVbIbiRJ"
      },
      "source": [
        "def log_sum_exp(value, dim=None, keepdim=False):\n",
        "    \"\"\"Numerically stable implementation of the operation\n",
        "    value.exp().sum(dim, keepdim).log()\n",
        "    \"\"\"\n",
        "    if dim is not None:\n",
        "        m, _ = torch.max(value, dim=dim, keepdim=True)\n",
        "        value0 = value - m\n",
        "        if keepdim is False:\n",
        "            m = m.squeeze(dim)\n",
        "        return m + torch.log(torch.sum(torch.exp(value0), dim=dim, keepdim=keepdim))\n",
        "    else:\n",
        "        m = torch.max(value)\n",
        "        sum_exp = torch.sum(torch.exp(value - m))\n",
        "        return m + torch.log(sum_exp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thSOGdCzdBSr"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNFASzF_ZuRl"
      },
      "source": [
        "class VocabEntry(object):\n",
        "    \"\"\"docstring for Vocab\"\"\"\n",
        "    def __init__(self, word2id=None):\n",
        "        super(VocabEntry, self).__init__()\n",
        "\n",
        "        if word2id:\n",
        "            self.word2id = word2id\n",
        "            self.unk_id = word2id['<unk>']\n",
        "        else:\n",
        "            self.word2id = dict()\n",
        "            self.unk_id = 3\n",
        "            self.word2id['<pad>'] = 0\n",
        "            self.word2id['<s>'] = 1\n",
        "            self.word2id['</s>'] = 2\n",
        "            self.word2id['<unk>'] = self.unk_id\n",
        "\n",
        "        self.id2word_ = {v: k for k, v in self.word2id.items()}\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        return self.word2id.get(word, self.unk_id)\n",
        "\n",
        "    def __contains__(self, word):\n",
        "        return word in self.word2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2id)\n",
        "\n",
        "    def add(self, word):\n",
        "        if word not in self:\n",
        "            wid = self.word2id[word] = len(self)\n",
        "            self.id2word[wid] = word\n",
        "            return wid\n",
        "\n",
        "        else:\n",
        "            return self[word]\n",
        "\n",
        "    def id2word(self, wid):\n",
        "        return self.id2word_[wid]\n",
        "\n",
        "    def decode_sentence(self, sentence):\n",
        "        decoded_sentence = []\n",
        "        for wid_t in sentence:\n",
        "            wid = wid_t.item()\n",
        "            decoded_sentence.append(self.id2word_[wid])\n",
        "        return decoded_sentence\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def from_corpus(fname):\n",
        "        vocab = VocabEntry()\n",
        "        with open(fname) as fin:\n",
        "            for line in fin:\n",
        "                _ = [vocab.add(word) for word in line.split()]\n",
        "\n",
        "        return vocab\n",
        "\n",
        "\n",
        "class MonoTextData(object):\n",
        "    \"\"\"docstring for MonoTextData\"\"\"\n",
        "    def __init__(self, fname, label=False, max_length=None, vocab=None):\n",
        "        super(MonoTextData, self).__init__()\n",
        "\n",
        "        self.data, self.vocab, self.dropped, self.labels = self._read_corpus(fname, label, max_length, vocab)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _read_corpus(self, fname, label, max_length, vocab):\n",
        "        data = []\n",
        "        labels = [] if label else None\n",
        "        dropped = 0\n",
        "        if not vocab:\n",
        "            vocab = defaultdict(lambda: len(vocab))\n",
        "            vocab['<pad>'] = 0\n",
        "            vocab['<s>'] = 1\n",
        "            vocab['</s>'] = 2\n",
        "            vocab['<unk>'] = 3\n",
        "\n",
        "        with open(fname) as fin:\n",
        "            for line in fin:\n",
        "                if label:\n",
        "                    split_line = line.split('\\t')\n",
        "                    lb = split_line[0]\n",
        "                    split_line = split_line[1].split()\n",
        "                else:\n",
        "                    split_line = line.split()\n",
        "                if len(split_line) < 1:\n",
        "                    dropped += 1\n",
        "                    continue\n",
        "\n",
        "                if max_length:\n",
        "                    if len(split_line) > max_length:\n",
        "                        dropped += 1\n",
        "                        continue\n",
        "\n",
        "                if label:\n",
        "                    labels.append(lb)\n",
        "                data.append([vocab[word] for word in split_line])\n",
        "\n",
        "        if isinstance(vocab, VocabEntry):\n",
        "            return data, vocab, dropped, labels\n",
        "\n",
        "        return data, VocabEntry(vocab), dropped, labels\n",
        "\n",
        "    def _to_tensor(self, batch_data, batch_first, device):\n",
        "        \"\"\"pad a list of sequences, and transform them to tensors\n",
        "        Args:\n",
        "            batch_data: a batch of sentences (list) that are composed of\n",
        "                word ids.\n",
        "            batch_first: If true, the returned tensor shape is\n",
        "                (batch, seq_len), otherwise (seq_len, batch)\n",
        "            device: torch.device\n",
        "        Returns: Tensor, Int list\n",
        "            Tensor: Tensor of the batch data after padding\n",
        "            Int list: a list of integers representing the length\n",
        "                of each sentence (including start and stop symbols)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # pad stop symbol\n",
        "        batch_data = [sent + [self.vocab['</s>']] for sent in batch_data]\n",
        "\n",
        "        sents_len = [len(sent) for sent in batch_data]\n",
        "\n",
        "        max_len = max(sents_len)\n",
        "\n",
        "        batch_size = len(sents_len)\n",
        "        sents_new = []\n",
        "\n",
        "        # pad start symbol\n",
        "        sents_new.append([self.vocab['<s>']] * batch_size)\n",
        "        for i in range(max_len):\n",
        "            sents_new.append([sent[i] if len(sent) > i else self.vocab['<pad>'] \\\n",
        "                               for sent in batch_data])\n",
        "\n",
        "\n",
        "        sents_ts = torch.tensor(sents_new, dtype=torch.long,\n",
        "                                 requires_grad=False, device=device)\n",
        "\n",
        "        if batch_first:\n",
        "            sents_ts = sents_ts.permute(1, 0).contiguous()\n",
        "\n",
        "        return sents_ts, [length + 1 for length in sents_len]\n",
        "\n",
        "\n",
        "    def data_iter(self, batch_size, device, batch_first=False, shuffle=True):\n",
        "        \"\"\"pad data with start and stop symbol, and pad to the same length\n",
        "        Returns:\n",
        "            batch_data: LongTensor with shape (seq_len, batch_size)\n",
        "            sents_len: list of data length, this is the data length\n",
        "                       after counting start and stop symbols\n",
        "        \"\"\"\n",
        "        index_arr = np.arange(len(self.data))\n",
        "\n",
        "        if shuffle:\n",
        "            np.random.shuffle(index_arr)\n",
        "\n",
        "        batch_num = int(np.ceil(len(index_arr)) / float(batch_size))\n",
        "        for i in range(batch_num):\n",
        "            batch_ids = index_arr[i * batch_size : (i+1) * batch_size]\n",
        "            batch_data = [self.data[index] for index in batch_ids]\n",
        "\n",
        "            # uncomment this line if the dataset has variable length\n",
        "            batch_data.sort(key=lambda e: -len(e))\n",
        "\n",
        "            batch_data, sents_len = self._to_tensor(batch_data, batch_first, device)\n",
        "\n",
        "            yield batch_data, sents_len\n",
        "\n",
        "    def create_data_batch_labels(self, batch_size, device, batch_first=False):\n",
        "        \"\"\"pad data with start and stop symbol, batching is performerd w.r.t.\n",
        "        the sentence length, so that each returned batch has the same length,\n",
        "        no further pack sequence function (e.g. pad_packed_sequence) is required\n",
        "        Returns: List\n",
        "            List: a list of batched data, each element is a tensor with shape\n",
        "                (seq_len, batch_size)\n",
        "        \"\"\"\n",
        "        sents_len = np.array([len(sent) for sent in self.data])\n",
        "        sort_idx = np.argsort(sents_len)\n",
        "        sort_len = sents_len[sort_idx]\n",
        "\n",
        "        # record the locations where length changes\n",
        "        change_loc = []\n",
        "        for i in range(1, len(sort_len)):\n",
        "            if sort_len[i] != sort_len[i-1]:\n",
        "                change_loc.append(i)\n",
        "        change_loc.append(len(sort_len))\n",
        "\n",
        "        batch_data_list = []\n",
        "        batch_label_list = []\n",
        "        total = 0\n",
        "        curr = 0\n",
        "        for idx in change_loc:\n",
        "            while curr < idx:\n",
        "                batch_data = []\n",
        "                batch_label = []\n",
        "                next = min(curr + batch_size, idx)\n",
        "                for id_ in range(curr, next):\n",
        "                    batch_data.append(self.data[sort_idx[id_]])\n",
        "                    batch_label.append(self.labels[sort_idx[id_]])\n",
        "                curr = next\n",
        "                batch_data, sents_len = self._to_tensor(batch_data, batch_first, device)\n",
        "                batch_data_list.append(batch_data)\n",
        "                batch_label_list.append(batch_label)\n",
        "\n",
        "                total += batch_data.size(0)\n",
        "                assert(sents_len == ([sents_len[0]] * len(sents_len)))\n",
        "\n",
        "        assert(total == len(self.data))\n",
        "        return batch_data_list, batch_label_list\n",
        "\n",
        "    def create_data_batch(self, batch_size, device, batch_first=False):\n",
        "        \"\"\"pad data with start and stop symbol, batching is performerd w.r.t.\n",
        "        the sentence length, so that each returned batch has the same length,\n",
        "        no further pack sequence function (e.g. pad_packed_sequence) is required\n",
        "        Returns: List\n",
        "            List: a list of batched data, each element is a tensor with shape\n",
        "                (seq_len, batch_size)\n",
        "        \"\"\"\n",
        "        sents_len = np.array([len(sent) for sent in self.data])\n",
        "        sort_idx = np.argsort(sents_len)\n",
        "        sort_len = sents_len[sort_idx]\n",
        "\n",
        "        # record the locations where length changes\n",
        "        change_loc = []\n",
        "        for i in range(1, len(sort_len)):\n",
        "            if sort_len[i] != sort_len[i-1]:\n",
        "                change_loc.append(i)\n",
        "        change_loc.append(len(sort_len))\n",
        "\n",
        "        batch_data_list = []\n",
        "        total = 0\n",
        "        curr = 0\n",
        "        for idx in change_loc:\n",
        "            while curr < idx:\n",
        "                batch_data = []\n",
        "                next = min(curr + batch_size, idx)\n",
        "                for id_ in range(curr, next):\n",
        "                    batch_data.append(self.data[sort_idx[id_]])\n",
        "                curr = next\n",
        "                batch_data, sents_len = self._to_tensor(batch_data, batch_first, device)\n",
        "                batch_data_list.append(batch_data)\n",
        "\n",
        "                total += batch_data.size(0)\n",
        "                assert(sents_len == ([sents_len[0]] * len(sents_len)))\n",
        "\n",
        "        assert(total == len(self.data))\n",
        "        return batch_data_list\n",
        "\n",
        "\n",
        "    def data_sample(self, nsample, device, batch_first=False, shuffle=True):\n",
        "        \"\"\"sample a subset of data (like data_iter)\n",
        "        Returns:\n",
        "            batch_data: LongTensor with shape (seq_len, batch_size)\n",
        "            sents_len: list of data length, this is the data length\n",
        "                       after counting start and stop symbols\n",
        "        \"\"\"\n",
        "\n",
        "        index_arr = np.arange(len(self.data))\n",
        "\n",
        "        if shuffle:\n",
        "            np.random.shuffle(index_arr)\n",
        "\n",
        "        batch_ids = index_arr[: nsample]\n",
        "        batch_data = [self.data[index] for index in batch_ids]\n",
        "\n",
        "        # uncomment this line if the dataset has variable length\n",
        "        batch_data.sort(key=lambda e: -len(e))\n",
        "\n",
        "        batch_data, sents_len = self._to_tensor(batch_data, batch_first, device)\n",
        "\n",
        "        return batch_data, sents_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmDObYSceW4"
      },
      "source": [
        "# LSTM Decoder - Constant Length Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAvQ-TwPb0ml"
      },
      "source": [
        "class LSTM_LM(nn.Module):\n",
        "    \"\"\"LSTM decoder with constant-length data\"\"\"\n",
        "    def __init__(self, args, vocab, model_init, emb_init):\n",
        "        super(LSTM_LM, self).__init__()\n",
        "        self.ni = args.ni\n",
        "        self.nh = args.nh\n",
        "\n",
        "        # no padding when setting padding_idx to -1\n",
        "        self.embed = nn.Embedding(len(vocab), args.ni, padding_idx=-1)\n",
        "\n",
        "        self.dropout_in = nn.Dropout(args.dropout_in)\n",
        "        self.dropout_out = nn.Dropout(args.dropout_out)\n",
        "\n",
        "        # concatenate z with input\n",
        "        self.lstm = nn.LSTM(input_size=args.ni,\n",
        "                            hidden_size=args.nh,\n",
        "                            num_layers=1,\n",
        "                            batch_first=True)\n",
        "\n",
        "        # prediction layer\n",
        "        self.pred_linear = nn.Linear(args.nh, len(vocab), bias=False)\n",
        "\n",
        "        vocab_mask = torch.ones(len(vocab))\n",
        "        # vocab_mask[vocab['<pad>']] = 0\n",
        "        self.loss = nn.CrossEntropyLoss(weight=vocab_mask, reduce=False)\n",
        "\n",
        "        self.reset_parameters(model_init, emb_init)\n",
        "\n",
        "    def reset_parameters(self, model_init, emb_init):\n",
        "        # for name, param in self.lstm.named_parameters():\n",
        "        #     # self.initializer(param)\n",
        "        #     if 'bias' in name:\n",
        "        #         nn.init.constant_(param, 0.0)\n",
        "        #         # model_init(param)\n",
        "        #     elif 'weight' in name:\n",
        "        #         model_init(param)\n",
        "\n",
        "        # model_init(self.trans_linear.weight)\n",
        "        # model_init(self.pred_linear.weight)\n",
        "        for param in self.parameters():\n",
        "            model_init(param)\n",
        "        emb_init(self.embed.weight)\n",
        "\n",
        "\n",
        "    def decode(self, input):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "\n",
        "        # not predicting start symbol\n",
        "        # sents_len -= 1\n",
        "\n",
        "        batch_size, seq_len = input.size()\n",
        "\n",
        "        # (batch_size, seq_len, ni)\n",
        "        word_embed = self.embed(input)\n",
        "        word_embed = self.dropout_in(word_embed)\n",
        "        \n",
        "        c_init = word_embed.new_zeros((1, batch_size, self.nh))\n",
        "        h_init = word_embed.new_zeros((1, batch_size, self.nh))\n",
        "        output, _ = self.lstm(word_embed, (h_init, c_init))\n",
        "\n",
        "        output = self.dropout_out(output)\n",
        "\n",
        "        # (batch_size, seq_len, vocab_size)\n",
        "        output_logits = self.pred_linear(output)\n",
        "\n",
        "        return output_logits\n",
        "\n",
        "    def reconstruct_error(self, x):\n",
        "        \"\"\"Cross Entropy in the language case\n",
        "        Args:\n",
        "            x: (batch_size, seq_len)\n",
        "            z: (batch_size, n_sample, nz)\n",
        "        Returns:\n",
        "            loss: (batch_size). Loss across different sentences\n",
        "        \"\"\"\n",
        "\n",
        "        #remove end symbol\n",
        "        src = x[:, :-1]\n",
        "\n",
        "        # remove start symbol\n",
        "        tgt = x[:, 1:]\n",
        "\n",
        "        batch_size, seq_len = src.size()\n",
        "\n",
        "        # (batch_size * n_sample, seq_len, vocab_size)\n",
        "        output_logits = self.decode(src)\n",
        "\n",
        "        tgt = tgt.contiguous().view(-1)\n",
        "\n",
        "        # (batch_size * seq_len)\n",
        "        loss = self.loss(output_logits.view(-1, output_logits.size(2)),\n",
        "                         tgt)\n",
        "\n",
        "\n",
        "        # (batch_size)\n",
        "        return loss.view(batch_size, -1).sum(-1)\n",
        "\n",
        "    def log_probability(self, x):\n",
        "        \"\"\"Cross Entropy in the language case\n",
        "        Args:\n",
        "            x: (batch_size, seq_len)\n",
        "        Returns:\n",
        "            log_p: (batch_size).\n",
        "        \"\"\"\n",
        "\n",
        "        return -self.reconstruct_error(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaFnPuWydLsf"
      },
      "source": [
        "# Standard VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeUhUQ5ba8dk"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    \"\"\"VAE with normal prior\"\"\"\n",
        "    def __init__(self, encoder, decoder, args):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        self.args = args\n",
        "\n",
        "        self.nz = args.nz\n",
        "\n",
        "        loc = torch.zeros(self.nz, device=args.device)\n",
        "        scale = torch.ones(self.nz, device=args.device)\n",
        "\n",
        "        self.prior = torch.distributions.normal.Normal(loc, scale)\n",
        "\n",
        "    def encode(self, x, nsamples=1):\n",
        "        \"\"\"\n",
        "        Returns: Tensor1, Tensor2\n",
        "            Tensor1: the tensor latent z with shape [batch, nsamples, nz]\n",
        "            Tensor2: the tenor of KL for each x with shape [batch]\n",
        "        \"\"\"\n",
        "        return self.encoder.encode(x, nsamples)\n",
        "\n",
        "    def encode_stats(self, x):\n",
        "        \"\"\"\n",
        "        Returns: Tensor1, Tensor2\n",
        "            Tensor1: the mean of latent z with shape [batch, nz]\n",
        "            Tensor2: the logvar of latent z with shape [batch, nz]\n",
        "        \"\"\"\n",
        "\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z, strategy, K=5):\n",
        "        \"\"\"generate samples from z given strategy\n",
        "        Args:\n",
        "            z: [batch, nsamples, nz]\n",
        "            strategy: \"beam\" or \"greedy\" or \"sample\"\n",
        "            K: the beam width parameter\n",
        "        Returns: List1\n",
        "            List1: a list of decoded word sequence\n",
        "        \"\"\"\n",
        "\n",
        "        if strategy == \"beam\":\n",
        "            return self.decoder.beam_search_decode(z, K)\n",
        "        elif strategy == \"greedy\":\n",
        "            return self.decoder.greedy_decode(z)\n",
        "        elif strategy == \"sample\":\n",
        "            return self.decoder.sample_decode(z)\n",
        "        else:\n",
        "            raise ValueError(\"the decoding strategy is not supported\")\n",
        "\n",
        "    def reconstruct(self, x, decoding_strategy=\"greedy\", K=5):\n",
        "        \"\"\"reconstruct from input x\n",
        "        Args:\n",
        "            x: (batch, *)\n",
        "            decoding_strategy: \"beam\" or \"greedy\" or \"sample\"\n",
        "            K: the beam width parameter (if applicable)\n",
        "        Returns: List1\n",
        "            List1: a list of decoded word sequence\n",
        "        \"\"\"\n",
        "        z = self.sample_from_inference(x).squeeze(1)\n",
        "\n",
        "        return self.decode(z, decoding_strategy, K)\n",
        "\n",
        "\n",
        "    def loss(self, x, kl_weight, nsamples=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: if the data is constant-length, x is the data tensor with\n",
        "                shape (batch, *). Otherwise x is a tuple that contains\n",
        "                the data tensor and length list\n",
        "        Returns: Tensor1, Tensor2, Tensor3\n",
        "            Tensor1: total loss [batch]\n",
        "            Tensor2: reconstruction loss shape [batch]\n",
        "            Tensor3: KL loss shape [batch]\n",
        "        \"\"\"\n",
        "\n",
        "        z, KL = self.encode(x, nsamples)\n",
        "\n",
        "        # (batch)\n",
        "        reconstruct_err = self.decoder.reconstruct_error(x, z).mean(dim=1)\n",
        "\n",
        "\n",
        "        return reconstruct_err + kl_weight * KL, reconstruct_err, KL\n",
        "\n",
        "    def nll_iw(self, x, nsamples, ns=100):\n",
        "        \"\"\"compute the importance weighting estimate of the log-likelihood\n",
        "        Args:\n",
        "            x: if the data is constant-length, x is the data tensor with\n",
        "                shape (batch, *). Otherwise x is a tuple that contains\n",
        "                the data tensor and length list\n",
        "            nsamples: Int\n",
        "                the number of samples required to estimate marginal data likelihood\n",
        "        Returns: Tensor1\n",
        "            Tensor1: the estimate of log p(x), shape [batch]\n",
        "        \"\"\"\n",
        "\n",
        "        # compute iw every ns samples to address the memory issue\n",
        "        # nsamples = 500, ns = 100\n",
        "        # nsamples = 500, ns = 10\n",
        "        tmp = []\n",
        "        for _ in range(int(nsamples / ns)):\n",
        "            # [batch, ns, nz]\n",
        "            # param is the parameters required to evaluate q(z|x)\n",
        "            z, param = self.encoder.sample(x, ns)\n",
        "\n",
        "            # [batch, ns]\n",
        "            log_comp_ll = self.eval_complete_ll(x, z)\n",
        "            log_infer_ll = self.eval_inference_dist(x, z, param)\n",
        "\n",
        "            tmp.append(log_comp_ll - log_infer_ll)\n",
        "\n",
        "        ll_iw = log_sum_exp(torch.cat(tmp, dim=-1), dim=-1) - math.log(nsamples)\n",
        "\n",
        "        return -ll_iw\n",
        "\n",
        "    def KL(self, x):\n",
        "        _, KL = self.encode(x, 1)\n",
        "\n",
        "        return KL\n",
        "\n",
        "    def eval_prior_dist(self, zrange):\n",
        "        \"\"\"perform grid search to calculate the true posterior\n",
        "        Args:\n",
        "            zrange: tensor\n",
        "                different z points that will be evaluated, with\n",
        "                shape (k^2, nz), where k=(zmax - zmin)/space\n",
        "        \"\"\"\n",
        "\n",
        "        # (k^2)\n",
        "        return self.prior.log_prob(zrange).sum(dim=-1)\n",
        "\n",
        "    def eval_complete_ll(self, x, z):\n",
        "        \"\"\"compute log p(z,x)\n",
        "        Args:\n",
        "            x: Tensor\n",
        "                input with shape [batch, seq_len]\n",
        "            z: Tensor\n",
        "                evaluation points with shape [batch, nsamples, nz]\n",
        "        Returns: Tensor1\n",
        "            Tensor1: log p(z,x) Tensor with shape [batch, nsamples]\n",
        "        \"\"\"\n",
        "\n",
        "        # [batch, nsamples]\n",
        "        log_prior = self.eval_prior_dist(z)\n",
        "        log_gen = self.eval_cond_ll(x, z)\n",
        "\n",
        "        return log_prior + log_gen\n",
        "\n",
        "    def eval_cond_ll(self, x, z):\n",
        "        \"\"\"compute log p(x|z)\n",
        "        \"\"\"\n",
        "\n",
        "        return self.decoder.log_probability(x, z)\n",
        "\n",
        "    def eval_log_model_posterior(self, x, grid_z):\n",
        "        \"\"\"perform grid search to calculate the true posterior\n",
        "         this function computes p(z|x)\n",
        "        Args:\n",
        "            grid_z: tensor\n",
        "                different z points that will be evaluated, with\n",
        "                shape (k^2, nz), where k=(zmax - zmin)/pace\n",
        "        Returns: Tensor\n",
        "            Tensor: the log posterior distribution log p(z|x) with\n",
        "                    shape [batch_size, K^2]\n",
        "        \"\"\"\n",
        "        try:\n",
        "            batch_size = x.size(0)\n",
        "        except:\n",
        "            batch_size = x[0].size(0)\n",
        "\n",
        "        # (batch_size, k^2, nz)\n",
        "        grid_z = grid_z.unsqueeze(0).expand(batch_size, *grid_z.size()).contiguous()\n",
        "\n",
        "        # (batch_size, k^2)\n",
        "        log_comp = self.eval_complete_ll(x, grid_z)\n",
        "\n",
        "        # normalize to posterior\n",
        "        log_posterior = log_comp - log_sum_exp(log_comp, dim=1, keepdim=True)\n",
        "\n",
        "        return log_posterior\n",
        "\n",
        "    def sample_from_prior(self, nsamples):\n",
        "        \"\"\"sampling from prior distribution\n",
        "        Returns: Tensor\n",
        "            Tensor: samples from prior with shape (nsamples, nz)\n",
        "        \"\"\"\n",
        "        return self.prior.sample((nsamples,))\n",
        "\n",
        "\n",
        "    def sample_from_inference(self, x, nsamples=1):\n",
        "        \"\"\"perform sampling from inference net\n",
        "        Returns: Tensor\n",
        "            Tensor: samples from infernece nets with\n",
        "                shape (batch_size, nsamples, nz)\n",
        "        \"\"\"\n",
        "        z, _ = self.encoder.sample(x, nsamples)\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "    def sample_from_posterior(self, x, nsamples):\n",
        "        \"\"\"perform MH sampling from model posterior\n",
        "        Returns: Tensor\n",
        "            Tensor: samples from model posterior with\n",
        "                shape (batch_size, nsamples, nz)\n",
        "        \"\"\"\n",
        "\n",
        "        # use the samples from inference net as initial points\n",
        "        # for MCMC sampling. [batch_size, nsamples, nz]\n",
        "        cur = self.encoder.sample_from_inference(x, 1)\n",
        "        cur_ll = self.eval_complete_ll(x, cur)\n",
        "        total_iter = self.args.mh_burn_in + nsamples * self.args.mh_thin\n",
        "        samples = []\n",
        "        for iter_ in range(total_iter):\n",
        "            next = torch.normal(mean=cur,\n",
        "                std=cur.new_full(size=cur.size(), fill_value=self.args.mh_std))\n",
        "            # [batch_size, 1]\n",
        "            next_ll = self.eval_complete_ll(x, next)\n",
        "            ratio = next_ll - cur_ll\n",
        "\n",
        "            accept_prob = torch.min(ratio.exp(), ratio.new_ones(ratio.size()))\n",
        "\n",
        "            uniform_t = accept_prob.new_empty(accept_prob.size()).uniform_()\n",
        "\n",
        "            # [batch_size, 1]\n",
        "            mask = (uniform_t < accept_prob).float()\n",
        "\n",
        "            mask_ = mask.unsqueeze(2)\n",
        "\n",
        "            cur = mask_ * next + (1 - mask_) * cur\n",
        "            cur_ll = mask * next_ll + (1 - mask) * cur_ll\n",
        "\n",
        "            if iter_ >= self.args.mh_burn_in and (iter_ - self.args.mh_burn_in) % self.args.mh_thin == 0:\n",
        "                samples.append(cur.unsqueeze(1))\n",
        "\n",
        "\n",
        "        return torch.cat(samples, dim=1)\n",
        "\n",
        "    def calc_model_posterior_mean(self, x, grid_z):\n",
        "        \"\"\"compute the mean value of model posterior, i.e. E_{z ~ p(z|x)}[z]\n",
        "        Args:\n",
        "            grid_z: different z points that will be evaluated, with\n",
        "                    shape (k^2, nz), where k=(zmax - zmin)/pace\n",
        "            x: [batch, *]\n",
        "        Returns: Tensor1\n",
        "            Tensor1: the mean value tensor with shape [batch, nz]\n",
        "        \"\"\"\n",
        "\n",
        "        # [batch, K^2]\n",
        "        log_posterior = self.eval_log_model_posterior(x, grid_z)\n",
        "        posterior = log_posterior.exp()\n",
        "\n",
        "        # [batch, nz]\n",
        "        return torch.mul(posterior.unsqueeze(2), grid_z.unsqueeze(0)).sum(1)\n",
        "\n",
        "    def calc_infer_mean(self, x):\n",
        "        \"\"\"\n",
        "        Returns: Tensor1\n",
        "            Tensor1: the mean of inference distribution, with shape [batch, nz]\n",
        "        \"\"\"\n",
        "\n",
        "        mean, logvar = self.encoder.forward(x)\n",
        "\n",
        "        return mean\n",
        "\n",
        "\n",
        "\n",
        "    def eval_inference_dist(self, x, z, param=None):\n",
        "        \"\"\"\n",
        "        Returns: Tensor\n",
        "            Tensor: the posterior density tensor with\n",
        "                shape (batch_size, nsamples)\n",
        "        \"\"\"\n",
        "        return self.encoder.eval_inference_dist(x, z, param)\n",
        "\n",
        "    def calc_mi_q(self, x):\n",
        "        \"\"\"Approximate the mutual information between x and z\n",
        "        under distribution q(z|x)\n",
        "        Args:\n",
        "            x: [batch_size, *]. The sampled data to estimate mutual info\n",
        "        \"\"\"\n",
        "\n",
        "        return self.encoder.calc_mi(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K74C9QdFf_uE"
      },
      "source": [
        "# Base Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5axM4Q1Hf8oc"
      },
      "source": [
        "class GaussianEncoderBase(nn.Module):\n",
        "    \"\"\"docstring for EncoderBase\"\"\"\n",
        "    def __init__(self):\n",
        "        super(GaussianEncoderBase, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, *)\n",
        "        Returns: Tensor1, Tensor2\n",
        "            Tensor1: the mean tensor, shape (batch, nz)\n",
        "            Tensor2: the logvar tensor, shape (batch, nz)\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def sample(self, input, nsamples):\n",
        "        \"\"\"sampling from the encoder\n",
        "        Returns: Tensor1, Tuple\n",
        "            Tensor1: the tensor latent z with shape [batch, nsamples, nz]\n",
        "            Tuple: contains the tensor mu [batch, nz] and\n",
        "                logvar[batch, nz]\n",
        "        \"\"\"\n",
        "\n",
        "        # (batch_size, nz)\n",
        "        mu, logvar = self.forward(input)\n",
        "\n",
        "        # (batch, nsamples, nz)\n",
        "        z = self.reparameterize(mu, logvar, nsamples)\n",
        "\n",
        "        return z, (mu, logvar)\n",
        "\n",
        "    def encode(self, input, nsamples):\n",
        "        \"\"\"perform the encoding and compute the KL term\n",
        "        Returns: Tensor1, Tensor2\n",
        "            Tensor1: the tensor latent z with shape [batch, nsamples, nz]\n",
        "            Tensor2: the tenor of KL for each x with shape [batch]\n",
        "        \"\"\"\n",
        "\n",
        "        # (batch_size, nz)\n",
        "        mu, logvar = self.forward(input)\n",
        "\n",
        "        # (batch, nsamples, nz)\n",
        "        z = self.reparameterize(mu, logvar, nsamples)\n",
        "\n",
        "        KL = 0.5 * (mu.pow(2) + logvar.exp() - logvar - 1).sum(dim=1)\n",
        "\n",
        "        return z, KL\n",
        "\n",
        "    def reparameterize(self, mu, logvar, nsamples=1):\n",
        "        \"\"\"sample from posterior Gaussian family\n",
        "        Args:\n",
        "            mu: Tensor\n",
        "                Mean of gaussian distribution with shape (batch, nz)\n",
        "            logvar: Tensor\n",
        "                logvar of gaussian distibution with shape (batch, nz)\n",
        "        Returns: Tensor\n",
        "            Sampled z with shape (batch, nsamples, nz)\n",
        "        \"\"\"\n",
        "        batch_size, nz = mu.size()\n",
        "        std = logvar.mul(0.5).exp()\n",
        "\n",
        "        mu_expd = mu.unsqueeze(1).expand(batch_size, nsamples, nz)\n",
        "        std_expd = std.unsqueeze(1).expand(batch_size, nsamples, nz)\n",
        "\n",
        "        eps = torch.zeros_like(std_expd).normal_()\n",
        "\n",
        "        return mu_expd + torch.mul(eps, std_expd)\n",
        "\n",
        "    def eval_inference_dist(self, x, z, param=None):\n",
        "        \"\"\"this function computes log q(z | x)\n",
        "        Args:\n",
        "            z: tensor\n",
        "                different z points that will be evaluated, with\n",
        "                shape [batch, nsamples, nz]\n",
        "        Returns: Tensor1\n",
        "            Tensor1: log q(z|x) with shape [batch, nsamples]\n",
        "        \"\"\"\n",
        "\n",
        "        nz = z.size(2)\n",
        "\n",
        "        if not param:\n",
        "            mu, logvar = self.forward(x)\n",
        "        else:\n",
        "            mu, logvar = param\n",
        "\n",
        "        # (batch_size, 1, nz)\n",
        "        mu, logvar = mu.unsqueeze(1), logvar.unsqueeze(1)\n",
        "        var = logvar.exp()\n",
        "\n",
        "        # (batch_size, nsamples, nz)\n",
        "        dev = z - mu\n",
        "\n",
        "        # (batch_size, nsamples)\n",
        "        log_density = -0.5 * ((dev ** 2) / var).sum(dim=-1) - \\\n",
        "            0.5 * (nz * math.log(2 * math.pi) + logvar.sum(-1))\n",
        "\n",
        "        return log_density\n",
        "\n",
        "    def calc_mi(self, x):\n",
        "        \"\"\"Approximate the mutual information between x and z\n",
        "        I(x, z) = E_xE_{q(z|x)}log(q(z|x)) - E_xE_{q(z|x)}log(q(z))\n",
        "        Returns: Float\n",
        "        \"\"\"\n",
        "\n",
        "        # [x_batch, nz]\n",
        "        mu, logvar = self.forward(x)\n",
        "\n",
        "        x_batch, nz = mu.size()\n",
        "\n",
        "        # E_{q(z|x)}log(q(z|x)) = -0.5*nz*log(2*\\pi) - 0.5*(1+logvar).sum(-1)\n",
        "        neg_entropy = (-0.5 * nz * math.log(2 * math.pi)- 0.5 * (1 + logvar).sum(-1)).mean()\n",
        "\n",
        "        # [z_batch, 1, nz]\n",
        "        z_samples = self.reparameterize(mu, logvar, 1)\n",
        "\n",
        "        # [1, x_batch, nz]\n",
        "        mu, logvar = mu.unsqueeze(0), logvar.unsqueeze(0)\n",
        "        var = logvar.exp()\n",
        "\n",
        "        # (z_batch, x_batch, nz)\n",
        "        dev = z_samples - mu\n",
        "\n",
        "        # (z_batch, x_batch)\n",
        "        log_density = -0.5 * ((dev ** 2) / var).sum(dim=-1) - \\\n",
        "            0.5 * (nz * math.log(2 * math.pi) + logvar.sum(-1))\n",
        "\n",
        "        # log q(z): aggregate posterior\n",
        "        # [z_batch]\n",
        "        log_qz = log_sum_exp(log_density, dim=1) - math.log(x_batch)\n",
        "\n",
        "        return (neg_entropy - log_qz.mean(-1)).item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy7mHiLvfBSM"
      },
      "source": [
        "# LSTM Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTUdcxSJfAIP"
      },
      "source": [
        "class LSTMEncoder(GaussianEncoderBase):\n",
        "    \"\"\"Gaussian LSTM Encoder with constant-length batching\"\"\"\n",
        "    def __init__(self, args, vocab_size, model_init, emb_init):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.ni = args.ni\n",
        "        self.nh = args.enc_nh\n",
        "        self.nz = args.nz\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, args.ni)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=args.ni,\n",
        "                            hidden_size=args.enc_nh,\n",
        "                            num_layers=1,\n",
        "                            batch_first=True,\n",
        "                            dropout=0)\n",
        "\n",
        "        # dimension transformation to z (mean and logvar)\n",
        "        self.linear = nn.Linear(args.enc_nh, 2 * args.nz, bias=False)\n",
        "\n",
        "        self.reset_parameters(model_init, emb_init)\n",
        "\n",
        "    def reset_parameters(self, model_init, emb_init):\n",
        "        # for name, param in self.lstm.named_parameters():\n",
        "        #     # self.initializer(param)\n",
        "        #     if 'bias' in name:\n",
        "        #         nn.init.constant_(param, 0.0)\n",
        "        #         # model_init(param)\n",
        "        #     elif 'weight' in name:\n",
        "        #         model_init(param)\n",
        "\n",
        "        # model_init(self.linear.weight)\n",
        "        # emb_init(self.embed.weight)\n",
        "        for param in self.parameters():\n",
        "            model_init(param)\n",
        "        emb_init(self.embed.weight)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, seq_len)\n",
        "        Returns: Tensor1, Tensor2\n",
        "            Tensor1: the mean tensor, shape (batch, nz)\n",
        "            Tensor2: the logvar tensor, shape (batch, nz)\n",
        "        \"\"\"\n",
        "\n",
        "        # (batch_size, seq_len-1, args.ni)\n",
        "        word_embed = self.embed(input)\n",
        "\n",
        "        _, (last_state, last_cell) = self.lstm(word_embed)\n",
        "\n",
        "        mean, logvar = self.linear(last_state).chunk(2, -1)\n",
        "\n",
        "        return mean.squeeze(0), logvar.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur2qRub2gw92"
      },
      "source": [
        "# Base Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuiQbuWjgtxu"
      },
      "source": [
        "class DecoderBase(nn.Module):\n",
        "    \"\"\"docstring for Decoder\"\"\"\n",
        "    def __init__(self):\n",
        "        super(DecoderBase, self).__init__()\n",
        "    \n",
        "    def decode(self, x, z):\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def reconstruct_error(self, x, z):\n",
        "        \"\"\"reconstruction loss\n",
        "        Args:\n",
        "            x: (batch_size, *)\n",
        "            z: (batch_size, n_sample, nz)\n",
        "        Returns:\n",
        "            loss: (batch_size, n_sample). Loss\n",
        "            across different sentence and z\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def beam_search_decode(self, z, K):\n",
        "        \"\"\"beam search decoding\n",
        "        Args:\n",
        "            z: (batch_size, nz)\n",
        "            K: the beam size\n",
        "        Returns: List1\n",
        "            List1: the decoded word sentence list\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def sample_decode(self, z):\n",
        "        \"\"\"sampling from z\n",
        "        Args:\n",
        "            z: (batch_size, nz)\n",
        "        Returns: List1\n",
        "            List1: the decoded word sentence list\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def greedy_decode(self, z):\n",
        "        \"\"\"greedy decoding from z\n",
        "        Args:\n",
        "            z: (batch_size, nz)\n",
        "        Returns: List1\n",
        "            List1: the decoded word sentence list\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def log_probability(self, x, z):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, *)\n",
        "            z: (batch_size, n_sample, nz)\n",
        "        Returns:\n",
        "            log_p: (batch_size, n_sample).\n",
        "                log_p(x|z) across different x and z\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDiDqeKBg7if"
      },
      "source": [
        "class BeamSearchNode(object):\n",
        "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
        "        '''\n",
        "        :param hiddenstate:\n",
        "        :param previousNode:\n",
        "        :param wordId:\n",
        "        :param logProb:\n",
        "        :param length:\n",
        "        '''\n",
        "        self.h = hiddenstate\n",
        "        self.prevNode = previousNode\n",
        "        self.wordid = wordId\n",
        "        self.logp = logProb\n",
        "        self.leng = length\n",
        "\n",
        "    def eval(self, alpha=1.0):\n",
        "        reward = 0\n",
        "        # Add here a function for shaping a reward\n",
        "\n",
        "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcQDDsCdgkY-"
      },
      "source": [
        "# LSTM Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akSxvmYrghiF"
      },
      "source": [
        "class LSTMDecoder(DecoderBase):\n",
        "    \"\"\"LSTM decoder with constant-length batching\"\"\"\n",
        "    def __init__(self, args, vocab, model_init, emb_init):\n",
        "        super(LSTMDecoder, self).__init__()\n",
        "        self.ni = args.ni\n",
        "        self.nh = args.dec_nh\n",
        "        self.nz = args.nz\n",
        "        self.vocab = vocab\n",
        "        self.device = args.device\n",
        "\n",
        "        # no padding when setting padding_idx to -1\n",
        "        self.embed = nn.Embedding(len(vocab), args.ni, padding_idx=-1)\n",
        "\n",
        "        self.dropout_in = nn.Dropout(args.dec_dropout_in)\n",
        "        self.dropout_out = nn.Dropout(args.dec_dropout_out)\n",
        "\n",
        "        # for initializing hidden state and cell\n",
        "        self.trans_linear = nn.Linear(args.nz, args.dec_nh, bias=False)\n",
        "\n",
        "        # concatenate z with input\n",
        "        self.lstm = nn.LSTM(input_size=args.ni + args.nz,\n",
        "                            hidden_size=args.dec_nh,\n",
        "                            num_layers=1,\n",
        "                            batch_first=True)\n",
        "\n",
        "        # prediction layer\n",
        "        self.pred_linear = nn.Linear(args.dec_nh, len(vocab), bias=False)\n",
        "\n",
        "        vocab_mask = torch.ones(len(vocab))\n",
        "        # vocab_mask[vocab['<pad>']] = 0\n",
        "        self.loss = nn.CrossEntropyLoss(weight=vocab_mask, reduce=False)\n",
        "\n",
        "        self.reset_parameters(model_init, emb_init)\n",
        "\n",
        "    def reset_parameters(self, model_init, emb_init):\n",
        "        # for name, param in self.lstm.named_parameters():\n",
        "        #     # self.initializer(param)\n",
        "        #     if 'bias' in name:\n",
        "        #         nn.init.constant_(param, 0.0)\n",
        "        #         # model_init(param)\n",
        "        #     elif 'weight' in name:\n",
        "        #         model_init(param)\n",
        "\n",
        "        # model_init(self.trans_linear.weight)\n",
        "        # model_init(self.pred_linear.weight)\n",
        "        for param in self.parameters():\n",
        "            model_init(param)\n",
        "        emb_init(self.embed.weight)\n",
        "\n",
        "    def decode(self, input, z):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input: (batch_size, seq_len)\n",
        "            z: (batch_size, n_sample, nz)\n",
        "        \"\"\"\n",
        "\n",
        "        # not predicting start symbol\n",
        "        # sents_len -= 1\n",
        "\n",
        "        batch_size, n_sample, _ = z.size()\n",
        "        seq_len = input.size(1)\n",
        "\n",
        "        # (batch_size, seq_len, ni)\n",
        "        word_embed = self.embed(input)\n",
        "        word_embed = self.dropout_in(word_embed)\n",
        "\n",
        "        if n_sample == 1:\n",
        "            z_ = z.expand(batch_size, seq_len, self.nz)\n",
        "\n",
        "        else:\n",
        "            word_embed = word_embed.unsqueeze(1).expand(batch_size, n_sample, seq_len, self.ni) \\\n",
        "                                   .contiguous()\n",
        "\n",
        "            # (batch_size * n_sample, seq_len, ni)\n",
        "            word_embed = word_embed.view(batch_size * n_sample, seq_len, self.ni)\n",
        "\n",
        "            z_ = z.unsqueeze(2).expand(batch_size, n_sample, seq_len, self.nz).contiguous()\n",
        "            z_ = z_.view(batch_size * n_sample, seq_len, self.nz)\n",
        "\n",
        "        # (batch_size * n_sample, seq_len, ni + nz)\n",
        "        word_embed = torch.cat((word_embed, z_), -1)\n",
        "\n",
        "        z = z.view(batch_size * n_sample, self.nz)\n",
        "        c_init = self.trans_linear(z).unsqueeze(0)\n",
        "        h_init = torch.tanh(c_init)\n",
        "        # h_init = self.trans_linear(z).unsqueeze(0)\n",
        "        # c_init = h_init.new_zeros(h_init.size())\n",
        "        output, _ = self.lstm(word_embed, (h_init, c_init))\n",
        "\n",
        "        output = self.dropout_out(output)\n",
        "\n",
        "        # (batch_size * n_sample, seq_len, vocab_size)\n",
        "        output_logits = self.pred_linear(output)\n",
        "\n",
        "        return output_logits\n",
        "\n",
        "    def reconstruct_error(self, x, z):\n",
        "        \"\"\"Cross Entropy in the language case\n",
        "        Args:\n",
        "            x: (batch_size, seq_len)\n",
        "            z: (batch_size, n_sample, nz)\n",
        "        Returns:\n",
        "            loss: (batch_size, n_sample). Loss\n",
        "            across different sentence and z\n",
        "        \"\"\"\n",
        "\n",
        "        #remove end symbol\n",
        "        src = x[:, :-1]\n",
        "\n",
        "        # remove start symbol\n",
        "        tgt = x[:, 1:]\n",
        "\n",
        "        batch_size, seq_len = src.size()\n",
        "        n_sample = z.size(1)\n",
        "\n",
        "        # (batch_size * n_sample, seq_len, vocab_size)\n",
        "        output_logits = self.decode(src, z)\n",
        "\n",
        "        if n_sample == 1:\n",
        "            tgt = tgt.contiguous().view(-1)\n",
        "        else:\n",
        "            # (batch_size * n_sample * seq_len)\n",
        "            tgt = tgt.unsqueeze(1).expand(batch_size, n_sample, seq_len) \\\n",
        "                     .contiguous().view(-1)\n",
        "\n",
        "        # (batch_size * n_sample * seq_len)\n",
        "        loss = self.loss(output_logits.view(-1, output_logits.size(2)),\n",
        "                         tgt)\n",
        "\n",
        "\n",
        "        # (batch_size, n_sample)\n",
        "        return loss.view(batch_size, n_sample, -1).sum(-1)\n",
        "\n",
        "\n",
        "    def log_probability(self, x, z):\n",
        "        \"\"\"Cross Entropy in the language case\n",
        "        Args:\n",
        "            x: (batch_size, seq_len)\n",
        "            z: (batch_size, n_sample, nz)\n",
        "        Returns:\n",
        "            log_p: (batch_size, n_sample).\n",
        "                log_p(x|z) across different x and z\n",
        "        \"\"\"\n",
        "\n",
        "        return -self.reconstruct_error(x, z)\n",
        "\n",
        "    def beam_search_decode(self, z, K=5):\n",
        "        \"\"\"beam search decoding, code is based on\n",
        "        https://github.com/pcyin/pytorch_basic_nmt/blob/master/nmt.py\n",
        "        the current implementation decodes sentence one by one, further batching would improve the speed\n",
        "        Args:\n",
        "            z: (batch_size, nz)\n",
        "            K: the beam width\n",
        "        Returns: List1\n",
        "            List1: the decoded word sentence list\n",
        "        \"\"\"\n",
        "\n",
        "        decoded_batch = []\n",
        "        batch_size, nz = z.size()\n",
        "\n",
        "        # (1, batch_size, nz)\n",
        "        c_init = self.trans_linear(z).unsqueeze(0)\n",
        "        h_init = torch.tanh(c_init)\n",
        "\n",
        "        # decoding goes sentence by sentence\n",
        "        for idx in range(batch_size):\n",
        "            # Start with the start of the sentence token\n",
        "            decoder_input = torch.tensor([[self.vocab[\"<s>\"]]], dtype=torch.long, device=self.device)\n",
        "            decoder_hidden = (h_init[:,idx,:].unsqueeze(1), c_init[:,idx,:].unsqueeze(1))\n",
        "\n",
        "            node = BeamSearchNode(decoder_hidden, None, decoder_input, 0., 1)\n",
        "            live_hypotheses = [node]\n",
        "\n",
        "            completed_hypotheses = []\n",
        "\n",
        "            t = 0\n",
        "            while len(completed_hypotheses) < K and t < 100:\n",
        "                t += 1\n",
        "\n",
        "                # (len(live), 1)\n",
        "                decoder_input = torch.cat([node.wordid for node in live_hypotheses], dim=0)\n",
        "\n",
        "                # (1, len(live), nh)\n",
        "                decoder_hidden_h = torch.cat([node.h[0] for node in live_hypotheses], dim=1)\n",
        "                decoder_hidden_c = torch.cat([node.h[1] for node in live_hypotheses], dim=1)\n",
        "\n",
        "                decoder_hidden = (decoder_hidden_h, decoder_hidden_c)\n",
        "\n",
        "\n",
        "                # (len(live), 1, ni) --> (len(live), 1, ni+nz)\n",
        "                word_embed = self.embed(decoder_input)\n",
        "                word_embed = torch.cat((word_embed, z[idx].view(1, 1, -1).expand(\n",
        "                    len(live_hypotheses), 1, nz)), dim=-1)\n",
        "\n",
        "                output, decoder_hidden = self.lstm(word_embed, decoder_hidden)\n",
        "\n",
        "                # (len(live), 1, vocab_size)\n",
        "                output_logits = self.pred_linear(output)\n",
        "                decoder_output = F.log_softmax(output_logits, dim=-1)\n",
        "\n",
        "                prev_logp = torch.tensor([node.logp for node in live_hypotheses], dtype=torch.float, device=self.device)\n",
        "                decoder_output = decoder_output + prev_logp.view(len(live_hypotheses), 1, 1)\n",
        "\n",
        "                # (len(live) * vocab_size)\n",
        "                decoder_output = decoder_output.view(-1)\n",
        "\n",
        "                # (K)\n",
        "                log_prob, indexes = torch.topk(decoder_output, K-len(completed_hypotheses))\n",
        "\n",
        "                live_ids = indexes // len(self.vocab)\n",
        "                word_ids = indexes % len(self.vocab)\n",
        "\n",
        "                live_hypotheses_new = []\n",
        "                for live_id, word_id, log_prob_ in zip(live_ids, word_ids, log_prob):\n",
        "                    node = BeamSearchNode((decoder_hidden[0][:, live_id, :].unsqueeze(1),\n",
        "                        decoder_hidden[1][:, live_id, :].unsqueeze(1)),\n",
        "                        live_hypotheses[live_id], word_id.view(1, 1), log_prob_, t)\n",
        "\n",
        "                    if word_id.item() == self.vocab[\"</s>\"]:\n",
        "                        completed_hypotheses.append(node)\n",
        "                    else:\n",
        "                        live_hypotheses_new.append(node)\n",
        "\n",
        "                live_hypotheses = live_hypotheses_new\n",
        "\n",
        "                if len(completed_hypotheses) == K:\n",
        "                    break\n",
        "\n",
        "            for live in live_hypotheses:\n",
        "                completed_hypotheses.append(live)\n",
        "\n",
        "            utterances = []\n",
        "            for n in sorted(completed_hypotheses, key=lambda node: node.logp, reverse=True):\n",
        "                utterance = []\n",
        "                utterance.append(self.vocab.id2word(n.wordid.item()))\n",
        "                # back trace\n",
        "                while n.prevNode != None:\n",
        "                    n = n.prevNode\n",
        "                    utterance.append(self.vocab.id2word(n.wordid.item()))\n",
        "\n",
        "                utterance = utterance[::-1]\n",
        "\n",
        "                utterances.append(utterance)\n",
        "\n",
        "                # only save the top 1\n",
        "                break\n",
        "\n",
        "            decoded_batch.append(utterances[0])\n",
        "\n",
        "        return decoded_batch\n",
        "\n",
        "    def greedy_decode(self, z):\n",
        "        \"\"\"greedy decoding from z\n",
        "        Args:\n",
        "            z: (batch_size, nz)\n",
        "        Returns: List1\n",
        "            List1: the decoded word sentence list\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = z.size(0)\n",
        "        decoded_batch = [[] for _ in range(batch_size)]\n",
        "\n",
        "        # (batch_size, 1, nz)\n",
        "        c_init = self.trans_linear(z).unsqueeze(0)\n",
        "        h_init = torch.tanh(c_init)\n",
        "\n",
        "        decoder_hidden = (h_init, c_init)\n",
        "        decoder_input = torch.tensor([self.vocab[\"<s>\"]] * batch_size, dtype=torch.long, device=self.device).unsqueeze(1)\n",
        "        end_symbol = torch.tensor([self.vocab[\"</s>\"]] * batch_size, dtype=torch.long, device=self.device)\n",
        "\n",
        "        mask = torch.ones((batch_size), dtype=torch.uint8, device=self.device)\n",
        "        length_c = 1\n",
        "        while mask.sum().item() != 0 and length_c < 100:\n",
        "\n",
        "            # (batch_size, 1, ni) --> (batch_size, 1, ni+nz)\n",
        "            word_embed = self.embed(decoder_input)\n",
        "            word_embed = torch.cat((word_embed, z.unsqueeze(1)), dim=-1)\n",
        "\n",
        "            output, decoder_hidden = self.lstm(word_embed, decoder_hidden)\n",
        "\n",
        "            # (batch_size, 1, vocab_size) --> (batch_size, vocab_size)\n",
        "            decoder_output = self.pred_linear(output)\n",
        "            output_logits = decoder_output.squeeze(1)\n",
        "\n",
        "            # (batch_size)\n",
        "            max_index = torch.argmax(output_logits, dim=1)\n",
        "            # max_index = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            decoder_input = max_index.unsqueeze(1)\n",
        "            length_c += 1\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                if mask[i].item():\n",
        "                    decoded_batch[i].append(self.vocab.id2word(max_index[i].item()))\n",
        "\n",
        "            mask = torch.mul((max_index != end_symbol), mask)\n",
        "\n",
        "        return decoded_batch\n",
        "\n",
        "    def sample_decode(self, z):\n",
        "        \"\"\"sampling decoding from z\n",
        "        Args:\n",
        "            z: (batch_size, nz)\n",
        "        Returns: List1\n",
        "            List1: the decoded word sentence list\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = z.size(0)\n",
        "        decoded_batch = [[] for _ in range(batch_size)]\n",
        "\n",
        "        # (batch_size, 1, nz)\n",
        "        c_init = self.trans_linear(z).unsqueeze(0)\n",
        "        h_init = torch.tanh(c_init)\n",
        "\n",
        "        decoder_hidden = (h_init, c_init)\n",
        "        decoder_input = torch.tensor([self.vocab[\"<s>\"]] * batch_size, dtype=torch.long, device=self.device).unsqueeze(1)\n",
        "        end_symbol = torch.tensor([self.vocab[\"</s>\"]] * batch_size, dtype=torch.long, device=self.device)\n",
        "\n",
        "        mask = torch.ones((batch_size), dtype=torch.uint8, device=self.device)\n",
        "        length_c = 1\n",
        "        while mask.sum().item() != 0 and length_c < 100:\n",
        "\n",
        "            # (batch_size, 1, ni) --> (batch_size, 1, ni+nz)\n",
        "            word_embed = self.embed(decoder_input)\n",
        "            word_embed = torch.cat((word_embed, z.unsqueeze(1)), dim=-1)\n",
        "\n",
        "            output, decoder_hidden = self.lstm(word_embed, decoder_hidden)\n",
        "\n",
        "            # (batch_size, 1, vocab_size) --> (batch_size, vocab_size)\n",
        "            decoder_output = self.pred_linear(output)\n",
        "            output_logits = decoder_output.squeeze(1)\n",
        "\n",
        "            # (batch_size)\n",
        "            sample_prob = F.softmax(output_logits, dim=1)\n",
        "            sample_index = torch.multinomial(sample_prob, num_samples=1).squeeze(1)\n",
        "\n",
        "            decoder_input = sample_index.unsqueeze(1)\n",
        "            length_c += 1\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                if mask[i].item():\n",
        "                    decoded_batch[i].append(self.vocab.id2word(sample_index[i].item()))\n",
        "\n",
        "            mask = torch.mul((sample_index != end_symbol), mask)\n",
        "\n",
        "        return decoded_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhxrJ12Yhs4P"
      },
      "source": [
        "# Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft5eXB7Zhr7z"
      },
      "source": [
        "class Logger(object):\n",
        "  def __init__(self, output_file):\n",
        "    self.terminal = sys.stdout\n",
        "    self.log = open(output_file, \"w\")\n",
        "\n",
        "  def write(self, message):\n",
        "    print(message, end=\"\", file=self.terminal, flush=True)\n",
        "    print(message, end=\"\", file=self.log, flush=True)\n",
        "\n",
        "  def flush(self):\n",
        "    self.terminal.flush()\n",
        "    self.log.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5m0qvUh6ze"
      },
      "source": [
        "# Initialize Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyLs_Yi3hxZb"
      },
      "source": [
        "def init_config():\n",
        "    parser = argparse.ArgumentParser(description='VAE mode collapse study')\n",
        "\n",
        "    # model hyperparameters\n",
        "    parser.add_argument('--dataset', type=str, required=True, help='dataset to use')\n",
        "\n",
        "    # optimization parameters\n",
        "    parser.add_argument('--momentum', type=float, default=0, help='sgd momentum')\n",
        "    parser.add_argument('--nsamples', type=int, default=1, help='number of samples for training')\n",
        "    parser.add_argument('--iw_nsamples', type=int, default=500,\n",
        "                         help='number of samples to compute importance weighted estimate')\n",
        "\n",
        "    # select mode\n",
        "    parser.add_argument('--eval', action='store_true', default=False, help='compute iw nll')\n",
        "    parser.add_argument('--load_path', type=str, default='')\n",
        "\n",
        "\n",
        "    # decoding\n",
        "    parser.add_argument('--decode_from', type=str, default=\"\", help=\"pretrained model path\")\n",
        "    parser.add_argument('--decoding_strategy', type=str, choices=[\"greedy\", \"beam\", \"sample\"], default=\"greedy\")\n",
        "    parser.add_argument('--decode_input', type=str, default=\"\", help=\"input text file to perform reconstruction\")\n",
        "\n",
        "\n",
        "    # annealing paramters\n",
        "    parser.add_argument('--warm_up', type=int, default=10, help=\"number of annealing epochs\")\n",
        "    parser.add_argument('--kl_start', type=float, default=1.0, help=\"starting KL weight\")\n",
        "\n",
        "    # inference parameters\n",
        "    parser.add_argument('--aggressive', type=int, default=0,\n",
        "                         help='apply aggressive training when nonzero, reduce to vanilla VAE when aggressive is 0')\n",
        "    # others\n",
        "    parser.add_argument('--seed', type=int, default=783435, metavar='S', help='random seed')\n",
        "\n",
        "    # these are for slurm purpose to save model\n",
        "    parser.add_argument('--jobid', type=int, default=0, help='slurm job id')\n",
        "    parser.add_argument('--taskid', type=int, default=0, help='slurm task id')\n",
        "\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    args.cuda = torch.cuda.is_available()\n",
        "\n",
        "    save_dir = \"models/%s\" % args.dataset\n",
        "    log_dir = \"logs/%s\" % args.dataset\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "    seed_set = [783435, 101, 202, 303, 404, 505, 606, 707, 808, 909]\n",
        "    args.seed = seed_set[args.taskid]\n",
        "\n",
        "    id_ = \"%s_aggressive%d_kls%.2f_warm%d_%d_%d_%d\" % \\\n",
        "            (args.dataset, args.aggressive, args.kl_start,\n",
        "             args.warm_up, args.jobid, args.taskid, args.seed)\n",
        "\n",
        "    save_path = os.path.join(save_dir, id_ + '.pt')\n",
        "\n",
        "    args.save_path = save_path\n",
        "    print(\"save path\", args.save_path)\n",
        "\n",
        "    args.log_path = os.path.join(log_dir, id_ + \".log\")\n",
        "    print(\"log path\", args.log_path)\n",
        "\n",
        "    # load config file into args\n",
        "    config_file = \"config.config_%s\" % args.dataset\n",
        "    params = importlib.import_module(config_file).params\n",
        "    args = argparse.Namespace(**vars(args), **params)\n",
        "\n",
        "    if 'label' in params:\n",
        "        args.label = params['label']\n",
        "    else:\n",
        "        args.label = False\n",
        "\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.cuda:\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FilAkD8xiG0r"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNh4GXMkiK7A"
      },
      "source": [
        "def reconstruct(model, data, strategy, fname, device):\n",
        "    with open(fname, \"w\") as fout:\n",
        "        for batch_data, sent_len in data.data_iter(batch_size=1, device=device,\n",
        "                                                   batch_first=True, shuffle=False):\n",
        "            decoded_batch = model.reconstruct(batch_data, strategy)\n",
        "\n",
        "            for sent in decoded_batch:\n",
        "                fout.write(\" \".join(sent) + \"\\n\")\n",
        "\n",
        "def sample_from_prior(model, z, strategy, fname):\n",
        "    with open(fname, \"w\") as fout:\n",
        "        decoded_batch = model.decode(z, strategy)\n",
        "\n",
        "        for sent in decoded_batch:\n",
        "            fout.write(\" \".join(sent) + \"\\n\")\n",
        "\n",
        "def calc_iwnll(model, test_data_batch, args, ns=100):\n",
        "    report_nll_loss = 0\n",
        "    report_num_words = report_num_sents = 0\n",
        "    for id_, i in enumerate(np.random.permutation(len(test_data_batch))):\n",
        "        batch_data = test_data_batch[i]\n",
        "        batch_size, sent_len = batch_data.size()\n",
        "\n",
        "        # not predict start symbol\n",
        "        report_num_words += (sent_len - 1) * batch_size\n",
        "\n",
        "        report_num_sents += batch_size\n",
        "        if id_ % (round(len(test_data_batch) / 10)) == 0:\n",
        "            print('iw nll computing %d0%%' % (id_/(round(len(test_data_batch) / 10))))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        loss = model.nll_iw(batch_data, nsamples=args.iw_nsamples, ns=ns)\n",
        "\n",
        "        report_nll_loss += loss.sum().item()\n",
        "\n",
        "    nll = report_nll_loss / report_num_sents\n",
        "    ppl = np.exp(nll * report_num_sents / report_num_words)\n",
        "\n",
        "    print('iw nll: %.4f, iw ppl: %.4f' % (nll, ppl))\n",
        "    sys.stdout.flush()\n",
        "    return nll, ppl\n",
        "\n",
        "def calc_mi(model, test_data_batch):\n",
        "    mi = 0\n",
        "    num_examples = 0\n",
        "    for batch_data in test_data_batch:\n",
        "        batch_size = batch_data.size(0)\n",
        "        num_examples += batch_size\n",
        "        mutual_info = model.calc_mi_q(batch_data)\n",
        "        mi += mutual_info * batch_size\n",
        "\n",
        "    return mi / num_examples\n",
        "\n",
        "def calc_au(model, test_data_batch, delta=0.01):\n",
        "    \"\"\"compute the number of active units\n",
        "    \"\"\"\n",
        "    cnt = 0\n",
        "    for batch_data in test_data_batch:\n",
        "        mean, _ = model.encode_stats(batch_data)\n",
        "        if cnt == 0:\n",
        "            means_sum = mean.sum(dim=0, keepdim=True)\n",
        "        else:\n",
        "            means_sum = means_sum + mean.sum(dim=0, keepdim=True)\n",
        "        cnt += mean.size(0)\n",
        "\n",
        "    # (1, nz)\n",
        "    mean_mean = means_sum / cnt\n",
        "\n",
        "    cnt = 0\n",
        "    for batch_data in test_data_batch:\n",
        "        mean, _ = model.encode_stats(batch_data)\n",
        "        if cnt == 0:\n",
        "            var_sum = ((mean - mean_mean) ** 2).sum(dim=0)\n",
        "        else:\n",
        "            var_sum = var_sum + ((mean - mean_mean) ** 2).sum(dim=0)\n",
        "        cnt += mean.size(0)\n",
        "\n",
        "    # (nz)\n",
        "    au_var = var_sum / (cnt - 1)\n",
        "\n",
        "    return (au_var >= delta).sum().item(), au_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecmXLN-YiZ-G"
      },
      "source": [
        "# Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53kZ7NVriVHP"
      },
      "source": [
        "def test(model, test_data_batch, mode, args, verbose=True):\n",
        "    report_kl_loss = report_rec_loss = 0\n",
        "    report_num_words = report_num_sents = 0\n",
        "    for i in np.random.permutation(len(test_data_batch)):\n",
        "        batch_data = test_data_batch[i]\n",
        "        batch_size, sent_len = batch_data.size()\n",
        "\n",
        "        # not predict start symbol\n",
        "        report_num_words += (sent_len - 1) * batch_size\n",
        "\n",
        "        report_num_sents += batch_size\n",
        "\n",
        "\n",
        "        loss, loss_rc, loss_kl = model.loss(batch_data, 1.0, nsamples=args.nsamples)\n",
        "\n",
        "        assert(not loss_rc.requires_grad)\n",
        "\n",
        "        loss_rc = loss_rc.sum()\n",
        "        loss_kl = loss_kl.sum()\n",
        "\n",
        "\n",
        "        report_rec_loss += loss_rc.item()\n",
        "        report_kl_loss += loss_kl.item()\n",
        "\n",
        "    mutual_info = calc_mi(model, test_data_batch)\n",
        "\n",
        "    test_loss = (report_rec_loss  + report_kl_loss) / report_num_sents\n",
        "\n",
        "    nll = (report_kl_loss + report_rec_loss) / report_num_sents\n",
        "    kl = report_kl_loss / report_num_sents\n",
        "    ppl = np.exp(nll * report_num_sents / report_num_words)\n",
        "    if verbose:\n",
        "        print('%s --- avg_loss: %.4f, kl: %.4f, mi: %.4f, recon: %.4f, nll: %.4f, ppl: %.4f' % \\\n",
        "               (mode, test_loss, report_kl_loss / report_num_sents, mutual_info,\n",
        "                report_rec_loss / report_num_sents, nll, ppl))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    return test_loss, nll, kl, ppl, mutual_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5olr-UBitO1"
      },
      "source": [
        "# Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI-IQPs6ixc1"
      },
      "source": [
        "def main(args):\n",
        "\n",
        "    class uniform_initializer(object):\n",
        "        def __init__(self, stdv):\n",
        "            self.stdv = stdv\n",
        "        def __call__(self, tensor):\n",
        "            nn.init.uniform_(tensor, -self.stdv, self.stdv)\n",
        "\n",
        "\n",
        "    class xavier_normal_initializer(object):\n",
        "        def __call__(self, tensor):\n",
        "            nn.init.xavier_normal_(tensor)\n",
        "\n",
        "    if args.cuda:\n",
        "        print('using cuda')\n",
        "\n",
        "    print(args)\n",
        "\n",
        "    opt_dict = {\"not_improved\": 0, \"lr\": 1., \"best_loss\": 1e4}\n",
        "\n",
        "    train_data = MonoTextData(args.train_data, label=args.label)\n",
        "\n",
        "    vocab = train_data.vocab\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    val_data = MonoTextData(args.val_data, label=args.label, vocab=vocab)\n",
        "    test_data = MonoTextData(args.test_data, label=args.label, vocab=vocab)\n",
        "\n",
        "    print('Train data: %d samples' % len(train_data))\n",
        "    print('finish reading datasets, vocab size is %d' % len(vocab))\n",
        "    print('dropped sentences: %d' % train_data.dropped)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    log_niter = (len(train_data)//args.batch_size)//10\n",
        "\n",
        "    model_init = uniform_initializer(0.01)\n",
        "    emb_init = uniform_initializer(0.1)\n",
        "\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    args.device = device\n",
        "\n",
        "    if args.enc_type == 'lstm':\n",
        "        encoder = LSTMEncoder(args, vocab_size, model_init, emb_init)\n",
        "        args.enc_nh = args.dec_nh\n",
        "    else:\n",
        "        raise ValueError(\"the specified encoder type is not supported\")\n",
        "\n",
        "    decoder = LSTMDecoder(args, vocab, model_init, emb_init)\n",
        "\n",
        "    vae = VAE(encoder, decoder, args).to(device)\n",
        "\n",
        "    if args.decode_from != \"\":\n",
        "        print('begin decoding')\n",
        "        vae.load_state_dict(torch.load(args.decode_from))\n",
        "        vae.eval()\n",
        "        save_dir = \"samples/\"\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        path = \".\".join(args.decode_from.split(\"/\")[-1].split(\".\")[:-1]) + \\\n",
        "                \"_{}\".format(args.decoding_strategy)\n",
        "        with torch.no_grad():\n",
        "            if args.decode_input != \"\":\n",
        "                decode_data = MonoTextData(args.decode_input, vocab=vocab)\n",
        "\n",
        "                reconstruct(vae, decode_data, args.decoding_strategy,\n",
        "                    os.path.join(save_dir, path + \".rec\"), args.device)\n",
        "            else:\n",
        "                z = vae.sample_from_prior(100)\n",
        "                sample_from_prior(vae, z, args.decoding_strategy,\n",
        "                    os.path.join(save_dir, path + \".sample\"))\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    if args.eval:\n",
        "        print('begin evaluation')\n",
        "        vae.load_state_dict(torch.load(args.load_path))\n",
        "        vae.eval()\n",
        "        with torch.no_grad():\n",
        "            test_data_batch = test_data.create_data_batch(batch_size=args.batch_size,\n",
        "                                                          device=device,\n",
        "                                                          batch_first=True)\n",
        "\n",
        "            test(vae, test_data_batch, \"TEST\", args)\n",
        "            au, au_var = calc_au(vae, test_data_batch)\n",
        "            print(\"%d active units\" % au)\n",
        "            # print(au_var)\n",
        "\n",
        "            test_data_batch = test_data.create_data_batch(batch_size=1,\n",
        "                                                          device=device,\n",
        "                                                          batch_first=True)\n",
        "            calc_iwnll(vae, test_data_batch, args)\n",
        "\n",
        "        return\n",
        "\n",
        "    enc_optimizer = optim.SGD(vae.encoder.parameters(), lr=1.0, momentum=args.momentum)\n",
        "    dec_optimizer = optim.SGD(vae.decoder.parameters(), lr=1.0, momentum=args.momentum)\n",
        "    opt_dict['lr'] = 1.0\n",
        "\n",
        "    iter_ = decay_cnt = 0\n",
        "    best_loss = 1e4\n",
        "    best_kl = best_nll = best_ppl = 0\n",
        "    pre_mi = 0\n",
        "    aggressive_flag = True if args.aggressive else False\n",
        "    vae.train()\n",
        "    start = time.time()\n",
        "\n",
        "    kl_weight = args.kl_start\n",
        "    anneal_rate = (1.0 - args.kl_start) / (args.warm_up * (len(train_data) / args.batch_size))\n",
        "\n",
        "    train_data_batch = train_data.create_data_batch(batch_size=args.batch_size,\n",
        "                                                    device=device,\n",
        "                                                    batch_first=True)\n",
        "\n",
        "    val_data_batch = val_data.create_data_batch(batch_size=args.batch_size,\n",
        "                                                device=device,\n",
        "                                                batch_first=True)\n",
        "\n",
        "    test_data_batch = test_data.create_data_batch(batch_size=args.batch_size,\n",
        "                                                  device=device,\n",
        "                                                  batch_first=True)\n",
        "    for epoch in range(args.epochs):\n",
        "        report_kl_loss = report_rec_loss = 0\n",
        "        report_num_words = report_num_sents = 0\n",
        "        for i in np.random.permutation(len(train_data_batch)):\n",
        "            batch_data = train_data_batch[i]\n",
        "            batch_size, sent_len = batch_data.size()\n",
        "\n",
        "            # not predict start symbol\n",
        "            report_num_words += (sent_len - 1) * batch_size\n",
        "\n",
        "            report_num_sents += batch_size\n",
        "\n",
        "            # kl_weight = 1.0\n",
        "            kl_weight = min(1.0, kl_weight + anneal_rate)\n",
        "\n",
        "            sub_iter = 1\n",
        "            batch_data_enc = batch_data\n",
        "            burn_num_words = 0\n",
        "            burn_pre_loss = 1e4\n",
        "            burn_cur_loss = 0\n",
        "            while aggressive_flag and sub_iter < 100:\n",
        "\n",
        "                enc_optimizer.zero_grad()\n",
        "                dec_optimizer.zero_grad()\n",
        "\n",
        "                burn_batch_size, burn_sents_len = batch_data_enc.size()\n",
        "                burn_num_words += (burn_sents_len - 1) * burn_batch_size\n",
        "\n",
        "                loss, loss_rc, loss_kl = vae.loss(batch_data_enc, kl_weight, nsamples=args.nsamples)\n",
        "\n",
        "                burn_cur_loss += loss.sum().item()\n",
        "                loss = loss.mean(dim=-1)\n",
        "\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(vae.parameters(), clip_grad)\n",
        "\n",
        "                enc_optimizer.step()\n",
        "\n",
        "                id_ = np.random.random_integers(0, len(train_data_batch) - 1)\n",
        "\n",
        "                batch_data_enc = train_data_batch[id_]\n",
        "\n",
        "                if sub_iter % 15 == 0:\n",
        "                    burn_cur_loss = burn_cur_loss / burn_num_words\n",
        "                    if burn_pre_loss - burn_cur_loss < 0:\n",
        "                        break\n",
        "                    burn_pre_loss = burn_cur_loss\n",
        "                    burn_cur_loss = burn_num_words = 0\n",
        "\n",
        "                sub_iter += 1\n",
        "\n",
        "                # if sub_iter >= 30:\n",
        "                #     break\n",
        "\n",
        "            # print(sub_iter)\n",
        "\n",
        "            enc_optimizer.zero_grad()\n",
        "            dec_optimizer.zero_grad()\n",
        "\n",
        "\n",
        "            loss, loss_rc, loss_kl = vae.loss(batch_data, kl_weight, nsamples=args.nsamples)\n",
        "\n",
        "            loss = loss.mean(dim=-1)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(vae.parameters(), clip_grad)\n",
        "\n",
        "            loss_rc = loss_rc.sum()\n",
        "            loss_kl = loss_kl.sum()\n",
        "\n",
        "            if not aggressive_flag:\n",
        "                enc_optimizer.step()\n",
        "\n",
        "            dec_optimizer.step()\n",
        "\n",
        "            report_rec_loss += loss_rc.item()\n",
        "            report_kl_loss += loss_kl.item()\n",
        "\n",
        "            if iter_ % log_niter == 0:\n",
        "                train_loss = (report_rec_loss  + report_kl_loss) / report_num_sents\n",
        "                if aggressive_flag or epoch == 0:\n",
        "                    vae.eval()\n",
        "                    with torch.no_grad():\n",
        "                        mi = calc_mi(vae, val_data_batch)\n",
        "                        au, _ = calc_au(vae, val_data_batch)\n",
        "                    vae.train()\n",
        "\n",
        "                    print('epoch: %d, iter: %d, avg_loss: %.4f, kl: %.4f, mi: %.4f, recon: %.4f,' \\\n",
        "                           'au %d, time elapsed %.2fs' %\n",
        "                           (epoch, iter_, train_loss, report_kl_loss / report_num_sents, mi,\n",
        "                           report_rec_loss / report_num_sents, au, time.time() - start))\n",
        "                else:\n",
        "                    print('epoch: %d, iter: %d, avg_loss: %.4f, kl: %.4f, recon: %.4f,' \\\n",
        "                           'time elapsed %.2fs' %\n",
        "                           (epoch, iter_, train_loss, report_kl_loss / report_num_sents,\n",
        "                           report_rec_loss / report_num_sents, time.time() - start))\n",
        "\n",
        "                sys.stdout.flush()\n",
        "\n",
        "                report_rec_loss = report_kl_loss = 0\n",
        "                report_num_words = report_num_sents = 0\n",
        "\n",
        "            iter_ += 1\n",
        "\n",
        "            if aggressive_flag and (iter_ % len(train_data_batch)) == 0:\n",
        "                vae.eval()\n",
        "                cur_mi = calc_mi(vae, val_data_batch)\n",
        "                vae.train()\n",
        "                print(\"pre mi:%.4f. cur mi:%.4f\" % (pre_mi, cur_mi))\n",
        "                if cur_mi - pre_mi < 0:\n",
        "                    aggressive_flag = False\n",
        "                    print(\"STOP BURNING\")\n",
        "\n",
        "                pre_mi = cur_mi\n",
        "\n",
        "        print('kl weight %.4f' % kl_weight)\n",
        "\n",
        "        vae.eval()\n",
        "        with torch.no_grad():\n",
        "            loss, nll, kl, ppl, mi = test(vae, val_data_batch, \"VAL\", args)\n",
        "            au, au_var = calc_au(vae, val_data_batch)\n",
        "            print(\"%d active units\" % au)\n",
        "            # print(au_var)\n",
        "\n",
        "        if loss < best_loss:\n",
        "            print('update best loss')\n",
        "            best_loss = loss\n",
        "            best_nll = nll\n",
        "            best_kl = kl\n",
        "            best_ppl = ppl\n",
        "            torch.save(vae.state_dict(), args.save_path)\n",
        "\n",
        "        if loss > opt_dict[\"best_loss\"]:\n",
        "            opt_dict[\"not_improved\"] += 1\n",
        "            if opt_dict[\"not_improved\"] >= decay_epoch and epoch >=15:\n",
        "                opt_dict[\"best_loss\"] = loss\n",
        "                opt_dict[\"not_improved\"] = 0\n",
        "                opt_dict[\"lr\"] = opt_dict[\"lr\"] * lr_decay\n",
        "                vae.load_state_dict(torch.load(args.save_path))\n",
        "                print('new lr: %f' % opt_dict[\"lr\"])\n",
        "                decay_cnt += 1\n",
        "                enc_optimizer = optim.SGD(vae.encoder.parameters(), lr=opt_dict[\"lr\"], momentum=args.momentum)\n",
        "                dec_optimizer = optim.SGD(vae.decoder.parameters(), lr=opt_dict[\"lr\"], momentum=args.momentum)\n",
        "\n",
        "        else:\n",
        "            opt_dict[\"not_improved\"] = 0\n",
        "            opt_dict[\"best_loss\"] = loss\n",
        "\n",
        "        if decay_cnt == max_decay:\n",
        "            break\n",
        "\n",
        "        if epoch % args.test_nepoch == 0:\n",
        "            with torch.no_grad():\n",
        "                loss, nll, kl, ppl, _ = test(vae, test_data_batch, \"TEST\", args)\n",
        "\n",
        "        vae.train()\n",
        "\n",
        "    # compute importance weighted estimate of log p(x)\n",
        "    vae.load_state_dict(torch.load(args.save_path))\n",
        "\n",
        "    vae.eval()\n",
        "    with torch.no_grad():\n",
        "        loss, nll, kl, ppl, _ = test(vae, test_data_batch, \"TEST\", args)\n",
        "        au, au_var = calc_au(vae, test_data_batch)\n",
        "        print(\"%d active units\" % au)\n",
        "        # print(au_var)\n",
        "\n",
        "    test_data_batch = test_data.create_data_batch(batch_size=1,\n",
        "                                                  device=device,\n",
        "                                                  batch_first=True)\n",
        "    with torch.no_grad():\n",
        "        calc_iwnll(vae, test_data_batch, args)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = init_config()\n",
        "    if args.decode_from == \"\" and not args.eval:\n",
        "        sys.stdout = Logger(args.log_path)\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}